{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77e72a81",
   "metadata": {},
   "source": [
    "### need to review some missing dependencies - or re-write this section (NOW WE PROCESS THE HLA TYPES TO MATCH WITH OUR AVAILABLE FUSION FILES - patient specific matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31b00b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the final output file for each cohort here were included in the handover folder\n",
    "#I may have removed one dependent file - \"pvf-list\" \n",
    "#.....................................................................................\n",
    "#TCGA - WES x 1058 <-> RNAseq x 1095. should expect 1055 rows)\n",
    "\n",
    "##download all raw files from S3 - renamed as raw_output/${ID}_final.result.txt\n",
    "# location of hla typing result from WES matched normal: \n",
    "# s3://crm.tumorstudy.analysis/tcga-run/HLA_Typing/HLA-HD/WES_slicedBAM/TCGA-*_final.result.txt\n",
    "# did not use result from RNAseq tumour / normal (n=40) this round!\n",
    "##handle \"not_typed\" by replacing space with underscore > extract col 2 & 3 > exclude \"-\" > \n",
    "##convert 2 cols into 1 col > extract 4digits only = clean file (should only have 1055 in total by the end)\n",
    "\n",
    "NUM=$1\n",
    "ID=$(sed -n ${NUM}p ../FINAL_genomicBAM_UUID_fileID_fileName.txt | awk '{print $1}')\n",
    "\n",
    "if [[ -e raw_output/${ID}_final.result.txt ]]\n",
    "then\n",
    "    sed 's/ /_/g' raw_output/${ID}_final.result.txt | sed 's/Not_typed/-/g' | awk '{print $2,$3}' | sed 's/ /\\n/g' | grep -v \"^-\" | sed 's/\\(\\*[0-9]\\{2\\}:[0-9]\\{2\\}\\)\\(:[0-9]\\{2\\}\\)/\\1/g' | uniq >> clean_output/${ID}_clean.result.txt\n",
    "    all_alleles=$(while read gene; do grep $gene clean_output/${ID}_clean.result.txt; done < pvf-list | sed 's/HLA-D/D/g' | paste -sd',')\n",
    "    echo -e \"${ID}\\t${all_alleles}\" >> ../TCGA_patient-specific-pvfAlleles.txt\n",
    "fi\n",
    "\n",
    "#.....................................................................................\n",
    "#MyBrCa - 990-123=867\n",
    "\n",
    "##download result file from S3 - renamed to raw_output/hlahd_${ID}N.txt\n",
    "# location of hla typing result: \n",
    "# s3://crm.tumorstudy.analysis/mybrca-run/HLA_Typing/HLA-HD/WES/BAM/*N_bam/result/*N_WES-MHC-bam_final.result.txt\n",
    "# did not use HLA-HD output from RNAseq tumour (n=123) this round! \n",
    "##format them similarly to the TCGA ones\n",
    "\n",
    "NUM=$1\n",
    "ID=$(sed -n ${NUM}p MYBRCA-867HLAHD-IDs.txt)\n",
    "\n",
    "sed 's/ /_/g' raw_output/hlahd_${ID}N.txt | sed 's/Not_typed/-/g' | awk '{print $2,$3}' | sed 's/ /\\n/g' | grep -v \"^-\" | sed 's/\\(\\*[0-9]\\{2\\}:[0-9]\\{2\\}\\)\\(:[0-9]\\{2\\}\\)/\\1/g' | uniq >> clean_output/hlahd_${ID}N_clean.txt\n",
    "all_alleles=$(while read gene; do grep $gene clean_output/hlahd_${ID}N_clean.txt; done < pvf-list | sed 's/HLA-D/D/g' | paste -sd',')\n",
    "echo -e \"${ID}\\t${all_alleles}\" >> ../MYBRCA_patient-specific-pvfAlleles.txt\n",
    "\n",
    "#.....................................................................................\n",
    "#NTUH - for the 223 samples\n",
    "##format NTUH sent us de list first\n",
    "#results are stored on Dropbox : BRCA_NTUH_CRMY\\NTUH\\HLA\\neopred_TW.HLA.summary.normal.selected.allhlatypes.txt\n",
    "NUM=$1\n",
    "ID=$(sed -n ${NUM}p NTUH_IDs.txt)\n",
    "grep $ID neopred_TW.HLA.summary.normal.selected.allhlatypes.txt | awk '{print $3,$4}' >> ntuh_output/hlahd_${ID}.txt\n",
    "\n",
    "##extract only the first two lines of each hla gene\n",
    "NUM=$1\n",
    "ID=$(sed -n ${NUM}p NTUH_IDs_inconsistent.txt)\n",
    "\n",
    "while read gene; do\n",
    "grep -m 2 $gene ntuh-original/hlahd_${ID}.txt >> ntuh_output/hlahd_${ID}.txt\n",
    "done < ntuh-hlahd-genes\n",
    "\n",
    "##now extract only the relevant genes from each file, exclude Not typed, only include up to 4 digits, only include Dxx*xx:xx instead of HLA-Dxx*xx:xx, REMOVED DUPS!!\n",
    "NUM=$1\n",
    "ID=$(sed -n ${NUM}p NTUH_IDs.txt)\n",
    "\n",
    "while read gene; do\n",
    "grep $gene ntuh_output/hlahd_${ID}.txt | awk '{print $NF}' >> ntuh_output_clean/hlahd_${ID}_clean.txt #this should give exactly 40 lines - more of a sanity check\n",
    "done < pvf-list\n",
    "\n",
    "all_alleles=$(grep -v Not ntuh_output_clean/hlahd_${ID}_clean.txt | uniq | paste -sd',' | sed 's/,HLA-D/,D/g' | sed 's/\\(\\*[0-9]\\{2\\}:[0-9]\\{2\\}\\)\\(:[0-9]\\{2\\}\\)/\\1/g')\n",
    "\n",
    "echo -e \"${ID}\\t${all_alleles}\" >> NTUH_patient-specific-pvfAlleles.txt\n",
    "\n",
    "##upload to $S3/tmp/NTUH_patient-specific-pvfAlleles.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e7850e",
   "metadata": {},
   "source": [
    "### (NOW WE RUN NEOANTIGEN PREDICTION using patient specific HLA type that we cleaned up in the previous step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66120da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will run pvf based on the IDs available. Write it in a way that handles missing fusion files/ wrong ID / wrong cohort entered\n",
    "#remember to manually conda activate agf-condaenv in screen and remove docker container esp before rerun\n",
    "#(only need conda env on main server :D - stereoseq no need)\n",
    "\n",
    "workdir=\"/shared/minhui/PVACFUSE\"\n",
    "cohort=$1\n",
    "NUM=$2\n",
    "datadir=\"\"\n",
    "s3dir=\"\"\n",
    "\n",
    "ID=$(sed -n ${NUM}p ${cohort}_patient-specific-pvfAlleles.txt | awk '{print $1}')\n",
    "ALLELES=$(sed -n ${NUM}p ${cohort}_patient-specific-pvfAlleles.txt | awk '{print $2}')\n",
    "\n",
    "#extract ID for getting the native fusion (filtered) file.\n",
    "#handle NTUH ID differently due to it's inconsistent naming- will use the first matching name (specifically for handling sample A010087L and A010087R. will use L for this round of analysis\n",
    "if [[ $cohort == \"NTUH\" ]]; then\n",
    "    ID_long=$(grep -m 1 $ID /shared/minhui/analysis/${cohort}/NTUH_ID_223only.txt)\n",
    "elif [[ $cohort == \"TCGA\" || $cohort == \"MyBrCa\" ]]; then\n",
    "    ID_long=$ID\n",
    "else\n",
    "    echo \"PLEASE ONLY ENTER NTUH / TCGA / MyBrCa as your first command line argument. Run terminated $(date | awk '{print $3\"/\"$2\"/\"$NF\"_\"$4}')\" >> ${workdir}/${cohort}_agfPVF.failed.txt\n",
    "fi\n",
    "\n",
    "for tool in Arriba FusionCatcher; do\n",
    "\tif [[ $tool == \"Arriba\" ]]; then \n",
    "\t\ttoool=\"arr\"\n",
    "\t\tagf_tool=\"arriba\"\n",
    "\telif [[ $tool == \"FusionCatcher\" ]]; then\n",
    "\t\ttoool=\"fc\"\n",
    "\t\tagf_tool=\"fusioncatcher\"\n",
    "\tfi\n",
    "done\n",
    "\n",
    "\tdatadir=\"/shared/minhui/analysis/${cohort}/${tool}/native-fusion-files-filtered\"\n",
    "\n",
    "\tif [[ -e ${datadir}/${toool}_${ID_long}_filtered.tsv ]]; then\n",
    "\t\n",
    "\t\tif [[ ! -d ${workdir}/${cohort}/sample_${ID} ]]; then\n",
    "        \t\tmkdir ${workdir}/${cohort}/sample_${ID}\n",
    "\t\tfi\n",
    "\t\t\n",
    "\t\techo \"\"LINE_${NUM} SAMPLE_${ID} ${tool}: START $(date | awk '{print $3\"/\"$2\"/\"$NF\"_\"$4}')\" >> ${workdir}/${cohort}_agfPVF.log.txt\n",
    "\n",
    "\t\t#run agf first\n",
    "        \techo -e \"\\n\\n ------------ AGFusion ------------ \" >> ${workdir}/${cohort}/sample_${ID}/sample_${ID}_${toool}.log.txt 2>&1\n",
    "        \techo -e \"$(date | awk '{print $3\"/\"$2\"/\"$NF\"_\"$4}')\\n\\n\" >> ${workdir}/${cohort}/sample_${ID}/sample_${ID}_${toool}.log.txt 2>&1\n",
    "        \tagfusion batch \\\n",
    "        \t-f ${datadir}/${toool}_${ID_long}_filtered.tsv \\\n",
    "        \t-a ${agf_tool} \\\n",
    "        \t-db /stereoseq/minhui/ref/agfusion.homo_sapiens.95.db \\\n",
    "        \t-o ${workdir}/${cohort}/sample_${ID}/agf_sample_${ID}_${toool} \\\n",
    "        \t--middlestar \\\n",
    "        \t--noncanonical >> ${workdir}/${cohort}/sample_${ID}/sample_${ID}_${toool}.log.txt 2>&1\n",
    "\n",
    "        \t#THEN PVF\n",
    "        \techo -e \"\\n\\n ------------ pVacFuse ------------ \" >> ${workdir}/${cohort}/sample_${ID}/sample_${ID}_${toool}.log.txt 2>&1\n",
    "        \techo -e \"$(date | awk '{print $3\"/\"$2\"/\"$NF\"_\"$4}')\\n\\n\" >> ${workdir}/${cohort}/sample_${ID}/sample_${ID}_${toool}.log.txt 2>&1\n",
    "        \tdocker run -v ${workdir}/${cohort}/sample_${ID}:/home/MY_STUFF/ griffithlab/pvactools:4.0.4 pvacfuse run \\\n",
    "        \t-a sample_name -e1 8,9,10 -e2 15 --iedb-install-directory /opt/iedb -k -t 8 \\\n",
    "        \t/home/MY_STUFF/agf_sample_${ID}_${toool} pvf_sample_${ID}_${toool} \\\n",
    "        \t${ALLELES} NetMHCpan NetMHCIIpan /home/MY_STUFF/pvf_sample_${ID}_${toool} >> ${workdir}/${cohort}/sample_${ID}/sample_${ID}_${toool}.log.txt 2>&1\n",
    "\n",
    "\t\techo \"\"LINE_${NUM} SAMPLE_${ID} ${tool}: DONE  $(date | awk '{print $3\"/\"$2\"/\"$NF\"_\"$4}')\" >> ${workdir}/${cohort}_agfPVF.log.txt\n",
    "\n",
    "\telse\n",
    "\t\techo \"NATIVE FUSION FILE FOR ${cohort} ${ID_long} ${tool} unavailable. Run terminated $(date | awk '{print $3\"/\"$2\"/\"$NF\"_\"$4}')\" >> ${workdir}/${cohort}_agfPVF.failed.txt\n",
    "\tfi\n",
    "done\n",
    "\n",
    "s3dir=\"s3://crm.tumorstudy.analysis/NTUHxCRMY/${cohort,,}-run/SampleSpecificNeoantigenPrediction\"\n",
    "aws s3 cp ${workdir}/${cohort}/sample_${ID}/ ${s3dir}/sample_${ID}/ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787fe87b",
   "metadata": {},
   "source": [
    "### (NOW WE ANALYSE THE FUSIONS WHILE NEOANTIGEN PREDICTION IS RUNNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734abd75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19b42cae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d53f45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "beaac352",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289e528c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e15d6b49",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
