{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d369e7c",
   "metadata": {},
   "source": [
    "# Download + rename raw fusion output files of Arriba & FusionCatcher from S3 to local \n",
    "# This is a bash script / bash notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f141834",
   "metadata": {},
   "source": [
    "### some pre-defined environment / global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77d8384",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3=\"s3://crm.tumorstudy.analysis\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b8853d",
   "metadata": {},
   "source": [
    "### (DOWNLOAD TCGA NORMALS X113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1c2f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: create the folders\n",
    "for cohort in TCGA-Normals; do\n",
    "    mkdir ${cohort}\n",
    "    mkdir ${cohort}/Arriba\n",
    "    mkdir ${cohort}/FusionCatcher\n",
    "    mkdir ${cohort}/logs\n",
    "done\n",
    "\n",
    "# Step 2: download + rename the Arriba and FusionCatcher raw fusion calls and log files \n",
    "# The code in this step was saved as thisscript.sh and \n",
    "# This step is executed with parallel as such: parallel -j 10 bash thisscript.sh ::: $(seq 113)\n",
    "if [[ -f FINAL_NORMALS_UUID_fileID_fileName.txt ]]\n",
    "then\n",
    "    while read line\n",
    "    do\n",
    "        ID=$(echo $line | awk '{print $3}')\n",
    "        aws s3 cp $S3/tcga-run/FT_Detection/RNAseq/from_FASTQfromGenomicBam_TissueNormal/Arriba/${ID}_arr_outdir/arriba-fusions.tsv Arriba/arr_${ID}.tsv\n",
    "        aws s3 cp $S3/tcga-run/FT_Detection/RNAseq/from_FASTQfromGenomicBam_TissueNormal/FusionCatcher/${ID}_fc_outdir/final-list_candidate-fusion-genes.txt FusionCatcher/fc_${ID}.tsv\n",
    "    done < FINAL_NORMALS_UUID_fileID_fileName.txt\n",
    "fi\n",
    "\n",
    "# Step 3: Extract the fusion calls by gene-pair name only\n",
    "# (you should get a total of N=2841 unique fusions)\n",
    "tail -qn +2 TCGA-Normals/*/*tsv | awk '{print $1\"_\"$2}' | sort -n | uniq -c | sort -r | awk '{print $1\"\\t\"$2}' >> normalFTs_withFreq.txt\n",
    "tail -qn +2 TCGA-Normals/*/*tsv | awk '{print $1\"_\"$2}' | sort -n | uniq -c | sort -r | awk '{print $2}' >> normalFTs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e5fa00",
   "metadata": {},
   "source": [
    "### (DOWNLOAD TCGA,NTUH,MYBRCA tumour FTs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eca218",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cohort in TCGA MyBrCa NTUH; do\n",
    "    mkdir ${cohort}\n",
    "    mkdir ${cohort}/Arriba\n",
    "    mkdir ${cohort}/FusionCatcher\n",
    "    mkdir ${cohort}/logs\n",
    "    mkdir ${cohort}/Arriba/native-fusion-files\n",
    "    mkdir ${cohort}/FusionCatcher/native-fusion-files\n",
    "    mkdir ${cohort}/Arriba/native-fusion-files-filtered\n",
    "    mkdir ${cohort}/FusionCatcher/native-fusion-files-filtered\n",
    "    mkdir ${cohort}/Arriba/genepair-files\n",
    "    mkdir ${cohort}/FusionCatcher/genepair-files\n",
    "    mkdir ${cohort}/Arriba/genepair-files-filtered\n",
    "    mkdir ${cohort}/FusionCatcher/genepair-files-filtered\n",
    "done\n",
    "\n",
    "#for each of the following steps, cd into the cohort directory before proceeding\n",
    "TCGA-tumor: \n",
    "S3=\"s3://crm.tumorstudy.analysis\"\n",
    "NUM=$1\n",
    "ID=$(sed -n ${NUM}p FINAL_genomicBAM_UUID_fileID_fileName.txt | awk '{print $1}')\n",
    "aws s3 cp $S3/tcga-run/FT_Detection/RNAseq/from_FASTQfromGenomicBam/Arriba/${ID}_arr_outdir/arriba-fusions.tsv Arriba/arr_${ID}.tsv\n",
    "aws s3 cp $S3/tcga-run/FT_Detection/RNAseq/from_FASTQfromGenomicBam/FusionCatcher/${ID}_fc_outdir/final-list_candidate-fusion-genes.txt FusionCatcher/fc_${ID}.tsv\n",
    "aws s3 cp $S3/tcga-run/FT_Detection/RNAseq/from_FASTQfromGenomicBam/Arriba/${ID}_arr_outdir/arr_${ID}.log.txt logs/\n",
    "aws s3 cp $S3/tcga-run/FT_Detection/RNAseq/from_FASTQfromGenomicBam/FusionCatcher/${ID}_fc_outdir/fc_${ID}.log.txt logs/\n",
    "\n",
    "MYBRCA-tumor:\n",
    "aws s3 cp $S3/mybrca-run/FT_Detection/Arriba/ALL-MYBRCA-SAMPLES.txt .\n",
    "S3=\"s3://crm.tumorstudy.analysis\"\n",
    "NUM=$1\n",
    "ID=$(sed -n ${NUM}p ALL-MYBRCA-SAMPLES.txt)\n",
    "aws s3 cp $S3/mybrca-run/FT_Detection/Arriba/${ID}T_arr_outdir/arriba-fusions.tsv Arriba/native-fusion-files/arr_${ID}.tsv\n",
    "aws s3 cp $S3/mybrca-run/FT_Detection/Arriba/${ID}T_arr_outdir/arr${ID}T.log.txt logs/\n",
    "aws s3 cp $S3/mybrca-run/FT_Detection/FusionCatcher/${ID}T_fc_outdir/final-list_candidate-fusion-genes.txt FusionCatcher/native-fusion-files/fc_${ID}.tsv\n",
    "aws s3 cp $S3/mybrca-run/FT_Detection/FusionCatcher/${ID}T_fc_outdir/fc-mybrca${ID}.log.txt logs/\n",
    "\n",
    "NTUH-tumor: (Arriba - all complete. FusionCatcher: only 200+ are completed)\n",
    "aws s3 cp $S3/ntuh-run/FT_Detection/NTUH_ID_all500here.txt .\n",
    "S3=\"s3://crm.tumorstudy.analysis\"\n",
    "NUM=$1\n",
    "short=$(sed -n ${NUM}p NTUH_ID_all500here.txt | awk '{print $2}')\n",
    "ID=$(sed -n ${NUM}p NTUH_ID_all500here.txt | awk '{print $3}')\n",
    "aws s3 cp $S3/ntuh-run/FT_Detection/Arriba/${ID}_arr_outdir/arriba-fusions.tsv Arriba/native-fusion-files/arr_${short}.tsv\n",
    "aws s3 cp $S3/ntuh-run/FT_Detection/Arriba/${ID}_arr_outdir/arr_${ID}.log.txt logs/arr_${short}.log.txt\n",
    "aws s3 cp $S3/ntuh-run/FT_Detection/FusionCatcher/${ID}_fc_outdir/final-list_candidate-fusion-genes.txt FusionCatcher/native-fusion-files/fc_${short}.tsv\n",
    "aws s3 cp $S3/ntuh-run/FT_Detection/FusionCatcher/${ID}_fc_outdir/fc_${ID}.log.txt logs/fc_${short}.log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e141c3",
   "metadata": {},
   "source": [
    "### (FILTER OUT NORMALS, EXTRACT GENE-PAIR FILE FOR BOTH B4 AND AFTER NORMAL FILTERING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c6648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want to make sure that samples with 0 output still have a file, just with no content!\n",
    "#the sciript below was saved as a .sh file, then executed with the command below\n",
    "#parallel -j 10 bash script-below.sh ::: MYBRCA $(seq 990)\n",
    "\n",
    "cohort=$1 #first command line argument when executing the script below\n",
    "NUM=$2 #second command line argument when executing the script below - used with parallel command to process multiple samples at once\n",
    "ID=\"\"\n",
    "if [[ $cohort == \"TCGA\" ]]; then\n",
    "    ID=$(sed -n ${NUM}p TCGA/FINAL_genomicBAM_UUID_fileID_fileName.txt | awk '{print $1}')\n",
    "elif [[ $cohort == \"MyBrCa\" ]]; then\n",
    "    ID=$(sed -n ${NUM}p MyBrCa/ALL-MYBRCA-SAMPLES.txt)\n",
    "elif [[ $cohort == \"NTUH\" ]]; then\n",
    "    #ID=$(sed -n ${NUM}p NTUH/NTUH_ID_all500here.txt | awk '{print $2}') #short ID\n",
    "    ID=$(sed -n ${NUM}p NTUH/NTUH_ID_223only.txt)\n",
    "fi\n",
    "\n",
    "for tool in Arriba FusionCatcher; do\n",
    "    if [[ $tool == \"Arriba\" ]]; then \n",
    "        toool=\"arr\"\n",
    "    elif [[ $tool == \"FusionCatcher\" ]]; then\n",
    "        toool=\"fc\"\n",
    "    fi\n",
    "\n",
    "    #-----------------------------------------------------\n",
    "    #ONE: extract unfiltered gene-pair (removing dups) of each sample\n",
    "    awk '{print $1\"_\"$2}' ${cohort}/${tool}/native-fusion-files/${toool}_${ID}.tsv | tail -n +2 | sort -n | uniq >> ${cohort}/${tool}/genepair-files/${toool}_${ID}.txt\n",
    "\n",
    "    #-----------------------------------------------------\n",
    "    #TWO: filter fusion-files files based on TCGA normals\n",
    "\n",
    "    #but first let's clean up those native-fusion-files to handle consecutive blank cells\n",
    "    sed 's/\\t\\t/\\t.\\t/g' ${cohort}/${tool}/native-fusion-files/${toool}_${ID}.tsv | sed 's/\\t\\t/\\t.\\t/g' >> ${cohort}/${tool}/native-fusion-files-clean/${toool}_${ID}_clean.tsv\n",
    "\n",
    "    #now let's extract the fusion that are neither in normals nor a self-fusion\n",
    "    head -1 ${cohort}/${tool}/native-fusion-files/${toool}_${ID}.tsv > ${cohort}/${tool}/native-fusion-files-filtered/${toool}_${ID}_filtered.tsv\n",
    "\n",
    "    while read line; do\n",
    "        gene1=$(echo $line | awk '{print $1}')\n",
    "        gene2=$(echo $line | awk '{print $2}')\n",
    "        genepair=$(echo $line | awk '{print $1\"_\"$2}')\n",
    "        if [[ ! $(grep $genepair TCGA-normals/normalFTs.txt) && $gene1 != $gene2 ]]; then\n",
    "            echo $line | sed 's/ /\\t/g' >> ${cohort}/${tool}/native-fusion-files-filtered/${toool}_${ID}_filtered.tsv\n",
    "        fi\n",
    "    done <<< $(tail -n +2 ${cohort}/${tool}/native-fusion-files-clean/${toool}_${ID}_clean.tsv)\n",
    "\n",
    "    #-----------------------------------------------------\n",
    "    #THREE: extract filtered gene-pair from filtered-native files (remove dups also)\n",
    "    awk '{print $1\"_\"$2}' ${cohort}/${tool}/native-fusion-files-filtered/${toool}_${ID}_filtered.tsv | tail -n +2 | sort -n | uniq >> ${cohort}/${tool}/genepair-files-filtered/${toool}_${ID}_filtered.txt\n",
    "\n",
    "    #ALSO try to filter gene-pair based on TCGA normals. hopefully we will get the exact same output \n",
    "    #(YES we got the same output - will stick to the first one as this does not produce output if 0 FT)\n",
    "    #while read genepair; do\n",
    "    #\tif [[ ! $(grep $genepair TCGA-normals/normalFTs.txt) ]]; then\n",
    "    #\t\techo $genepair >> ${cohort}/${tool}/genepair-files-filtered/${toool}_${ID}_filteredv2.txt\n",
    "    #\tfi\n",
    "    #done < ${cohort}/${tool}/genepair-files/${toool}_${ID}.txt\n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d242507c",
   "metadata": {},
   "source": [
    "### (NOW WE ANALYSE THE FUSIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeaa10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##mkdir for combined fusion analysis\n",
    "\n",
    "for cohort in TCGA NTUH MyBrCa; do\n",
    "    mkdir ${cohort}/CombinedBothTools\n",
    "    mkdir ${cohort}/CombinedBothTools/genepair-files\n",
    "    mkdir ${cohort}/CombinedBothTools/genepair-files-filtered\n",
    "done\n",
    "\n",
    "##combine the gene-pair files before and after filtering\n",
    "cohort=$1\n",
    "NUM=$2\n",
    "ID=\"\"\n",
    "if [[ $cohort == \"TCGA\" ]]; then\n",
    "    ID=$(sed -n ${NUM}p TCGA/FINAL_genomicBAM_UUID_fileID_fileName.txt | awk '{print $1}')\n",
    "elif [[ $cohort == \"MyBrCa\" ]]; then\n",
    "    ID=$(sed -n ${NUM}p MyBrCa/ALL-MYBRCA-SAMPLES.txt)\n",
    "elif [[ $cohort == \"NTUH\" ]]; then\n",
    "    #ID=$(sed -n ${NUM}p NTUH/NTUH_ID_all500here.txt | awk '{print $2}') #short ID\n",
    "    ID=$(sed -n ${NUM}p NTUH/NTUH_ID_223only.txt)\n",
    "fi\n",
    "\n",
    "\n",
    "cat ${cohort}/Arriba/genepair-files/arr_${ID}.txt ${cohort}/FusionCatcher/genepair-files/fc_${ID}.txt | sort -n | uniq >> ${cohort}/CombinedBothTools/genepair-files/combined_${ID}.txt\n",
    "cat ${cohort}/Arriba/genepair-files-filtered/arr_${ID}_filtered.txt ${cohort}/FusionCatcher/genepair-files-filtered/fc_${ID}_filtered.txt | sort -n | uniq >> ${cohort}/CombinedBothTools/genepair-files-filtered/combined_${ID}_filtered.txt\n",
    "\n",
    "\n",
    "##overview of fusion counts - for filling in the 3 flow chart in a slide\n",
    "for cohort in TCGA NTUH MyBrCa; do\n",
    "    echo \"---------------- COHORT: ${cohort^^} \"----------------\"\n",
    "    cat ${cohort}/Arriba/genepair-files/* | sort -n | uniq | wc -l | while read count; do echo \"Arriba gross total: ${count}\"; done\n",
    "    cat ${cohort}/FusionCatcher/genepair-files/* | sort -n | uniq | wc -l | while read count; do echo \"FusionCatcher gross total: ${count}\"; done\n",
    "    cat ${cohort}/CombinedBothTools/genepair-files/* | sort -n | uniq | wc -l | while read count; do echo \"Unique FT count: ${count}\"; done\n",
    "    cat ${cohort}/CombinedBothTools/genepair-files-filtered/* | sort -n | uniq | wc -l | while read count; do echo \"Filtered unique FT count: ${count}\"; done\n",
    "    cat ${cohort}/CombinedBothTools/genepair-files-filtered/* | sort -n | uniq -c | awk '{print $1,$2}' | sort -rn | grep -c \"^1[[:blank:]]\" | while read count; do echo \"Freq=1 N= ${count}\"; done\n",
    "    cat ${cohort}/CombinedBothTools/genepair-files-filtered/* | sort -n | uniq -c | awk '{print $1,$2}' | sort -rn | grep -cv \"^1[[:blank:]]\" | while read count; do echo \"Freq>1 N= ${count}\"; done\n",
    "    echo \" \"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebecd0f8",
   "metadata": {},
   "source": [
    "### overview of fusioncount per sample - sanity check for FT count distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ea9d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##use bash to gather raw counts, use python to bin + calculate frequency distribution using interval of 50\n",
    "for cohort in TCGA NTUH MyBrCa; do\n",
    "    wc -l ${cohort}/CombinedBothTools/genepair-files/*txt | sed '$d' | awk '{print $1}' | sort -n >> subanalysis/${cohort}_FTcountPerSample_beforeFilter.txt\n",
    "    wc -l ${cohort}/CombinedBothTools/genepair-files-filtered/*txt | sed '$d' | awk '{print $1}' | sort -n >> subanalysis/${cohort}_FTcountPerSample_afterFilter.txt\n",
    "done\n",
    "\n",
    "## cd to subanalysis\n",
    "wc -l *\n",
    "  990 MyBrCa_FTcountPerSample_afterFilter.txt\n",
    "  990 MyBrCa_FTcountPerSample_beforeFilter.txt\n",
    "  223 NTUH_FTcountPerSample_afterFilter.txt\n",
    "  223 NTUH_FTcountPerSample_beforeFilter.txt\n",
    " 1095 TCGA_FTcountPerSample_afterFilter.txt\n",
    " 1095 TCGA_FTcountPerSample_beforeFilter.txt\n",
    " 4616 total\n",
    "\n",
    "\n",
    "(FREQ DISTRO OF ALL UNIQUE+FILTERED FTS)\n",
    "##python script to handle them at once\n",
    "#dependency: these 6 files countaining raw FT counts in the same directory as the script file.\n",
    "#MyBrCa_FTcountPerSample_afterFilter.txt,MyBrCa_FTcountPerSample_beforeFilter.txt,NTUH_FTcountPerSample_afterFilter.txt,NTUH_FTcountPerSample_beforeFilter.txt,TCGA_FTcountPerSample_afterFilter.txt,TCGA_FTcountPerSample_beforeFilter.txt\n",
    "\n",
    "##generate freq distro able to plot histogram in excel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cohorts=[\"MyBrCa\",\"NTUH\",\"TCGA\"]\n",
    "dynamic_list={}\n",
    "dynamic_total={\"MyBrCa\":990,\"NTUH\":223,\"TCGA\":1095}\n",
    "\n",
    "\n",
    "for cohort in cohorts:\n",
    "\tdynamic_list[f'{cohort}'] = []\n",
    "\tFILENAME= cohort + \"_FTcountPerSample_afterFilter.txt\"\n",
    "\twith open(f'{FILENAME}','r') as file:\n",
    "\t\tdynamic_list[f'{cohort}'] = [int(line.strip()) for line in file]\n",
    "\t\t\n",
    "\n",
    "#bins=[1,51,101,151,201,251,301,351,401,451,501,551,601,651,701,751,801,851,901,951,1001,1051,1101,1151,1201,1251,1300]\n",
    "\n",
    "#use np function to bin the numerical values\n",
    "#MA , _ = np.histogram(dynamic_list[\"MyBrCa_after\"], bins)\n",
    "#NA , _ = np.histogram(dynamic_list[\"NTUH_after\"], bins)\n",
    "#TA , bin_edges = np.histogram(dynamic_list[\"TCGA_after\"], bins)\t\n",
    "\n",
    "##set up freq_table\n",
    "#freq_table = pd.DataFrame({'Interval': [(bin_edges[i],bin_edges[i+1]) for i in range(len(bin_edges) - 1)], 'MyBrCa_After': MA, 'NTUH_After': NA, 'TCGA_After': TA})\n",
    "\n",
    "#freq_table['Interval'] = [\"1-50\",\"51-100\",\"101-150\",\"151-200\",\"201-250\",\"251-300\",\"301-350\",\"351-400\",\"401-450\",\"451-500\",\"501-550\",\"551-600\",\"601-650\",\"651-700\",\"701-750\",\"751-800\",\"801-850\",\"851-900\",\"901-950\",\"951-1000\",\"1001-1050\",\"1051-1100\",\"1101-1150\",\"1151-1200\",\"1201-1250\",\"1251-1300\"]\n",
    "\n",
    "#freq_table.to_csv('FTcount_FreqDistro_allCohort.tsv',sep='\\t',index=False)\n",
    "\n",
    "\n",
    "##now try to make them into density plot with smooth edges\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cohorts=[\"MyBrCa\",\"NTUH\",\"TCGA\"]\n",
    "dynamic_list={}\n",
    "dynamic_total={\"MyBrCa\":990,\"NTUH\":223,\"TCGA\":1095}\n",
    "dynamic_color={\"MyBrCa\":\"#E8A2D1\",\"NTUH\":\"#D7ED9E\",\"TCGA\":\"#B3E3ED\"}\n",
    "\n",
    "\n",
    "for cohort in cohorts:\n",
    "\tdynamic_list[f'{cohort}'] = []\n",
    "\tFILENAME= cohort + \"_FTcountPerSample_afterFilter.txt\"\n",
    "\twith open(f'{FILENAME}','r') as file:\n",
    "\t\tdynamic_list[f'{cohort}'] = [int(line.strip()) for line in file]\n",
    "\t\t#dynamic_list[f'{cohort}'] = [line/dynamic_total[f'{cohort}'] for line in dynamic_list[f'{cohort}']]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for cohort in cohorts:\n",
    "\tsns.kdeplot(dynamic_list[f'{cohort}'], fill=True, label=f'{cohort}', color=dynamic_color[f'{cohort}'], ax=ax)\n",
    "\n",
    "ax.set_xlabel('No. of Fusion Transcript per Sample')\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend()\n",
    "plt.title(\"Distribution of fusion transcript count per sample\")\n",
    "plt.savefig('density.png')\n",
    "\n",
    "\n",
    "##now make them into violin plot\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "dynamic_list={}\n",
    "dynamic_total={\"MyBrCa\":990,\"NTUH\":223,\"TCGA\":1095}\n",
    "dynamic_color={\"MyBrCa\":\"#E8A2D1\",\"NTUH\":\"#D7ED9E\",\"TCGA\":\"#B3E3ED\"}\n",
    "data=[] # Create a list of data and labels for the x-axis\n",
    "cohorts=[\"MyBrCa\",\"NTUH\",\"TCGA\"]\n",
    "\n",
    "for cohort in cohorts:\n",
    "\tdynamic_list[f'{cohort}'] = []\n",
    "\tFILENAME= cohort + \"_FTcountPerSample_afterFilter.txt\"\n",
    "\twith open(f'{FILENAME}','r') as file:\n",
    "\t\tdynamic_list[f'{cohort}'] = [int(line.strip()) for line in file]\n",
    "\t\n",
    "\tdata.append(dynamic_list[f'{cohort}'])\n",
    "\n",
    "mycolors = [\"#E8A2D1\",\"#D7ED9E\",\"#B3E3ED\"]  # Define colors for the violin plots\n",
    "\n",
    "\n",
    "sns.violinplot(data=data, palette=mycolors) # Create a violin plot\n",
    "\n",
    "plt.xlabel(\"Cohort\") # Set labels and title\n",
    "plt.ylabel(\"No. of Fusion Transcript/ Sample\")\n",
    "plt.title(\"Distribution of fusion transcript count per sample\")\n",
    "plt.xticks(range(len(cohorts)), cohorts) # Specify x-axis labels\n",
    "plt.savefig('violin.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b734d8",
   "metadata": {},
   "source": [
    "### (FREQ DISTRO OF ONLY THE RECURRENT (IE. FREQ>1) FTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089b3ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2934,5139,4491\n",
    "\n",
    "for cohort in TCGA NTUH MyBrCa; do\n",
    "#cat ${cohort}/CombinedBothTools/genepair-files-filtered/*txt | sort -n | uniq -c | awk '{print $1,$2}' | sort -rn | grep -v \"^1[[:blank:]]\" | awk '{print $NF}' >> subanalysis/2_FreqDistro_RecurrentFT/${cohort}_recurrentFTs.txt\n",
    "cat ${cohort}/CombinedBothTools/genepair-files-filtered/*txt | sort -n | uniq -c | awk '{print $1,$2}' | sort -rn | awk '{print $NF}' >> subanalysis/2_FreqDistro_RecurrentFT/${cohort}_allFTs.txt\n",
    "done\n",
    "\n",
    "for cohort in TCGA NTUH MyBrCa; do\n",
    "mkdir ${cohort}/CombinedBothTools/genepair-files-filtered-recurrent/\n",
    "done\n",
    "\n",
    "#generate genepair-file for only the recurrent FTs\n",
    "cohort=$1\n",
    "NUM=$2\n",
    "ID=\"\"\n",
    "if [[ $cohort == \"TCGA\" ]]; then\n",
    "\tID=$(sed -n ${NUM}p TCGA/FINAL_genomicBAM_UUID_fileID_fileName.txt | awk '{print $1}')\n",
    "elif [[ $cohort == \"MyBrCa\" ]]; then\n",
    "\tID=$(sed -n ${NUM}p MyBrCa/ALL-MYBRCA-SAMPLES.txt)\n",
    "elif [[ $cohort == \"NTUH\" ]]; then\n",
    "\t#ID=$(sed -n ${NUM}p NTUH/NTUH_ID_all500here.txt | awk '{print $2}') #short ID\n",
    "\tID=$(sed -n ${NUM}p NTUH/NTUH_ID_223only.txt)\n",
    "fi\n",
    "\n",
    "while read fusion; do\n",
    "if [[ $(grep \"^$fusion$\" subanalysis/2_FreqDistro_RecurrentFT/${cohort}_recurrentFTs.txt) ]]; then\n",
    "\techo $fusion >> ${cohort}/CombinedBothTools/genepair-files-filtered-recurrent/recurrent_${ID}.txt\n",
    "fi\n",
    "done < ${cohort}/CombinedBothTools/genepair-files-filtered/combined_${ID}_filtered.txt\n",
    "\n",
    "\n",
    "#now generate a list of counts per sample based on the recurrent.txt\n",
    "for cohort in TCGA NTUH MyBrCa; do\n",
    "wc -l ${cohort}/CombinedBothTools/genepair-files-filtered-recurrent/* | sed '$d' | awk '{print $1}' | sort -n >> subanalysis/2_FreqDistro_RecurrentFT/${cohort}_recurrentFTcountPerSample.txt\n",
    "done\n",
    "\n",
    "#now check their freq distro in python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "cohorts=[\"MyBrCa\",\"NTUH\",\"TCGA\"]\n",
    "\n",
    "dynamic_list={}\n",
    "\n",
    "#use for loop to dynamically import FT count into corresponding lists\n",
    "for cohort in cohorts:\n",
    "\tdynamic_list[f'{cohort}'] = []\n",
    "\tFILENAME= cohort + \"_recurrentFTcountPerSample.txt\"\n",
    "\twith open(f'{FILENAME}','r') as file:\n",
    "\t\tdynamic_list[f'{cohort}'] = [int(line.strip()) for line in file]\n",
    "\n",
    "#bins=[1,21,41,61,81,101,201,301,401,600]\n",
    "bins=[1,51,101,151,201,251,301,351,401,451,700]\n",
    "\n",
    "#use np function to bin the numerical values\n",
    "MR , _ = np.histogram(dynamic_list[\"MyBrCa\"], bins)\n",
    "NR , _ = np.histogram(dynamic_list[\"NTUH\"], bins)\n",
    "TR , bin_edges = np.histogram(dynamic_list[\"TCGA\"], bins)\t\n",
    "\n",
    "##set up freq_table\n",
    "freq_table = pd.DataFrame({'Interval': [(bin_edges[i],bin_edges[i+1]) for i in range(len(bin_edges) - 1)], 'MyBrCa': MR, 'NTUH': NR, 'TCGA': TR})\n",
    "\n",
    "#freq_table['Interval'] = [\"1-20\",\"21-40\",\"41-60\",\"61-80\",\"81-100\",\"101-200\",\"201-300\",\"301-400\",\"401-600\"]\n",
    "freq_table['Interval'] = [\"1-50\",\"51-100\",\"101-150\",\"151-200\",\"201-250\",\"251-300\",\"301-350\",\"351-400\",\"401-450\",\"450+\"]\n",
    "\n",
    "freq_table.to_csv('RecurrentFTcount_FreqDistro_allCohort_V3.tsv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7d7557",
   "metadata": {},
   "source": [
    "### (download neoantigens for analysis - on my laptop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ea7488",
   "metadata": {},
   "outputs": [],
   "source": [
    "####download filtered.tsv\n",
    "#mkdir ${cohort}/pvf_output\n",
    "cohort=$1\n",
    "NUM=$2\n",
    "ID=\"\"\n",
    "if [[ $cohort == \"TCGA\" ]]; then\n",
    "\t#ID=$(sed -n ${NUM}p TCGA/FINAL_genomicBAM_UUID_fileID_fileName.txt | awk '{print $1}')\n",
    "\tID=$(sed -n ${NUM}p TCGA/TCGA_patient-specific-pvfAlleles.txt | awk '{print $1}')\n",
    "elif [[ $cohort == \"MyBrCa\" ]]; then\n",
    "\t#ID=$(sed -n ${NUM}p MyBrCa/ALL-MYBRCA-SAMPLES.txt)\n",
    "\tID=$(sed -n ${NUM}p MyBrCa/MyBrCa_patient-specific-pvfAlleles.txt | awk '{print $1}')\n",
    "elif [[ $cohort == \"NTUH\" ]]; then\n",
    "\t#ID=$(sed -n ${NUM}p NTUH/NTUH_ID_all500here.txt | awk '{print $2}') #short ID\n",
    "\tID=$(sed -n ${NUM}p NTUH/NTUH_patient-specific-pvfAlleles.txt | awk '{print $1}')\n",
    "fi\n",
    "\n",
    "s3dir=\"s3://crm.tumorstudy.analysis/NTUHxCRMY/${cohort,,}-run/SampleSpecificNeoantigenPrediction\"\n",
    "for tool in arr fc; do\n",
    "    aws s3 cp ${s3dir}/sample_${ID}/pvf_sample_${ID}_${tool}/combined/pvf_sample_${ID}_${tool}.filtered.tsv ${cohort}/pvf_output/\n",
    "done\n",
    "\n",
    "\n",
    "####clean up the spaces for easy formating + extract uniq peptide from each file\n",
    "#${cohort}/unique_peptide\n",
    "cohort=$1\n",
    "NUM=$2\n",
    "ID=\"\"\n",
    "if [[ $cohort == \"TCGA\" ]]; then\n",
    "\t#ID=$(sed -n ${NUM}p TCGA/FINAL_genomicBAM_UUID_fileID_fileName.txt | awk '{print $1}')\n",
    "\tID=$(sed -n ${NUM}p TCGA/TCGA_patient-specific-pvfAlleles.txt | awk '{print $1}')\n",
    "elif [[ $cohort == \"MyBrCa\" ]]; then\n",
    "\t#ID=$(sed -n ${NUM}p MyBrCa/ALL-MYBRCA-SAMPLES.txt)\n",
    "\tID=$(sed -n ${NUM}p MyBrCa/MyBrCa_patient-specific-pvfAlleles.txt | awk '{print $1}')\n",
    "elif [[ $cohort == \"NTUH\" ]]; then\n",
    "\t#ID=$(sed -n ${NUM}p NTUH/NTUH_ID_all500here.txt | awk '{print $2}') #short ID\n",
    "\tID=$(sed -n ${NUM}p NTUH/NTUH_patient-specific-pvfAlleles.txt | awk '{print $1}')\n",
    "fi\n",
    "\n",
    "\n",
    "tail -qn +2 ${cohort}/pvf_output/pvf_sample_${ID}_*.filtered.tsv | sed 's/ //g' | awk '{print $10}'| sort -n | uniq >> ${cohort}/unique_peptide/uniq_${ID}.txt\n",
    "\n",
    "#rm */unique_peptide/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef93059",
   "metadata": {},
   "source": [
    "### (this one need redo)\n",
    "### count total uniq peptide per cohort (this is for concordance analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906a3a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cohort in TCGA NTUH MyBrCa; do \n",
    "    #cat ${cohort}/unique_peptide/* | sort -n | uniq | wc -l | while read count; do echo \"${cohort} unique peptide count: ${count}\"; done\n",
    "    cat ${cohort}/unique_peptide/* | sort -n | uniq >> ${cohort}/${cohort}_ALLunique_peptides.txt\n",
    "done\n",
    "\n",
    "##check freq distro of peptide counts\n",
    "#now generate a list of counts per sample based on the recurrent.txt\n",
    "for cohort in TCGA NTUH MyBrCa; do\n",
    "    wc -l ${cohort}/unique_peptide/* | sed '$d' | awk '{print $1}' | sort -n >> subanalysis/4_FreqDistro_Allpeptide/${cohort}_PeptidecountPerSample.txt\n",
    "done\n",
    "\n",
    "#now check their freq distro in python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "cohorts=[\"MyBrCa\",\"NTUH\",\"TCGA\"]\n",
    "\n",
    "dynamic_list={}\n",
    "\n",
    "#use for loop to dynamically import FT count into corresponding lists\n",
    "for cohort in cohorts:\n",
    "\tdynamic_list[f'{cohort}'] = []\n",
    "\tFILENAME= cohort + \"_PeptidecountPerSample.txt\"\n",
    "\twith open(f'{FILENAME}','r') as file:\n",
    "\t\tdynamic_list[f'{cohort}'] = [int(line.strip()) for line in file]\n",
    "\n",
    "bins=[1,51,101,151,201,251,301,351,1100]\n",
    "\n",
    "#use np function to bin the numerical values\n",
    "MR , _ = np.histogram(dynamic_list[\"MyBrCa\"], bins)\n",
    "NR , _ = np.histogram(dynamic_list[\"NTUH\"], bins)\n",
    "TR , bin_edges = np.histogram(dynamic_list[\"TCGA\"], bins)\t\n",
    "\n",
    "##set up freq_table\n",
    "freq_table = pd.DataFrame({'Interval': [(bin_edges[i],bin_edges[i+1]) for i in range(len(bin_edges) - 1)], 'MyBrCa': MR, 'NTUH': NR, 'TCGA': TR})\n",
    "\n",
    "freq_table['Interval'] = [\"1-50\",\"51-100\",\"101-150\",\"151-200\",\"201-250\",\"251-300\",\"301-350\",\"351+\"]\n",
    "\n",
    "freq_table.to_csv('PeptideCount_FreqDistro_allCohort.tsv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce38ae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "##fusion concordance\n",
    "\n",
    "cohort=$1\n",
    "aws s3 cp $S3/tmp/analysis/${cohort}/CombinedBothTools/genepair-files-filtered/ ${cohort}/CombinedBothTools/genepair-files-filtered/ --recursive\n",
    "\n",
    "for cohort in TCGA NTUH MyBrCa; do \n",
    "    cat ${cohort}/CombinedBothTools/genepair-files-filtered/* | sort -n | uniq >> venn/${cohort}_ALLunique_FTs.txt\n",
    "done\n",
    "\n",
    "(i did it manually in python using set() and intersection())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665ba64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### recurrent peptide concordance\n",
    "\n",
    "for cohort in TCGA NTUH MyBrCa; do\n",
    "cat ${cohort}/unique_peptide/* | sort -n | uniq -c | awk '{print $1,$2}' | sort -rn | grep -v \"^1[[:blank:]]\" | awk '{print $NF}' >> subanalysis/5_FreqDistro_RecurrentPeptide/${cohort}_recurrentPeptides.txt\n",
    "done\n",
    "\n",
    "${cohort}_ALLunique_peptides.txt\n",
    "\n",
    "#grep recurrent peptides for freq distro\n",
    "cohort=$1\n",
    "NUM=$2\n",
    "ID=$(sed -n ${NUM}p ${cohort}/${cohort}_patient-specific-pvfAlleles.txt | awk '{print $1}')\n",
    "\n",
    "while read peptide; do\n",
    "if [[ $(grep \"^$peptide$\" subanalysis/5_FreqDistro_RecurrentPeptide/${cohort}_recurrentPeptides.txt) ]]; then\n",
    "\techo $peptide >> ${cohort}/unique_peptide_recurrent/recurrent_${ID}.txt\n",
    "fi\n",
    "done < ${cohort}/unique_peptide/uniq_${ID}.txt\n",
    "\n",
    "#now generate a list of counts per sample based on the recurrent.txt\n",
    "for cohort in TCGA NTUH MyBrCa; do\n",
    "wc -l ${cohort}/unique_peptide_recurrent/* | sed '$d' | awk '{print $1}' | sort -n >> subanalysis/5_FreqDistro_RecurrentPeptide/${cohort}_recurrentPeptidecountPerSample.txt\n",
    "done\n",
    "\n",
    "#now do freq distro using python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "cohorts=[\"MyBrCa\",\"NTUH\",\"TCGA\"]\n",
    "\n",
    "dynamic_list={}\n",
    "\n",
    "#use for loop to dynamically import FT count into corresponding lists\n",
    "for cohort in cohorts:\n",
    "\tdynamic_list[f'{cohort}'] = []\n",
    "\tFILENAME= cohort + \"_recurrentPeptidecountPerSample.txt\"\n",
    "\twith open(f'{FILENAME}','r') as file:\n",
    "\t\tdynamic_list[f'{cohort}'] = [int(line.strip()) for line in file]\n",
    "\n",
    "#bins=[1,21,41,61,81,101,121,141,161,181,200]\n",
    "bins=[1,51,101,151,201,251,301,351,1100]\n",
    "\n",
    "#use np function to bin the numerical values\n",
    "MR , _ = np.histogram(dynamic_list[\"MyBrCa\"], bins)\n",
    "NR , _ = np.histogram(dynamic_list[\"NTUH\"], bins)\n",
    "TR , bin_edges = np.histogram(dynamic_list[\"TCGA\"], bins)\t\n",
    "\n",
    "##set up freq_table\n",
    "freq_table = pd.DataFrame({'Interval': [(bin_edges[i],bin_edges[i+1]) for i in range(len(bin_edges) - 1)], 'MyBrCa': MR, 'NTUH': NR, 'TCGA': TR})\n",
    "\n",
    "#freq_table['Interval'] = [\"1-20\",\"21-40\",\"41-60\",\"61-80\",\"81-100\",\"101-120\",\"121-140\",\"141-160\",\"161-180\",\"181-200\"]\n",
    "freq_table['Interval'] = [\"1-50\",\"51-100\",\"101-150\",\"151-200\",\"201-250\",\"251-300\",\"301-350\",\"351+\"]\n",
    "\n",
    "freq_table.to_csv('RecurrentPeptideCount_FreqDistro_allCohort_V2.tsv',sep='\\t',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0581ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#also look at concordance of recurrent peptides\n",
    "cd subanalysis/5_FreqDistro_RecurrentPeptide\n",
    "\n",
    "#python code\n",
    "with open(f'MyBrCa_recurrentPeptides.txt','r') as file:\n",
    "\tmybrca = [line.strip() for line in file]\n",
    "\tmybrca = set(mybrca)\n",
    "\n",
    "with open(f'NTUH_recurrentPeptides.txt','r') as file:\n",
    "\tntuh = [line.strip() for line in file]\n",
    "\tntuh = set(ntuh)\n",
    "\n",
    "with open(f'TCGA_recurrentPeptides.txt','r') as file:\n",
    "\ttcga = [line.strip() for line in file]\n",
    "\ttcga = set(tcga)\n",
    "\n",
    "MxN=len(mybrca.intersection(ntuh))\n",
    "MxT=len(mybrca.intersection(tcga))\n",
    "TxN=len(tcga.intersection(ntuh))\n",
    "MxNxT=len(tcga.intersection(ntuh).intersection(mybrca))\n",
    "\n",
    "print (f\"MYBRCA x NTUH count: {MxN}\")\n",
    "print (f\"MYBRCA x TCGA count: {MxT}\")\n",
    "print (f\"  TCGA x NTUH count: {TxN}\")\n",
    "print (f\"MYBRCA x NTUH x TCGA count : {MxNxT}\")\n",
    "\n",
    "print (f\"Unique to MYBRCA: {len(mybrca)-MxN-(MxT-MxNxT)}\")\n",
    "print (f\"Unique to NTUH: {len(ntuh)-MxN-(TxN-MxNxT)}\")\n",
    "print (f\"Unique to TCGA: {len(tcga)-MxT-(TxN-MxNxT)}\")\n",
    "\n",
    "print (f\"MYBRCA x NTUH concordance rate: {MxN/(len(mybrca)+len(ntuh))}\")\n",
    "print (f\"MYBRCA x TCGA concordance rate: {MxT/(len(mybrca)+len(tcga))}\")\n",
    "print (f\"  TCGA x NTUH concordance rate: {TxN/(len(tcga)+len(ntuh))}\")\n",
    "\n",
    "\n",
    "#and concordance of recurrent FTs\n",
    "\n",
    "with open(f'MyBrCa_recurrentFTs.txt','r') as file:\n",
    "\tmybrca = [line.strip() for line in file]\n",
    "\tmybrca = set(mybrca)\n",
    "\n",
    "with open(f'NTUH_recurrentFTs.txt','r') as file:\n",
    "\tntuh = [line.strip() for line in file]\n",
    "\tntuh = set(ntuh)\n",
    "\n",
    "with open(f'TCGA_recurrentFTs.txt','r') as file:\n",
    "\ttcga = [line.strip() for line in file]\n",
    "\ttcga = set(tcga)\n",
    "\n",
    "MxN=len(mybrca.intersection(ntuh))\n",
    "MxT=len(mybrca.intersection(tcga))\n",
    "TxN=len(tcga.intersection(ntuh))\n",
    "MxNxT=len(tcga.intersection(ntuh).intersection(mybrca))\n",
    "\n",
    "print (f\"MYBRCA x NTUH count: {MxN}\")\n",
    "print (f\"MYBRCA x TCGA count: {MxT}\")\n",
    "print (f\"  TCGA x NTUH count: {TxN}\")\n",
    "print (f\"MYBRCA x NTUH x TCGA count : {MxNxT}\")\n",
    "\n",
    "print (f\"Unique to MYBRCA: {len(mybrca)-MxN-(MxT-MxNxT)}\")\n",
    "print (f\"Unique to NTUH: {len(ntuh)-MxN-(TxN-MxNxT)}\")\n",
    "print (f\"Unique to TCGA: {len(tcga)-MxT-(TxN-MxNxT)}\")\n",
    "\n",
    "print (f\"MYBRCA x NTUH concordance rate: {MxN/(len(mybrca)+len(ntuh))}\")\n",
    "print (f\"MYBRCA x TCGA concordance rate: {MxT/(len(mybrca)+len(tcga))}\")\n",
    "print (f\"  TCGA x NTUH concordance rate: {TxN/(len(tcga)+len(ntuh))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e049841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for FT & peptide\n",
    "mkdir subanalysis/6_FreqDistro_FreqofFTandPeptides/\n",
    "for cohort in TCGA NTUH MyBrCa; do\n",
    "    cat ${cohort}/CombinedBothTools/genepair-files-filtered/*txt | sort -n | uniq -c | awk '{print $1,$2}' | sort -rn  >> subanalysis/6_FreqDistro_FreqofFTandPeptides/${cohort}_FreqOfFTs.txt\n",
    "    cat ${cohort}/unique_peptide/* | sort -n | uniq -c | awk '{print $1,$2}' | sort -rn >> subanalysis/6_FreqDistro_FreqofFTandPeptides/${cohort}_FreqOfPeptides.txt\n",
    "done\n",
    "\n",
    "for cohort in MyBrCa NTUH TCGA; do \n",
    "    awk '{print $1}' ${cohort}_FreqOfFTs.txt >> ${cohort}_FreqOfFTs_onlyFreq.txt\n",
    "    awk '{print $1}' ${cohort}_FreqOfPeptides.txt >> ${cohort}_FreqOfPeptides_onlyFreq.txt\n",
    "done\n",
    "\n",
    "#bring this to python (for FTs)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "cohorts=[\"MyBrCa\",\"NTUH\",\"TCGA\"]\n",
    "\n",
    "dynamic_list={}\n",
    "for cohort in cohorts:\n",
    "\tdynamic_list[f'{cohort}'] = []\n",
    "\tFILENAME= cohort + \"_FreqOfFTs_onlyFreq.txt\"\n",
    "\twith open(f'{FILENAME}','r') as file:\n",
    "\t\tdynamic_list[f'{cohort}'] = [int(line.strip()) for line in file]\n",
    "\n",
    "bins=[0,2,21,41,61,81,101,121,141,161,181,201,1000]\n",
    "\n",
    "#use np function to bin the numerical values\n",
    "MR , _ = np.histogram(dynamic_list[\"MyBrCa\"], bins)\n",
    "NR , _ = np.histogram(dynamic_list[\"NTUH\"], bins)\n",
    "TR , bin_edges = np.histogram(dynamic_list[\"TCGA\"], bins)\t\n",
    "\n",
    "##set up freq_table\n",
    "freq_table = pd.DataFrame({'Interval': [(bin_edges[i],bin_edges[i+1]) for i in range(len(bin_edges) - 1)], 'MyBrCa': MR, 'NTUH': NR, 'TCGA': TR})\n",
    "\n",
    "freq_table['Interval'] = [\"1\",\"2-20\",\"21-40\",\"41-60\",\"61-80\",\"81-100\",\"101-120\",\"121-140\",\"141-160\",\"161-180\",\"181-200\",\"201+\"]\n",
    "\n",
    "freq_table.to_csv('FTs_FreqDistro_allCohort.tsv',sep='\\t',index=False)\n",
    "\n",
    "\n",
    "dynamic_list={}\n",
    "for cohort in cohorts:\n",
    "\tdynamic_list[f'{cohort}'] = []\n",
    "\tFILENAME= cohort + \"_FreqOfPeptides_onlyFreq.txt\"\n",
    "\twith open(f'{FILENAME}','r') as file:\n",
    "\t\tdynamic_list[f'{cohort}'] = [int(line.strip()) for line in file]\n",
    "\n",
    "#use np function to bin the numerical values\n",
    "MR , _ = np.histogram(dynamic_list[\"MyBrCa\"], bins)\n",
    "NR , _ = np.histogram(dynamic_list[\"NTUH\"], bins)\n",
    "TR , bin_edges = np.histogram(dynamic_list[\"TCGA\"], bins)\t\n",
    "\n",
    "##set up freq_table\n",
    "freq_table=pd.DataFrame()\n",
    "freq_table = pd.DataFrame({'Interval': [(bin_edges[i],bin_edges[i+1]) for i in range(len(bin_edges) - 1)], 'MyBrCa': MR, 'NTUH': NR, 'TCGA': TR})\n",
    "\n",
    "freq_table['Interval'] = [\"1\",\"2-20\",\"21-40\",\"41-60\",\"61-80\",\"81-100\",\"101-120\",\"121-140\",\"141-160\",\"161-180\",\"181-200\",\"201+\"]\n",
    "\n",
    "freq_table.to_csv('Peptides_FreqDistro_allCohort.tsv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe53006",
   "metadata": {},
   "outputs": [],
   "source": [
    "##freq distro of best scoring peptide within each sample\n",
    "\n",
    "####clean up the spaces for easy formating + extract uniq peptide from each file\n",
    "#${cohort}/unique_peptide\n",
    "cohort=$1\n",
    "NUM=$2\n",
    "ID=$(sed -n ${NUM}p ${cohort}/${cohort}_patient-specific-pvfAlleles.txt | awk '{print $1}')\n",
    "\n",
    "tail -qn +2 ${cohort}/pvf_output/pvf_sample_${ID}_*.filtered.tsv | sed 's/ //g' | awk '{print $12\"\\t\"$10}'| sort -n | head -1 >> ${cohort}/lowestIC50score_peptide/best_${ID}.txt\n",
    "\n",
    "\n",
    "##extract those best scores into subanalysis dir for freq distro of scores\n",
    "for cohort in TCGA NTUH MyBrCa; do\n",
    "awk '{print $1}' ${cohort}/lowestIC50score_peptide/* | sort -n >> subanalysis/7_FreqDistro_BestPeptideScore/${cohort}_bestScores.txt\n",
    "done\n",
    "\n",
    "#(do freq distro in python in a bit)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "cohorts=[\"MyBrCa\",\"NTUH\",\"TCGA\"]\n",
    "\n",
    "dynamic_list={}\n",
    "for cohort in cohorts:\n",
    "\tdynamic_list[f'{cohort}'] = []\n",
    "\tFILENAME= cohort + \"_bestScores.txt\"\n",
    "\twith open(f'{FILENAME}','r') as file:\n",
    "\t\tdynamic_list[f'{cohort}'] = [float(line.strip()) for line in file]\n",
    "\n",
    "bins=[1,21,41,61,81,101,201,301,401,500]\n",
    "\n",
    "#use np function to bin the numerical values\n",
    "MR , _ = np.histogram(dynamic_list[\"MyBrCa\"], bins)\n",
    "NR , _ = np.histogram(dynamic_list[\"NTUH\"], bins)\n",
    "TR , bin_edges = np.histogram(dynamic_list[\"TCGA\"], bins)\t\n",
    "\n",
    "##set up freq_table\n",
    "freq_table = pd.DataFrame({'Interval': [(bin_edges[i],bin_edges[i+1]) for i in range(len(bin_edges) - 1)], 'MyBrCa': MR, 'NTUH': NR, 'TCGA': TR})\n",
    "\n",
    "freq_table['Interval'] = [\"1-20\",\"21-40\",\"41-60\",\"61-80\",\"81-100\",\"101-200\",\"201-300\",\"301-400\",\"401-500\"]\n",
    "\n",
    "freq_table.to_csv('PeptideBestScore_FreqDistro_allCohort.tsv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e496059",
   "metadata": {},
   "outputs": [],
   "source": [
    "##also check the frequency of those best scoring peptides (highest freq is in the single digit only - this one can abort mission)\n",
    "for cohort in TCGA NTUH MyBrCa; do\n",
    "cat ${cohort}/lowestIC50score_peptide/* | sort -n | uniq -c | awk '{print $1,$2}' | sort -rn | head\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eb6db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##check total num of coding transcripts \n",
    "\n",
    "for cohort in TCGA NTUH MyBrCa; do\n",
    "ls ${cohort}/*/agf*/ | grep -v \"sample\" | grep -v '^$' | sed 's/\\-[0-9]*_/_/g' | sed 's/\\-[0-9]*$//g' | sort -n | uniq >> ${cohort}_CodingTranscripts.txt\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d6647c",
   "metadata": {},
   "outputs": [],
   "source": [
    "68.97 \n",
    "A030078\n",
    "##freq distro of median scoring peptide within each sample\n",
    "\n",
    "####clean up the spaces for easy formating + extract uniq peptide from each file\n",
    "#${cohort}/unique_peptide\n",
    "cohort=$1\n",
    "NUM=$2\n",
    "ID=$(sed -n ${NUM}p ${cohort}/${cohort}_patient-specific-pvfAlleles.txt | awk '{print $1}')\n",
    "\n",
    "tail -qn +2 ${cohort}/pvf_output/pvf_sample_${ID}_*.filtered.tsv | sed 's/ //g' | awk '{print $12\"\\t\"$10}'| sort -n >> ${cohort}/medianIC50score_peptide/all_${ID}.txt\n",
    "\n",
    "awk '{print $1}' ${cohort}/medianIC50score_peptide/all_${ID}.txt | awk '{a[i++]=$1;} END {if (i % 2 == 0) print (a[int(i/2)-1] + a[int(i/2)]) / 2; else print a[int(i/2)];}' >> ${cohort}/medianIC50score_peptide/median_${ID}.txt\n",
    "\n",
    "##extract those best scores into subanalysis dir for freq distro of scores\n",
    "for cohort in TCGA NTUH MyBrCa; do\n",
    "cat ${cohort}/medianIC50score_peptide/median_*.txt | sort -n >> subanalysis/8_FreqDistro_MedianPeptideScore/${cohort}_medianScores.txt\n",
    "done\n",
    "\n",
    "##COMMENT: the problem with this approach is that median score can be halved but peptide can't maybe we will resort to selecting the one halfway - if we do need to look into identity of median scoring peptide. can ignore for now.\n",
    "\n",
    "#execute density.py and violin.py for all cohorts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d8cc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "##ideas:\n",
    "(DONE) freq distro of all unique+filtered FTs (to see if any specific samples are contributing to most of the counts)\n",
    "(DONE) freq distro of 'recurrent, IE freq >1' FTs (to see if there is any difference - NTUH still way more :) )\n",
    "> freq distro of FT further seperated by clinical subtypes (how to use p-value test to check statistical significance?)\n",
    "(DONE) freq distro of neoantigen count per sample \n",
    "> also try to correct for num of alleles available in pvf; ratio of FT <-> neoantigen per sample\n",
    "(DONE) venn diagram X2 - for ALL FT and neoantigen\n",
    "(DONE) venn diagram X2 - for RECURRENT FT and neoantigen\n",
    "(DONE) freq distro of freq of FT & peptides across the whole cohort (get an idea of what are the proportions of peptides that are found in many samples)\n",
    "> beat and median IC50 score WITHIN the sample (as density plot)\n",
    "> 'control' for molecular subtype and hla type\n",
    "> approach #1: analyse neoantigen count seperately for patients w same hla type (ie. control for hla type) - then check for concordance across cohorts\n",
    "> approach #2: rerun extraction of unique peptide + hla information - then check for concordance across cohorts :D\n",
    "\n",
    "\n",
    "#discovered problems:\n",
    "> TCGA runs ... got problem? 21 files cant download (prob the ones we didnt have hla results for - we didn't handle this properly). it's ok, should not majorly affect out result. just state this in the slides\n",
    "> need to redo filtering .. my grep got bug, should grep \"^$fusion$\" instead of just $fusion when filtering out normals. i assume this will increase the num of FTs by a handful but should not majorly affect the results. we will redo this later .\n",
    "> need to redo normals filtering (too). exclude those that are found in only one sample (which is majority of them!! 2053/2841) - tho it doesn't seem to make much difference. maybe it's ok\n",
    ">include GTex breast tissues for background control?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
