{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Fusion Transcript (FT) Raw Data Exploration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook details the processes (semi-automated) done to further process the raw output files from Arriba and FusionCatcher fusion transcript callers. \n",
    "\n",
    "1. Run the `pypolars-process-ft-tsv.py` script to generate fusion transcript list from Arriba and FusionCatcher output files. The script takes a mandatory input of path to the directory where sample-specific fusion call output files from Arriba or FusionCatcher are stored as the first argument, and the specific string that is used to identify tool name (`arr` for Arriba fusion transcript call output file prefix, for instance). \n",
    "\n",
    "\tFor example:\n",
    "\t> ``` pypolars-process-ft-tsv.py data/FTmyBRCAs_raw/Arriba arr ```\n",
    "\n",
    "\tDo the same for the FusionCatcher raw output files, as well as the same Arriba and FusionCatcher output files generated from the processing 113 TCGA-Normals (to use as a panel of normals for FT filtering).\n",
    "\n",
    "2. Then, load up the two datasets on Jupyter Notebook and concatenate the dataframes together so that Arriba+FusionCatcher unfiltered FT data are combined into one data table and saved in one `.parquet` and `.tsv` file. Do the same for the `TCGANormals` panel of normals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, import packages\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "from itables import init_notebook_mode, show\n",
    "init_notebook_mode()\n",
    "import itables.options as opt\n",
    "opt.maxBytes = \"100KB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up MyBrCa datasets\n",
    "arr_mdf = pl.scan_parquet('output/MyBrCa/Arriba-FT-all-unfilt-list-v2.parquet')\n",
    "fc_mdf = pl.scan_parquet('output/MyBrCa/FusionCatcher-FT-all-unfilt-list-v2.parquet')\n",
    "\n",
    "# now load TCGANormals\n",
    "arr_norms_mdf = pl.scan_parquet('output/TCGANormals/Arriba-Normal-FT-all-unfilt-list-v2.parquet')\n",
    "fc_norms_mdf = pl.scan_parquet('output/TCGANormals/FusionCatcher-Normal-FT-all-unfilt-list-v2.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset 1A** (MyBrCa): Arriba unfiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"Arriba MyBrCa datatable dimension: \" + f\"<b>{arr_mdf.collect().shape}</b>\"))\n",
    "\n",
    "display(HTML(\"Showing truncated table:\"))\n",
    "\n",
    "show(pl.DataFrame.to_pandas(arr_mdf.collect(), use_pyarrow_extension_array=True).head(), classes=\"display compact\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset 1B** (MyBrCa): FusionCatcher unfiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"FusionCatcher MyBrCa datatable dimension: \" + f\"<b>{fc_mdf.collect().shape}</b>\"))\n",
    "\n",
    "display(HTML(\"Showing truncated table:\"))\n",
    "\n",
    "show(pl.DataFrame.to_pandas(fc_mdf.collect(), use_pyarrow_extension_array=True).head(), classes=\"display compact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset 2A** (TCGA Normals): Arriba unfiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"Arriba TCGA-Normals datatable dimension: \" + f\"<b>{arr_norms_mdf.collect().shape}</b>\"))\n",
    "\n",
    "display(HTML(\"Showing truncated table:\"))\n",
    "\n",
    "show(pl.DataFrame.to_pandas(arr_norms_mdf.collect(), use_pyarrow_extension_array=True).head(), classes=\"display compact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset 2B** (TCGA Normals): FusionCatcher unfiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"FusionCatcher TCGA-Normals datatable dimension: \" + f\"<b>{fc_norms_mdf.collect().shape}</b>\"))\n",
    "\n",
    "display(HTML(\"Showing truncated table:\"))\n",
    "\n",
    "show(pl.DataFrame.to_pandas(fc_norms_mdf.collect(), use_pyarrow_extension_array=True).head(), classes=\"display compact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Concatenate Arriba and FusionCatcher Datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can merge the two dataframes into one masterFrame for each cohort data (MyBrCa & TCGA panel of normals) using Polars' `concat` (vertical concatenation is the default, where two dataframes sharing the exact same columns would be joined together, adding all rows of dataframe 1 and 2 vertically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout --no-display\n",
    "\n",
    "joined_df = pl.concat(\n",
    "    [\n",
    "        arr_mdf.collect(),\n",
    "        fc_mdf.collect()\n",
    "    ]\n",
    ")\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"Concatenated MyBrCa Arriba+FusionCatcher datatable dimension: \" + f\"<b>{joined_df.shape}</b>\"))\n",
    "\n",
    "# pl.DataFrame.to_pandas(joined_df, use_pyarrow_extension_array=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# save joined df to files\n",
    "### UNCOMMENT TO SAVE\n",
    "# joined_df.write_csv('output/MyBrCa/Arr_FC-concat-FT-all-unfilt-list-v2.tsv', separator='\\t')\n",
    "\n",
    "# joined_df.write_parquet('output/MyBrCa/Arr_FC-concat-FT-all-unfilt-list-v2.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same with the TCGA panel of normal FTs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout --no-display\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "joined_norms_df = pl.concat(\n",
    "    [\n",
    "        arr_norms_mdf.collect(),\n",
    "        fc_norms_mdf.collect()\n",
    "    ]\n",
    ")\n",
    "display(HTML(\"Concatenated TCGA Normals Arriba+FusionCatcher datatable dimension: \" + f\"<b>{joined_norms_df.shape}</b>\"))\n",
    "\n",
    "# pl.DataFrame.to_pandas(joined_norms_df, use_pyarrow_extension_array=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# save joined df to files\n",
    "\n",
    "### UNCOMMENT TO SAVE\n",
    "# joined_norms_df.write_csv('output/TCGANormals/Arr_FC-Normals-concat-FT-all-unfilt-list-v2.tsv', separator='\\t')\n",
    "\n",
    "# joined_norms_df.write_parquet('output/TCGANormals/Arr_FC-Normals-concat-FT-all-unfilt-list-v2.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Filter MyBrCa Merged Datatable using Panel of Normals**\n",
    "Now we can filter the unfiltered, concatenated FT dataframes by discarding those that are present in TCGA Normal datatable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first load the parquet files\n",
    "# load up MyBrCa datasets\n",
    "mybrca_ccdf = pl.scan_parquet('output/MyBrCa/Arr_FC-concat-FT-all-unfilt-list-v2.parquet')\n",
    "\n",
    "# now load TCGANormals\n",
    "norms_ccdf = pl.scan_parquet('output/TCGANormals/Arr_FC-Normals-concat-FT-all-unfilt-list-v2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mybrca_ccdf_pan = pl.DataFrame.to_pandas(mybrca_ccdf.collect(), use_pyarrow_extension_array=True)\n",
    "\n",
    "display(HTML(\"Concatenated MyBrCa Arriba+FusionCatcher datatable dimension: \" + f\"<b>{mybrca_ccdf_pan.shape}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms_ccdf_pan = pl.DataFrame.to_pandas(norms_ccdf.collect(), use_pyarrow_extension_array=True)\n",
    "\n",
    "display(HTML(\"Concatenated TCGA Normals Arriba+FusionCatcher datatable dimension: \" + f\"<b>{norms_ccdf_pan.shape}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Polars' `filter` expression with `is_in` and the negation `~` to keep only unique rows for column `breakpointID` in MyBrCa dataframe that are NOT in the `breakpointID` in TCGANormals dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normfilt_mybrca_ccdf = mybrca_ccdf.collect().filter(~pl.col('breakpointID').is_in(norms_ccdf.collect()['breakpointID']))\n",
    "\n",
    "display(HTML(\"Normal-filtered MyBrCa Concatenated FT Datatable dimension: \" + f\"<b>{normfilt_mybrca_ccdf.shape}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `group_by` and `n_unique()` in Polars to create a count table for unique `breakpointID` and how \"shared\" it is across our MyBrCa cohort. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# test on toy df\n",
    "# df = pl.DataFrame(\n",
    "#     {\n",
    "#         \"a\": [1, 1, 2, 3, 4, 5],\n",
    "#         \"b\": [0.5, 0.5, 1.0, 2.0, 3.0, 3.0],\n",
    "#         \"c\": [True, True, True, False, True, True],\n",
    "#     }\n",
    "# )\n",
    "# print(df)\n",
    "# print(df.select(pl.col([\"a\", \"b\"])))\n",
    "# print(df.select(pl.col([\"a\", \"b\"])).group_by(\"b\").n_unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we subset the dataframe into just `breakpointID` and `sampleID` and then use `group_by` on the `breakpointID` column, then counting number of unique occurences of each unique `breakpointID` in the `sampleID` column. \n",
    "\n",
    "This would return a count of unique samples (patients) one particular unique breakpoint appears in. I call this the `sharednessDegree`.\n",
    "\n",
    "> **NOTE:** This is the best way to address miscounting breakpoints that appear in multiple rows due to differences in gene naming but they are only seen in one sample. Using other counting strategies such as window function (`.over` method) will count these duplicate rows as separate entities when in reality they are the same breakpoint seen in just one patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normfilt_mybrca_sharedness = normfilt_mybrca_ccdf.select(pl.col([\"breakpointID\", \"sampleID\"])).group_by(\"breakpointID\").n_unique().rename({\"sampleID\": \"sharednessDegree\"})\n",
    "\n",
    "show(normfilt_mybrca_sharedness.sort(\"sharednessDegree\", descending=True), maxBytes=0, classes=\"display compact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Plotting the Sharedness Degree**\n",
    "\n",
    "We have used Polars to easily group and count the number of patients sharing a particular breakpoint ID for each unique breakpoint ID as above, let's formalize that again by using Pandas instead.\n",
    "\n",
    "First, subset the filtered dataframe to just the two columns we are interested in using Polars, but this time prepend the \"P\" string to all values of the `sampleID` column, then convert to Pandas for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_sample_array = normfilt_mybrca_ccdf.select(\n",
    "    pl.col(\"breakpointID\"),\n",
    "    pl.concat_str(pl.lit(\"P\"), pl.col(\"sampleID\")).alias(\"sampleID\")\n",
    ")\n",
    "\n",
    "bpsample_pdf = bp_sample_array.to_pandas()\n",
    "\n",
    "show(bpsample_pdf, maxBytes=0, classes=\"display compact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the annotation redundancy in `fusionGeneID` column in the original df, we now have rows in `breakpointID` and `sampleID` that are repeated (i.e. `6:36132629-17:44965446\tP1` as seen above). Let's filter these out, as they represent the same putative FT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates based on both columns\n",
    "bpsample_pdf_unique = bpsample_pdf.drop_duplicates()\n",
    "\n",
    "# see how many duplicates were removed\n",
    "print(\"Original number of rows:\", len(bpsample_pdf))\n",
    "print(\"Number of rows after removing duplicates:\", len(bpsample_pdf_unique))\n",
    "print(\"Number of duplicates removed:\", len(bpsample_pdf) - len(bpsample_pdf_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(\"Nonredundant filtered breakpoint-sample datatable dimension: \" + f\"<b>{bpsample_pdf_unique.shape}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now group by each unique `breakpointID` and count how many `sampleID` is associated with this breakpoint (*sharedness degree*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by breakpointID and count unique sampleIDs\n",
    "breakpoint_counts = bpsample_pdf_unique.groupby('breakpointID')['sampleID'].nunique().reset_index()\n",
    "\n",
    "# Rename the column for clarity\n",
    "breakpoint_counts = breakpoint_counts.rename(columns={'sampleID': 'sharednessDegree'})\n",
    "\n",
    "show(breakpoint_counts, maxBytes=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can count the number of unique `breakpointID`s for each `sharednessDegree` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique breakpointIDs for each sharednessDegree value\n",
    "sharedness_counts = (\n",
    "    breakpoint_counts\n",
    "    .groupby('sharednessDegree')\n",
    "    .agg(\n",
    "        unique_bp_count=('breakpointID', 'nunique')\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values('sharednessDegree')\n",
    ")\n",
    "\n",
    "show(sharedness_counts.reset_index(drop=True), maxBytes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 8), dpi=300)\n",
    "sns.barplot(x=sharedness_counts['sharednessDegree'],y=sharedness_counts['unique_bp_count'], color='steelblue')\n",
    "\n",
    "# Add value labels on top of the bars\n",
    "for i, v in enumerate(sharedness_counts['unique_bp_count']):\n",
    "    plt.text(i, v, str(v), color='black', ha='center', fontweight='bold', fontsize=8)\n",
    "    \n",
    "# Set labels and title\n",
    "plt.xlabel('Sharedness Degree (Number of Patients A Unique FT Is Observed)')\n",
    "plt.ylabel('Count of Unique FTs')\n",
    "plt.title('Frequency of Unique Tumor-Specific FTs by Sharedness Degree')\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Using Graph Theory to Investigate Bipartite Relationship**\n",
    "\n",
    "We can use graph theory to explore the underlying bipartite network between unique `breakpointID` and `sampleID`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design an Analysis Class\n",
    "Create a complex class called `NetworkAnalyzer` to do graph network analysis between `breakpointID` and `sampleID`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from networkx.algorithms.bipartite import density as bipartite_density\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class NetworkAnalyzer:\n",
    "\tdef __init__(self, df=None, patient_col=None, breakpoint_col=None, \n",
    "\t\t\t\t\tprecomputed_matrix=None, patients=None, breakpoints=None):\n",
    "\t\t\"\"\"\n",
    "\t\tInitialize NetworkAnalyzer with either DataFrame or precomputed matrix.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\tdf (pl.DataFrame.Polars, optional): Input DataFrame HAS TO BE IN POLARS\n",
    "\t\t\tpatient_col (str, optional): Column name for patients\n",
    "\t\t\tbreakpoint_col (str, optional): Column name for breakpoints\n",
    "\t\t\tprecomputed_matrix (csr_matrix, optional): Pre-computed sparse adjacency matrix\n",
    "\t\t\tpatients (list, optional): List of patient IDs (required if using precomputed_matrix)\n",
    "\t\t\tbreakpoints (list, optional): List of breakpoint IDs (required if using precomputed_matrix)\n",
    "\t\t\"\"\"\n",
    "\t\tif precomputed_matrix is not None:\n",
    "\t\t\tif patients is None or breakpoints is None:\n",
    "\t\t\t\traise ValueError(\"Must provide patients and breakpoints lists with precomputed matrix\")\n",
    "\t\t\t# Keep matrix in sparse format\n",
    "\t\t\tself.adj_matrix_sparse = precomputed_matrix\n",
    "\t\t\tself.patients = patients\n",
    "\t\t\tself.breakpoints = breakpoints\n",
    "\t\telif df is not None and patient_col and breakpoint_col:\n",
    "\t\t\tself.df = df\n",
    "\t\t\tself.patient_col = patient_col\n",
    "\t\t\tself.breakpoint_col = breakpoint_col\n",
    "\t\t\t\n",
    "\t\t\t# Get unique sets\n",
    "\t\t\tself.patients = sorted(df[patient_col].unique().to_list())\n",
    "\t\t\tself.breakpoints = sorted(df[breakpoint_col].unique().to_list())\n",
    "\t\t\t\n",
    "\t\t\t# Create sparse adjacency matrix\n",
    "\t\t\tself.adj_matrix_sparse, self.patient_idx_dict, self.breakpoint_idx_dict = self._create_adjacency_matrix()\n",
    "\t\telse:\n",
    "\t\t\traise ValueError(\"Must provide either DataFrame with column names or precomputed matrix with labels\")\n",
    "\n",
    "\t\t# Don't calculate metrics immediately - do it lazily\n",
    "\t\tself._metrics_calculated = False\n",
    "\t\t\n",
    "\tdef _ensure_metrics_calculated(self):\n",
    "\t\t\"\"\"Calculate metrics if they haven't been calculated yet.\"\"\"\n",
    "\t\tif not self._metrics_calculated:\n",
    "\t\t\tself._calculate_metrics()\n",
    "\t\t\tself._metrics_calculated = True\n",
    "\n",
    "\tdef _create_adjacency_matrix(self) -> csr_matrix:\n",
    "\t\t\"\"\"Create the sparse adjacency matrix from the input DataFrame.\"\"\"\n",
    "\t\tmatrix = np.zeros((len(self.patients), len(self.breakpoints)))\n",
    "\t\tconnections = self.df.group_by(self.patient_col).agg(\n",
    "\t\t\tpl.col(self.breakpoint_col).alias('breakpoints')\n",
    "\t\t).to_dict(as_series=False)\n",
    "\n",
    "\t\tpatient_idx = {p: i for i, p in enumerate(self.patients)}\n",
    "\t\tbreakpoint_idx = {b: i for i, b in enumerate(self.breakpoints)}\n",
    "\n",
    "\t\tfor i, patient in enumerate(connections[self.patient_col]):\n",
    "\t\t\tfor bp in connections['breakpoints'][i]:\n",
    "\t\t\t\tmatrix[patient_idx[patient]][breakpoint_idx[bp]] = 1\n",
    "\n",
    "\t\treturn csr_matrix(matrix), patient_idx, breakpoint_idx\n",
    "\n",
    "\tdef _calculate_metrics(self):\n",
    "\t\t\"\"\"Calculate various network metrics.\"\"\"\n",
    "\t\t# Convert to dense only when needed for specific calculations\n",
    "\t\tdense_matrix = self.adj_matrix_sparse.toarray()\n",
    "\t\t\n",
    "\t\tself.patient_degrees = np.asarray(self.adj_matrix_sparse.sum(axis=1)).flatten()\n",
    "\t\tself.breakpoint_degrees = np.asarray(self.adj_matrix_sparse.sum(axis=0)).flatten()\n",
    "\t\t\n",
    "\t\t# Only calculate similarity matrices if needed for visualization\n",
    "\t\tself.patient_similarity = squareform(pdist(dense_matrix, metric='jaccard'))\n",
    "\t\tself.breakpoint_similarity = squareform(pdist(dense_matrix.T, metric='jaccard'))\n",
    "\n",
    "\t\t# Create bipartite graph more efficiently\n",
    "\t\tG = nx.Graph()\n",
    "\t\tG.add_nodes_from(range(len(self.patients)), bipartite=0)\n",
    "\t\tG.add_nodes_from(range(len(self.patients), len(self.patients) + len(self.breakpoints)), bipartite=1)\n",
    "\t\t\n",
    "\t\t# Add edges using sparse matrix coordinates\n",
    "\t\trows, cols = self.adj_matrix_sparse.nonzero()\n",
    "\t\tedges = zip(rows, cols + len(self.patients))\n",
    "\t\tG.add_edges_from(edges)\n",
    "\n",
    "\t\tself.density = bipartite_density(G, range(len(self.patients), len(self.patients) + len(self.breakpoints)))\n",
    "\t\t\n",
    "\t\t# Calculate centrality\n",
    "\t\tcentrality = nx.degree_centrality(G)\n",
    "\t\tself.breakpoint_centrality = [centrality[i + len(self.patients)] for i in range(len(self.breakpoints))]\n",
    "\n",
    "\tdef save_matrix(self, filename):\n",
    "\t\t\"\"\"\n",
    "\t\tSave the adjacency matrix in CSR format along with patient and breakpoint labels.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\tfilename (str): Base filename to save the data (without extension)\n",
    "\t\t\"\"\"\n",
    "\t\t# Save the sparse matrix\n",
    "\t\tsparse_matrix = self.adj_matrix_sparse\n",
    "\t\tnp.savez(f\"{filename}_adjac_matrix.npz\",\n",
    "\t\t\t\t\tdata=sparse_matrix.data,\n",
    "\t\t\t\t\tindices=sparse_matrix.indices,\n",
    "\t\t\t\t\tindptr=sparse_matrix.indptr,\n",
    "\t\t\t\t\tshape=sparse_matrix.shape)\n",
    "\t\t\n",
    "\t\t# Save the labels\n",
    "\t\tnp.save(f\"{filename}_matrix_label_patients.npy\", np.array(self.patients))\n",
    "\t\tnp.save(f\"{filename}_matrix_label_breakpoints.npy\", np.array(self.breakpoints))\n",
    "\n",
    "\t@classmethod\n",
    "\tdef load_from_files(cls, filename):\n",
    "\t\t\"\"\"\n",
    "\t\tLoad a NetworkAnalyzer instance from saved files.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\tfilename (str): Base filename (without extension) used when saving\n",
    "\t\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tNetworkAnalyzer: New instance with loaded data\n",
    "\t\t\"\"\"\n",
    "\t\t# Load the sparse matrix\n",
    "\t\tloader = np.load(f\"{filename}_adjac_matrix.npz\")\n",
    "\t\tmatrix = csr_matrix((loader['data'], loader['indices'], loader['indptr']), shape=loader['shape'])\n",
    "\t\t\n",
    "\t\t# Load the labels\n",
    "\t\tpatients = np.load(f\"{filename}_matrix_label_patients.npy\").tolist()\n",
    "\t\tbreakpoints = np.load(f\"{filename}_matrix_label_breakpoints.npy\").tolist()\n",
    "\n",
    "\t\treturn cls(precomputed_matrix=matrix, patients=patients, breakpoints=breakpoints)\n",
    "\t\n",
    "\tdef create_adjacency_matrix_plot(self, top_bins: list = None, bottom_bins: list = None) -> go.Figure:\n",
    "\t\t\"\"\"Create a standalone plot of the patient-breakpoint adjacency matrix.\"\"\"\n",
    "\t\t# Ensure metrics are calculated before accessing them\n",
    "\t\tself._ensure_metrics_calculated()\n",
    "\n",
    "\t\t# Convert to dense only when needed for specific calculations\n",
    "\t\tdense_matrix = self.adj_matrix_sparse.toarray()\n",
    "\n",
    "\t\t# If no bins are provided, plot the full adjacency matrix\n",
    "\t\tif not top_bins and not bottom_bins:\n",
    "\t\t\ttop_bins = list(range(len(self.breakpoints)))\n",
    "\t\t\ttop_breakpoints = self.breakpoints\n",
    "\t\t\ttop_matrix = dense_matrix\n",
    "\t\telif top_bins:\n",
    "\t\t\ttop_breakpoints = [self.breakpoints[i] for i in top_bins]\n",
    "\t\t\ttop_matrix = dense_matrix[:, top_bins]\n",
    "\n",
    "\t\t# Create the top breakpoints plot\n",
    "\t\tfig = go.Figure(\n",
    "\t\t\tdata=go.Heatmap(\n",
    "\t\t\t\tz=top_matrix,\n",
    "\t\t\t\tx=top_breakpoints,\n",
    "\t\t\t\ty=self.patients,\n",
    "\t\t\t\tcolorscale=\"Blues\",\n",
    "\t\t\t\tshowscale=True,\n",
    "\t\t\t\thoverongaps=False,\n",
    "\t\t\t\thoverinfo='text',\n",
    "\t\t\t\ttext=[[f\"Patient: {p}<br>Breakpoint: {b}<br>Connected: {'Yes' if top_matrix[i][j] else 'No'}\"\n",
    "\t\t\t\t\t\tfor j, b in enumerate(top_breakpoints)]\n",
    "\t\t\t\t\t\tfor i, p in enumerate(self.patients)],\n",
    "\t\t\t\tcolorbar=dict(title=\"Connection\"),\n",
    "\t\t\t\tname=\"Top Breakpoints\"\n",
    "\t\t\t)\n",
    "\t\t)\n",
    "\n",
    "\t\t# If bottom bins are provided, add the bottom breakpoints plot\n",
    "\t\tif bottom_bins:\n",
    "\t\t\tbottom_breakpoints = [self.breakpoints[i] for i in bottom_bins]\n",
    "\t\t\tbottom_matrix = dense_matrix[:, bottom_bins]\n",
    "\n",
    "\t\t\tfig.add_trace(\n",
    "\t\t\t\tgo.Heatmap(\n",
    "\t\t\t\t\tz=bottom_matrix,\n",
    "\t\t\t\t\tx=bottom_breakpoints,\n",
    "\t\t\t\t\ty=self.patients,\n",
    "\t\t\t\t\tcolorscale=\"Blues\",\n",
    "\t\t\t\t\tshowscale=True,\n",
    "\t\t\t\t\thoverongaps=False,\n",
    "\t\t\t\t\thoverinfo='text',\n",
    "\t\t\t\t\ttext=[[f\"Patient: {p}<br>Breakpoint: {b}<br>Connected: {'Yes' if bottom_matrix[i][j] else 'No'}\"\n",
    "\t\t\t\t\t\t\tfor j, b in enumerate(bottom_breakpoints)]\n",
    "\t\t\t\t\t\t\tfor i, p in enumerate(self.patients)],\n",
    "\t\t\t\t\tcolorbar=dict(title=\"Connection\"),\n",
    "\t\t\t\t\tname=\"Bottom Breakpoints\"\n",
    "\t\t\t\t)\n",
    "\t\t\t)\n",
    "\n",
    "\t\tfig.update_layout(\n",
    "\t\t\theight=1000,\n",
    "\t\t\twidth=1400,\n",
    "\t\t\ttitle=dict(\n",
    "\t\t\t\ttext=\"Adjacency Matrix\",\n",
    "\t\t\t\tx=0.5,\n",
    "\t\t\t\ty=0.95,\n",
    "\t\t\t\tfont=dict(size=18)\n",
    "\t\t\t),\n",
    "\t\t\txaxis_title=\"Breakpoints\",\n",
    "\t\t\tyaxis_title=\"Patients\",\n",
    "\t\t\ttemplate=\"simple_white\"\n",
    "\t\t)\n",
    "\n",
    "\t\treturn fig\n",
    "\n",
    "\tdef create_degree_distribution_plot(self, top_bins: list = None, bottom_bins: list = None) -> go.Figure:\n",
    "\t\t\"\"\"Create a standalone plot of the breakpoint degree distribution.\"\"\"\n",
    "\t\t# Ensure metrics are calculated before accessing them\n",
    "\t\tself._ensure_metrics_calculated()\n",
    "\n",
    "\t\t# If no bins are provided, plot the full degree distribution\n",
    "\t\tif not top_bins and not bottom_bins:\n",
    "\t\t\ttop_bins = list(range(len(self.breakpoints)))\n",
    "\t\t\ttop_breakpoints = self.breakpoints\n",
    "\t\t\ttop_degrees = [self.breakpoint_degrees[i] for i in top_bins]\n",
    "\n",
    "\t\telif top_bins:\n",
    "\t\t\ttop_breakpoints = [self.breakpoints[i] for i in top_bins]\n",
    "\t\t\ttop_degrees = [self.breakpoint_degrees[i] for i in top_bins]\n",
    "\n",
    "\t\t# Create the top breakpoints plot\n",
    "\t\tfig = go.Figure(\n",
    "\t\t\tdata=go.Bar(\n",
    "\t\t\t\tx=top_breakpoints,\n",
    "\t\t\t\ty=top_degrees,\n",
    "\t\t\t\thovertext=[f\"Breakpoint: {bp}<br>Connected to {deg} patients\"\n",
    "\t\t\t\t\t\t\tfor bp, deg in zip(top_breakpoints, top_degrees)],\n",
    "\t\t\t\thoverinfo='text',\n",
    "\t\t\t\tmarker_color='rgb(158,202,225)',\n",
    "\t\t\t\tmarker_line_color='rgb(8,48,107)',\n",
    "\t\t\t\tmarker_line_width=1.5,\n",
    "\t\t\t\tname=\"Degree Distribution\"\n",
    "\t\t\t)\n",
    "\t\t)\n",
    "\n",
    "\t\t# If bottom bins are provided, add the bottom breakpoints plot\n",
    "\t\tif bottom_bins:\n",
    "\t\t\tbottom_breakpoints = [self.breakpoints[i] for i in bottom_bins]\n",
    "\t\t\tbottom_degrees = [self.breakpoint_degrees[i] for i in bottom_bins]\n",
    "\n",
    "\t\t\tfig.add_trace(\n",
    "\t\t\t\tgo.Bar(\n",
    "\t\t\t\t\tx=bottom_breakpoints,\n",
    "\t\t\t\t\ty=bottom_degrees,\n",
    "\t\t\t\t\thovertext=[f\"Breakpoint: {bp}<br>Connected to {deg} patients\"\n",
    "\t\t\t\t\t\t\t\tfor bp, deg in zip(bottom_breakpoints, bottom_degrees)],\n",
    "\t\t\t\t\thoverinfo='text',\n",
    "\t\t\t\t\tmarker_color='rgb(158,202,225)',\n",
    "\t\t\t\t\tmarker_line_color='rgb(8,48,107)',\n",
    "\t\t\t\t\tmarker_line_width=1.5,\n",
    "\t\t\t\t\tname=\"Bottom Breakpoints\"\n",
    "\t\t\t\t)\n",
    "\t\t\t)\n",
    "\n",
    "\t\tfig.update_layout(\n",
    "\t\t\theight=1000,\n",
    "\t\t\twidth=1400,\n",
    "\t\t\ttitle=dict(\n",
    "\t\t\t\ttext=\"Breakpoint Degree Distribution\",\n",
    "\t\t\t\tx=0.5,\n",
    "\t\t\t\ty=0.95,\n",
    "\t\t\t\tfont=dict(size=18)\n",
    "\t\t\t),\n",
    "\t\t\txaxis_title=\"Breakpoints\",\n",
    "\t\t\tyaxis_title=\"Number of Patients\",\n",
    "\t\t\ttemplate=\"simple_white\"\n",
    "\t\t)\n",
    "\n",
    "\t\treturn fig\n",
    "\t\n",
    "\tdef create_patient_similarity_matrix_plot(self) -> go.Figure:\n",
    "\t\t\"\"\"Create a standalone plot of the patient similarity matrix.\"\"\"\n",
    "\t\t# Ensure metrics are calculated before accessing them\n",
    "\t\tself._ensure_metrics_calculated()\n",
    "\n",
    "\t\t# Extract the lower triangular portion of the matrix (excluding the diagonal)\n",
    "\t\tpatient_similarity_lower = np.tril(1 - self.patient_similarity, -1)\n",
    "\n",
    "\t\t# Create a mask for the upper triangular portion (excluding the diagonal)\n",
    "\t\tpatient_similarity_mask = np.tri(len(self.patients), len(self.patients), k=1, dtype=bool)\n",
    "\n",
    "\t\t# Create the heatmap data, setting the upper triangular portion to the maximum value\n",
    "\t\tpatient_similarity_data = np.where(patient_similarity_mask, np.max(patient_similarity_lower), 1 - self.patient_similarity)\n",
    "\n",
    "\t\tfig = go.Figure(\n",
    "\t\t\tdata=go.Heatmap(\n",
    "\t\t\t\tz=patient_similarity_data,\n",
    "\t\t\t\tx=self.patients,\n",
    "\t\t\t\ty=self.patients,\n",
    "\t\t\t\tcolorscale=\"Viridis\",\n",
    "\t\t\t\tshowscale=True,\n",
    "\t\t\t\tcolorbar=dict(title=\"Similarity\"),\n",
    "\t\t\t\tname=\"Patient Similarity\"\n",
    "\t\t\t)\n",
    "\t\t)\n",
    "\n",
    "\t\tfig.update_layout(\n",
    "\t\t\theight=1000,\n",
    "\t\t\twidth=1400,\n",
    "\t\t\ttitle=dict(\n",
    "\t\t\t\ttext=\"Patient Similarity Matrix\",\n",
    "\t\t\t\tx=0.5,\n",
    "\t\t\t\ty=0.95,\n",
    "\t\t\t\tfont=dict(size=18)\n",
    "\t\t\t),\n",
    "\t\t\txaxis_title=\"Patients\",\n",
    "\t\t\tyaxis_title=\"Patients\",\n",
    "\t\t\ttemplate=\"simple_white\"\n",
    "\t\t)\n",
    "\n",
    "\t\treturn fig\n",
    "\n",
    "\tdef create_breakpoint_cooccurrence_plot(self) -> go.Figure:\n",
    "\t\t\"\"\"Create a standalone plot of the breakpoint co-occurrence matrix.\"\"\"\n",
    "\t\t# Ensure metrics are calculated before accessing them\n",
    "\t\tself._ensure_metrics_calculated()\n",
    "\n",
    "\t\t# Extract the lower triangular portion of the matrix (excluding the diagonal)\n",
    "\t\tbreakpoint_similarity_lower = np.tril(1 - self.breakpoint_similarity, -1)\n",
    "\n",
    "\t\t# Create a mask for the upper triangular portion (excluding the diagonal)\n",
    "\t\tbreakpoint_similarity_mask = np.tri(len(self.breakpoints), len(self.breakpoints), k=1, dtype=bool)\n",
    "\n",
    "\t\t# Create the heatmap data, setting the upper triangular portion to the maximum value\n",
    "\t\tbreakpoint_similarity_data = np.where(breakpoint_similarity_mask, np.max(breakpoint_similarity_lower), 1 - self.breakpoint_similarity)\n",
    "\n",
    "\t\tfig = go.Figure(\n",
    "\t\t\tdata=go.Heatmap(\n",
    "\t\t\t\tz=breakpoint_similarity_data,\n",
    "\t\t\t\tx=self.breakpoints,\n",
    "\t\t\t\ty=self.breakpoints,\n",
    "\t\t\t\tcolorscale=\"Viridis\",\n",
    "\t\t\t\tshowscale=True,\n",
    "\t\t\t\tcolorbar=dict(title=\"Co-occurrence\"),\n",
    "\t\t\t\tname=\"Breakpoint Co-occurrence\"\n",
    "\t\t\t)\n",
    "\t\t)\n",
    "\n",
    "\t\tfig.update_layout(\n",
    "\t\t\theight=1000,\n",
    "\t\t\twidth=1400,\n",
    "\t\t\ttitle=dict(\n",
    "\t\t\t\ttext=\"Breakpoint Co-occurrence Matrix\",\n",
    "\t\t\t\tx=0.5,\n",
    "\t\t\t\ty=0.95,\n",
    "\t\t\t\tfont=dict(size=18)\n",
    "\t\t\t),\n",
    "\t\t\txaxis_title=\"Breakpoints\",\n",
    "\t\t\tyaxis_title=\"Breakpoints\",\n",
    "\t\t\ttemplate=\"simple_white\"\n",
    "\t\t)\n",
    "\n",
    "\t\treturn fig\n",
    "\n",
    "\tdef create_dashboard(self) -> go.Figure:\n",
    "\t\t\"\"\"Create a comprehensive visualization dashboard.\"\"\"\n",
    "\t\t# Ensure metrics are calculated before accessing them\n",
    "\t\tself._ensure_metrics_calculated()\n",
    "\t\t# Convert to dense only when needed for specific calculations\n",
    "\t\tdense_matrix = self.adj_matrix_sparse.toarray()\n",
    "\t\tfig = make_subplots(\n",
    "\t\t\trows=2, cols=2,\n",
    "\t\t\tsubplot_titles=(\"Patient-Breakpoint Adjacency Matrix\", \n",
    "\t\t\t\t\t\t\t\"Breakpoint Degree Distribution\",\n",
    "\t\t\t\t\t\t\t\"Patient Similarity Matrix\", \n",
    "\t\t\t\t\t\t\t\"Breakpoint Co-occurrence Matrix\"),\n",
    "\t\t\tspecs=[[{\"type\": \"heatmap\"}, {\"type\": \"bar\"}],\n",
    "\t\t\t\t\t[{\"type\": \"heatmap\"}, {\"type\": \"heatmap\"}]]\n",
    "\t\t)\n",
    "\n",
    "\t\t# 1. Adjacency Matrix with custom hover text\n",
    "\t\thover_text = [[f\"Patient: {p}<br>Breakpoint: {b}<br>Connected: {'Yes' if dense_matrix[i][j] else 'No'}\"\n",
    "\t\t\t\t\t\tfor j, b in enumerate(self.breakpoints)]\n",
    "\t\t\t\t\t\tfor i, p in enumerate(self.patients)]\n",
    "\n",
    "\t\tfig.add_trace(\n",
    "\t\t\tgo.Heatmap(\n",
    "\t\t\t\tz=dense_matrix,\n",
    "\t\t\t\tx=self.breakpoints,\n",
    "\t\t\t\ty=self.patients,\n",
    "\t\t\t\tcolorscale=\"Blues\",\n",
    "\t\t\t\tshowscale=False,\n",
    "\t\t\t\thoverongaps=False,\n",
    "\t\t\t\thoverinfo='text',\n",
    "\t\t\t\ttext=hover_text,\n",
    "\t\t\t\tcolorbar=dict(title=\"Connection\"),\n",
    "\t\t\t\tname=\"Connections\"\n",
    "\t\t\t),\n",
    "\t\t\trow=1, col=1\n",
    "\t\t)\n",
    "\n",
    "\t\t# 2. Degree Distribution with custom hover\n",
    "\t\thover_text = [f\"Breakpoint: {bp}<br>Connected to {deg} patients\"\n",
    "\t\t\t\t\t\tfor bp, deg in zip(self.breakpoints, self.breakpoint_degrees)]\n",
    "\n",
    "\t\tfig.add_trace(\n",
    "\t\t\tgo.Bar(\n",
    "\t\t\t\tx=self.breakpoints,\n",
    "\t\t\t\ty=self.breakpoint_degrees,\n",
    "\t\t\t\thovertext=hover_text,\n",
    "\t\t\t\thoverinfo='text',\n",
    "\t\t\t\tmarker_color='rgb(158,202,225)',\n",
    "\t\t\t\tmarker_line_color='rgb(8,48,107)',\n",
    "\t\t\t\tmarker_line_width=1.5,\n",
    "\t\t\t\tname=\"Breakpoint Degrees\"\n",
    "\t\t\t),\n",
    "\t\t\trow=1, col=2\n",
    "\t\t)\n",
    "\n",
    "\t\t# 3. Patient Similarity Matrix\n",
    "\t\tfig.add_trace(\n",
    "\t\t\tgo.Heatmap(\n",
    "\t\t\t\tz=1 - self.patient_similarity,\n",
    "\t\t\t\tx=self.patients,\n",
    "\t\t\t\ty=self.patients,\n",
    "\t\t\t\tcolorscale=\"Viridis\",\n",
    "\t\t\t\tshowscale=False,\n",
    "\t\t\t\tcolorbar=dict(title=\"Similarity\"),\n",
    "\t\t\t\tname=\"Patient Similarity\"\n",
    "\t\t\t),\n",
    "\t\t\trow=2, col=1\n",
    "\t\t)\n",
    "\n",
    "\t\t# 4. Breakpoint Co-occurrence\n",
    "\t\tfig.add_trace(\n",
    "\t\t\tgo.Heatmap(\n",
    "\t\t\t\tz=1 - self.breakpoint_similarity,\n",
    "\t\t\t\tx=self.breakpoints,\n",
    "\t\t\t\ty=self.breakpoints,\n",
    "\t\t\t\tcolorscale=\"Viridis\",\n",
    "\t\t\t\tshowscale=True,\n",
    "\t\t\t\tcolorbar=dict(title=\"Co-occurrence\"),\n",
    "\t\t\t\tname=\"Breakpoint Co-occurrence\"\n",
    "\t\t\t),\n",
    "\t\t\trow=2, col=2\n",
    "\t\t)\n",
    "\n",
    "\t\tfig.update_layout(\n",
    "\t\t\theight=1000,\n",
    "\t\t\twidth=1200,\n",
    "\t\t\ttitle=dict(\n",
    "\t\t\t\ttext=\"Network Analysis Dashboard\",\n",
    "\t\t\t\tx=0.5,\n",
    "\t\t\t\ty=0.95,\n",
    "\t\t\t\tfont=dict(size=24)\n",
    "\t\t\t),\n",
    "\t\t\tshowlegend=False,\n",
    "\t\t\ttemplate=\"simple_white\"\n",
    "\t\t)\n",
    "\n",
    "\t\tfont_size = 14\n",
    "\t\tfig.update_xaxes(title_text=\"Breakpoints\", title_font=dict(size=font_size), row=1, col=1)\n",
    "\t\tfig.update_yaxes(title_text=\"Patients\", title_font=dict(size=font_size), row=1, col=1)\n",
    "\t\tfig.update_xaxes(title_text=\"Breakpoints\", title_font=dict(size=font_size), row=1, col=2)\n",
    "\t\tfig.update_yaxes(title_text=\"Number of Patients\", title_font=dict(size=font_size), row=1, col=2)\n",
    "\t\tfig.update_xaxes(title_text=\"Patients\", title_font=dict(size=font_size), row=2, col=1)\n",
    "\t\tfig.update_yaxes(title_text=\"Patients\", title_font=dict(size=font_size), row=2, col=1)\n",
    "\t\tfig.update_xaxes(title_text=\"Breakpoints\", title_font=dict(size=font_size), row=2, col=2)\n",
    "\t\tfig.update_yaxes(title_text=\"Breakpoints\", title_font=dict(size=font_size), row=2, col=2)\n",
    "\n",
    "\t\treturn fig\n",
    "\t\n",
    "\tdef get_breakpoint_bins(self, top_percentile: float = 0.001, bottom_percentile: float = 0.001) -> tuple:\n",
    "\t\t\"\"\"\n",
    "\t\tCalculate the indexes of the breakpoints at the specified percentiles.\n",
    "\t\tReturns a tuple of two lists:\n",
    "\t\t- The first list contains the indexes of the top `top_percentile` breakpoints by degree.\n",
    "\t\t- The second list contains the indexes of the bottom `bottom_percentile` breakpoints by degree.\n",
    "\t\t\"\"\"\n",
    "\t\t# Ensure metrics are calculated before accessing them\n",
    "\t\tself._ensure_metrics_calculated()\n",
    "\t\tsorted_degrees = sorted(self.breakpoint_degrees)\n",
    "\t\ttop_cutoff = int(len(sorted_degrees) * top_percentile)\n",
    "\t\tbottom_cutoff = int(len(sorted_degrees) * (1 - bottom_percentile))\n",
    "\n",
    "\t\ttop_bins = [i for i in range(top_cutoff)]\n",
    "\t\tbottom_bins = [i for i in range(bottom_cutoff, len(sorted_degrees))]\n",
    "\n",
    "\t\treturn top_bins, bottom_bins\n",
    "\n",
    "\tdef print_summary_stats(self):\n",
    "\t\t# Ensure metrics are calculated before accessing them\n",
    "\t\tself._ensure_metrics_calculated()\n",
    "\t\tprint(f\"Network Summary Statistics:\")\n",
    "\t\tprint(f\"---------------------------\")\n",
    "\t\tprint(f\"Number of Patients: {len(self.patients)}\")\n",
    "\t\tprint(f\"Number of Breakpoints: {len(self.breakpoints)}\")\n",
    "\t\tprint(f\"Network Density: {self.density:.3f}\")\n",
    "\t\tprint(f\"Average Patient Degree: {np.mean(self.patient_degrees):.2f}\")\n",
    "\t\tprint(f\"Average Breakpoint Degree: {np.mean(self.breakpoint_degrees):.2f}\")\n",
    "\t\tprint(f\"\\nTop Breakpoints by Degree:\")\n",
    "\t\tfor bp, degree in sorted(zip(self.breakpoints, self.breakpoint_degrees), \n",
    "\t\t\t\t\t\t\t\tkey=lambda x: x[1], reverse=True)[:5]:\n",
    "\t\t\tprint(f\"  {bp}: {degree}\")\n",
    "\t\t# print(f\"\\nTop 10 Breakpoints by Degree Centrality:\")\n",
    "    \t# # Create list of (breakpoint, centrality) tuples and sort by centrality\n",
    "\t\t# centrality_pairs = list(zip(self.breakpoints, self.breakpoint_centrality))\n",
    "\t\t# sorted_by_centrality = sorted(centrality_pairs, key=lambda x: x[1], reverse=True)\n",
    "\t\t\n",
    "\t\t# # Print top 10\n",
    "\t\t# for bp, centrality in sorted_by_centrality[:10]:\n",
    "\t\t# \tprint(f\"  {bp}: {centrality:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Class on Toy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create toy data\n",
    "np.random.seed(420)  # for reproducibility\n",
    "\n",
    "patients = [f'P{i}' for i in range(1, 21)]  # 20 patients\n",
    "breakpoints = [f'BP{i}' for i in range(1, 16)]  # 15 breakpoints\n",
    "\n",
    "# Create random connections (each patient has 2-6 breakpoints)\n",
    "data = []\n",
    "for patient in patients:\n",
    "\tnum_breakpoints = np.random.randint(2, 7)\n",
    "\tpatient_breakpoints = np.random.choice(breakpoints, size=num_breakpoints, replace=False)\n",
    "\tfor bp in patient_breakpoints:\n",
    "\t\tdata.append({'patient_id': patient, 'breakpoint_id': bp})\n",
    "\n",
    "# Create Polars DataFrame\n",
    "df = pl.DataFrame(data)\n",
    "\n",
    "# now test the class\n",
    "# Create and display visualization\n",
    "analyzer = NetworkAnalyzer(df, patient_col='patient_id', breakpoint_col='breakpoint_id')\n",
    "fig = analyzer.create_dashboard()\n",
    "# fig.show()\n",
    "\n",
    "# Print summary statistics\n",
    "analyzer.print_summary_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate NetworkAnalyzer class on MyBrCa Data\n",
    "\n",
    "Now instantiate the class we built on our normal-filtered, unique-breakpoint-only subset dataFrame from MyBrCa FT data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is DF with redundant duplicate breakpointID-sampleID pairing\n",
    "print(bp_sample_array.shape)\n",
    "bp_sample_array.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is DF with redundant duplicate breakpointID-sampleID pairing FILTERED OUT\n",
    "bpsample_poldf = pl.from_pandas(bpsample_pdf_unique)\n",
    "print(bpsample_poldf.shape)\n",
    "bpsample_poldf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving adjacency matrix to file as precomputed npz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# # call the method save_matrix directly\n",
    "# analyzer_my.save_matrix('output/adj_matrix_ALLDEG_mybrca_v2')\n",
    "\n",
    "# # try reloading into an instance using the decorated class method load_from_file\n",
    "# analyzer_my_reloaded = NetworkAnalyzer.load_from_files('output/adj_matrix_ALLDEG_mybrca_v2')\n",
    "\n",
    "# # Print summary statistics\n",
    "# analyzer_my.print_summary_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping FT Breakpoints Seen in More than 10 Patients (1% of the MyBrCa Cohort)\n",
    "\n",
    "The distribution of the Sharedness Degree of each unique breakpoint, is as expected, skewed towards having a lot of unique, patient-specific connections, and very few shared breakpoints across patients. \n",
    "\n",
    "We can try to visualize the adjacency matrix, but because of the massive matrix dimension we have (**988 patients x 43927 unique breakpoints**), it is best to first filter out patient-specific breakpoints first. In fact, due to the fact that the putative FT neoantigen distribution is so skewed towards individualized presence, let's create a filtering threshold of keeping only the breakpoint IDs that are seen in **more than 9 patients (approximately 1% of the MyBrCa cohort)**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## go back to the sharednessDegree Pandas dataFrame\n",
    "# # select rows that has sharednessDegree > 10\n",
    "\n",
    "bp_sharedness_gt9 = pl.from_pandas(breakpoint_counts).filter(pl.col('sharednessDegree') > 9)\n",
    "\n",
    "show(bp_sharedness_gt9, maxBytes=0)\n",
    "\n",
    "#### or directly use the Polars dataframe normfilt_mybrca_sharedness\n",
    "# \n",
    "# bp_sharedness_gt9 = normfilt_mybrca_sharedness.filter(pl.col('sharednessDegree') > 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the unique, filtered, thresholded elements in the `breakpointID` column of the filtered dataFrame above, as the filtering list to keep only these same breakpoints in the other dataFrame used to instantiate `NetworkAnalyzer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bp_sharedness_gt9 is the dataframe with unique breakpointIDs to be used as filter\n",
    "# bpsample_poldf is the dataframe to be filtered\n",
    "\n",
    "filt_bpsample_poldf = bpsample_poldf.filter(\n",
    "    pl.col(\"breakpointID\").is_in(bp_sharedness_gt9[\"breakpointID\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instantiate a NetworkAnalyzer instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_my_filt = NetworkAnalyzer(filt_bpsample_poldf, patient_col='sampleID', breakpoint_col='breakpointID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary statistics\n",
    "analyzer_my_filt.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# call the method save_matrix directly; UNCOMMENT TO SAVE\n",
    "# analyzer_my_filt.save_matrix('output/adj_matrix_GT9FILT_mybrca_v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot Adjacency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = analyzer_my_filt.create_adjacency_matrix_plot()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = analyzer_my_filt.create_degree_distribution_plot()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = analyzer_my_filt.create_patient_similarity_matrix_plot()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = analyzer_my_filt.create_breakpoint_cooccurrence_plot()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_matrix = analyzer_my_filt.adj_matrix_sparse.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_minimal_covering_subset(adjacency_matrix, coverage_threshold, label_to_index_dict=None):\n",
    "    \"\"\"\n",
    "    Find minimal subset of rows that covers at least coverage_threshold fraction of columns,\n",
    "    with handling for dictionary-based label mapping.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    adjacency_matrix : np.ndarray\n",
    "        Binary matrix where rows are members of set A and columns are members of set B\n",
    "        1 indicates overlap, 0 indicates no overlap\n",
    "    coverage_threshold : float\n",
    "        Fraction of set B that needs to be covered (between 0 and 1)\n",
    "    label_to_index_dict : dict, optional\n",
    "        Dictionary mapping labels (strings) to indices (int)\n",
    "        Example: {'label1': 0, 'label2': 1, ...}\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (selected_indices, selected_labels, actual_coverage)\n",
    "        - selected_indices: List of numerical indices of selected rows\n",
    "        - selected_labels: List of original labels corresponding to the indices\n",
    "        - actual_coverage: Achieved coverage fraction\n",
    "    \"\"\"\n",
    "    # Create reverse mapping from index to label\n",
    "    if label_to_index_dict is not None:\n",
    "        index_to_label = {v: k for k, v in label_to_index_dict.items()}\n",
    "    \n",
    "    # Work with transpose of the matrix\n",
    "    working_matrix = adjacency_matrix.T\n",
    "    num_rows, num_cols = working_matrix.shape\n",
    "    \n",
    "    # Calculate target coverage\n",
    "    target_coverage = int(np.ceil(num_cols * coverage_threshold))\n",
    "    print(f\"Target coverage: {target_coverage} columns out of {num_cols}\")\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    selected_rows = []\n",
    "    covered_cols = np.zeros(num_cols, dtype=bool)\n",
    "    \n",
    "    while np.sum(covered_cols) < target_coverage:\n",
    "        # Calculate coverage gains for remaining rows\n",
    "        available_rows = [i for i in range(num_rows) if i not in selected_rows]\n",
    "        \n",
    "        if not available_rows:\n",
    "            break\n",
    "            \n",
    "        coverage_gains = np.array([\n",
    "            np.sum(~covered_cols & (working_matrix[i] == 1))\n",
    "            for i in available_rows\n",
    "        ])\n",
    "        \n",
    "        if np.max(coverage_gains) == 0:\n",
    "            print(\"No more improvements possible\")\n",
    "            break\n",
    "            \n",
    "        # Select the row that covers the most new columns\n",
    "        best_row_idx = available_rows[np.argmax(coverage_gains)]\n",
    "        selected_rows.append(best_row_idx)\n",
    "        \n",
    "        # Print progress with label if available\n",
    "        if label_to_index_dict is not None:\n",
    "            label = index_to_label[best_row_idx]\n",
    "            new_coverage = np.sum(~covered_cols & (working_matrix[best_row_idx] == 1))\n",
    "            print(f\"Selected {label} (index {best_row_idx}) covering {new_coverage} new columns\")\n",
    "        \n",
    "        # Update covered columns\n",
    "        covered_cols = covered_cols | (working_matrix[best_row_idx] == 1)\n",
    "    \n",
    "    # Calculate actual coverage achieved\n",
    "    actual_coverage = np.sum(covered_cols) / num_cols\n",
    "    print(f\"Achieved {actual_coverage:.2%} coverage\")\n",
    "    \n",
    "    # Convert indices to labels if dictionary provided\n",
    "    if label_to_index_dict is not None:\n",
    "        selected_labels = [index_to_label[idx] for idx in selected_rows]\n",
    "    else:\n",
    "        selected_labels = None\n",
    "    \n",
    "    return selected_rows, selected_labels, actual_coverage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakpoint_dict = analyzer_my_filt.breakpoint_idx_dict\n",
    "filt_bp_indices, filt_bp_labels, coverage = find_minimal_covering_subset(filt_matrix, coverage_threshold=1.0, label_to_index_dict=breakpoint_dict)\n",
    "print(f\"Minimal coverage: {coverage}\")\n",
    "print(f\"Minimal set of breakpoints: {filt_bp_indices}; Length of set: {len(filt_bp_indices)}\")\n",
    "print(f\"Labels of the minimal cover set of breakpoints: {filt_bp_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_dict = analyzer_my_filt.patient_idx_dict\n",
    "print(patient_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset original matrix\n",
    "minimal_set_cover_subset_matrix = filt_matrix[:, filt_bp_indices]\n",
    "minimal_set_cover_subset_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset the original df used to generate analyzer_my_filt instance\n",
    "\n",
    "filt_bpsample_minsetcover_poldf = bpsample_poldf.filter(\n",
    "    pl.col(\"breakpointID\").is_in(filt_bp_labels)\n",
    ")\n",
    "\n",
    "show(filt_bpsample_minsetcover_poldf, maxBytes=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new NetworkAnalyzer instance\n",
    "analyzer_subset_filt = NetworkAnalyzer(filt_bpsample_minsetcover_poldf, patient_col=\"sampleID\", breakpoint_col=\"breakpointID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary statistics\n",
    "analyzer_subset_filt.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = analyzer_subset_filt.create_adjacency_matrix_plot()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Filtering Out non-TNBCs**\n",
    "\n",
    "We can filter out rows corresponding to `sampleID` more than 172, because these are not TNBC samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
