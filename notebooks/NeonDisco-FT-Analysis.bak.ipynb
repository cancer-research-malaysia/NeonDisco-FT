{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "title: \"Fusion Transcript (FT) Data Exploration\"\n",
    "author: Suffian Azizan\n",
    "date: \"{{ datetime.now().strftime('%Y-%m-%d') }}\"\n",
    "output:\n",
    "    html:\n",
    "        theme: united\n",
    "        code_tools: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # first, import packages\n",
    "# import polars as pl\n",
    "# import pandas as pd\n",
    "# from IPython.display import HTML, display\n",
    "# from itables import init_notebook_mode, to_html_datatable, show\n",
    "# init_notebook_mode(all_interactive=True)\n",
    "# import itables.options as opt\n",
    "# opt.maxBytes = \"100KB\"\n",
    "\n",
    "# # load pretty jupyter's magics\n",
    "# %load_ext pretty_jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook details the processes (semi-automated) done to further process the raw output files from Arriba and FusionCatcher fusion transcript callers. \n",
    "\n",
    "1. Run the `pypolars-process-ft-tsv.py` script to generate fusion transcript list from Arriba and FusionCatcher output files. The script takes a mandatory input of path to the directory where sample-specific fusion call output files from Arriba or FusionCatcher are stored as the first argument, and the specific string that is used to identify tool name (`arr` for Arriba fusion transcript call output file prefix, for instance). \n",
    "\n",
    "\tFor example:\n",
    "\t> ``` pypolars-process-ft-tsv.py data/FTmyBRCAs_raw/Arriba arr ```\n",
    "\n",
    "\tDo the same for the FusionCatcher raw output files, as well as the same Arriba and FusionCatcher output files generated from the processing 113 TCGA-Normals (to use as a panel of normals for FT filtering).\n",
    "\n",
    "2. Then, load up the two datasets on Jupyter Notebook and concatenate the dataframes together so that Arriba+FusionCatcher unfiltered FT data are combined into one data table and saved in one `.parquet` and `.tsv` file. Do the same for the `TCGANormals` panel of normals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up MyBrCa datasets\n",
    "arr_mdf = pl.scan_parquet('../output/MyBrCa/Arriba-FT-all-unfilt-list-v2.parquet')\n",
    "fc_mdf = pl.scan_parquet('../output/MyBrCa/FusionCatcher-FT-all-unfilt-list-v2.parquet')\n",
    "\n",
    "# now load TCGANormals\n",
    "arr_tdf = pl.scan_parquet('../output/TCGANormals/Arriba-Normal-FT-all-unfilt-list-v2.parquet')\n",
    "fc_tdf = pl.scan_parquet('../output/TCGANormals/FusionCatcher-Normal-FT-all-unfilt-list-v2.parquet')\n",
    "\n",
    "html_arr_t = to_html_datatable(pl.DataFrame.to_pandas(arr_tdf.collect(), use_pyarrow_extension_array=True).head(5), display_logo_when_loading=False)\n",
    "\n",
    "html_fc_t = to_html_datatable(pl.DataFrame.to_pandas(fc_tdf.collect(), use_pyarrow_extension_array=True).head(), display_logo_when_loading=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loaded Polars dataFrames\n",
    "[//]: # (-.- .tabset .tabset-pills)\n",
    "\n",
    "Here all datasets from the two different fusion transcript calling tools run on both MyBrCa and TCGA-Normals cohorts are shown in tabs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%jmd \n",
    "\n",
    "#### **Dataset 1A** (MyBrCa): Arriba unfiltered\n",
    "Arriba MyBrCa datatable dimension: <b>{{arr_mdf.collect().shape}}</b>\n",
    "\n",
    "Showing truncated table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.-|m { input: false, output: true}\n",
    "show(pl.DataFrame.to_pandas(arr_mdf.collect(), use_pyarrow_extension_array=True).head(5), maxBytes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%jmd\n",
    "\n",
    "#### **Dataset 1B** (MyBrCa): FusionCatcher unfiltered\n",
    "Arriba MyBrCa datatable dimension: <b>{{fc_mdf.collect().shape}}</b>\n",
    "\n",
    "Showing truncated table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.-|m { input: false, output: true}\n",
    "show(pl.DataFrame.to_pandas(fc_mdf.collect(), use_pyarrow_extension_array=True).head(5), maxBytes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%jmd \n",
    "\n",
    "#### **Dataset 2A** (TCGA Normals): FusionCatcher unfiltered\n",
    "Arriba TCGA datatable dimension: <b>{{arr_tdf.collect().shape}}</b>\n",
    "\n",
    "Showing truncated table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.-|m { input: false, output: true}\n",
    "show(pl.DataFrame.to_pandas(arr_tdf.collect(), use_pyarrow_extension_array=True).head(5), maxBytes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%jmd\n",
    "\n",
    "#### **Dataset 2B** (TCGA Normals): FusionCatcher unfiltered\n",
    "FusionCatcher TCGA Normals datatable dimension: <b>{{fc_tdf.collect().shape}}</b>\n",
    "\n",
    "Showing truncated table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.-|m { input: false, output: true}\n",
    "show(pl.DataFrame.to_pandas(fc_tdf.collect(), use_pyarrow_extension_array=True).head(5), maxBytes=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Concatenate Arriba and FusionCatcher Datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can merge the two dataframes into one masterFrame for each cohort data (MyBrCa & TCGA panel of normals) using Polars' `concat`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Vertical concatenation is the default, where two dataframes sharing the exact same columns would be joined together, adding all rows of dataframe 1 and 2 vertically.\n",
    "\n",
    "[//]: # (-.- .alert .alert-warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.-|m { input: false, output: false}\n",
    "joined_df = pl.concat(\n",
    "    [\n",
    "        arr_mdf.collect(),\n",
    "        fc_mdf.collect()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.-|m { input: false, output: true}\n",
    "display(HTML(f\"Concatenated MyBrCa Arriba+FusionCatcher datatable dimension: \" + f\"<b>{joined_df.shape}</b>\"))\n",
    "\n",
    "show(joined_df.head(5), maxBytes=0, classes=\"display compact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same with the TCGA panel of normal FTs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.-|m { input: false, output: false}\n",
    "joined_norms_df = pl.concat(\n",
    "    [\n",
    "        arr_tdf.collect(),\n",
    "        fc_tdf.collect()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.-|m { input: false, output: true}\n",
    "\n",
    "display(HTML(f\"Concatenated TCGA-Normals Arriba+FusionCatcher datatable dimension: \" + f\"<b>{joined_norms_df.shape}</b>\"))\n",
    "\n",
    "show(joined_norms_df.head(5), maxBytes=0, classes=\"display compact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Filter MyBrCa Merged DataFrame using Panel of Normals**\n",
    "Now we can filter the unfiltered, concatenated FT dataframes by discarding those that are present in TCGA Normal data.\n",
    "\n",
    "First, load the parquet file from the MyBrCa datasets. Then load the parquet file from the TCGANormals datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.-|m { input: true, output: true}\n",
    "\n",
    "mybrca_ccdf = pl.scan_parquet('../output/MyBrCa/Arr_FC-concat-FT-all-unfilt-list-v2.parquet')\n",
    "\n",
    "tcganorms_ccdf = pl.scan_parquet('../output/TCGANormals/Arr_FC-Normals-concat-FT-all-unfilt-list-v2.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Once they are loaded, we can convert to Pandas from Polars for ease of processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.-|m { input: true, output: true, input_fold: show}\n",
    "\n",
    "my_concat_df = pl.DataFrame.to_pandas(mybrca_ccdf.collect(), use_pyarrow_extension_array=True)\n",
    "tn_concat_df = pl.DataFrame.to_pandas(tcganorms_ccdf.collect(), use_pyarrow_extension_array=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.-|m { input: false, output: true}\n",
    "display(HTML(f\"Concatenated MyBrCa Arriba+FusionCatcher datatable dimension: \" + f\"<b>{my_concat_df.shape}</b>\"))\n",
    "\n",
    "show(my_concat_df.head(5), maxBytes=0, classes=\"display compact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.-|m { input: false, output: true}\n",
    "display(HTML(f\"Concatenated TCGA Normals Arriba+FusionCatcher datatable dimension: \" + f\"<b>{tn_concat_df.shape}</b>\"))\n",
    "\n",
    "show(tn_concat_df.head(5), maxBytes=0, classes=\"display compact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use Polars' `filter` expression with `is_in` and the negation `~` to keep only unique rows for column `breakpointID` in MyBrCa dataframe that are **NOT** in the `breakpointID` in TCGANormals dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.-|m { input: true, output: true, input_fold: show}\n",
    "\n",
    "normfilt_mybrca_ccdf = mybrca_ccdf.collect().filter(~pl.col('breakpointID').is_in(tcganorms_ccdf.collect()['breakpointID']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TCGA Normal-filtered dataframe is as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.-|m { input: false, output: true}\n",
    "display(HTML(f\"<b>Normal-filtered</b> Concatenated MyBrCa FT DataFrame dimension: <b>{normfilt_mybrca_ccdf.shape}</b>\"))\n",
    "\n",
    "show(normfilt_mybrca_ccdf, maxBytes=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interrogate Shared `breakpointID`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most identifying column for our FT data is the gene fusion breakpoint information coded in the column `breakpointID`. We can interrogate our dataframe based on this column to explore the *sharedness* of each unique breakpoint (*how many patients share the same breakpoints*).\n",
    "\n",
    "To do so, subset the dataframe into just `breakpointID` and `sampleID` and then use `group_by` on the `breakpointID` column, then counting number of unique occurences of each unique `breakpointID` in the `sampleID` column using `n_unique()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.-|m { input: true, input_fold: show}\n",
    "\n",
    "normfilt_my_sharedness = normfilt_mybrca_ccdf.select(pl.col([\"breakpointID\", \"sampleID\"])).group_by(\"breakpointID\").n_unique().rename({\"sampleID\": \"sharednessDegree\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would return a count of unique samples (patients) one particular unique breakpoint appears in. This is the `sharednessDegree` henceforth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.-|m { input: false, input_fold: hide}\n",
    "\n",
    "sorted_normfilt_my_sharedness = normfilt_my_sharedness.sort(\"sharednessDegree\", descending=True)\n",
    "\n",
    "display(HTML(\"Nonredundant <b>normal-filtered</b> sharednessDegree dimension: \" + f\"<b>{sorted_normfilt_my_sharedness.shape}</b>\"))\n",
    "\n",
    "show(sorted_normfilt_my_sharedness, maxBytes=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** This is the best way to address miscounting breakpoints that appear in multiple rows due to differences in gene naming but they are only seen in one sample. Using other counting strategies such as window function (`.over` method) will count these duplicate rows as separate entities when in reality they are the same breakpoint seen in just one patient.\n",
    "\n",
    "[//]: # (-.- .alert .alert-warning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Plot the Sharedness Degree**\n",
    "\n",
    "We have used Polars to easily group and count the number of patients sharing a particular breakpoint ID for each unique breakpoint ID as above, let's formalize that again by using Pandas instead.\n",
    "\n",
    "First, subset the filtered dataframe to just the two columns we are interested in using Polars, but this time prepend the \"P\" string to all values of the `sampleID` column, then convert to Pandas for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.-|m { input: true, input_fold: show}\n",
    "bp_sample_array_pdf = normfilt_mybrca_ccdf.select(\n",
    "    pl.col(\"breakpointID\"),\n",
    "    pl.concat_str(pl.lit(\"P\"), pl.col(\"sampleID\")).alias(\"sampleID\")\n",
    ").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.-|m { input: false, output: true}\n",
    "display(HTML(\"Nonredundant <b>normal-filtered</b> BreakPoint–Patient Connection dimension: \" + f\"<b>{bp_sample_array_pdf.shape}</b>\"))\n",
    "\n",
    "show(bp_sample_array_pdf, maxBytes=0, classes=\"display compact\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the annotation redundancy in `fusionGeneID` column in the original df, we now have rows in `breakpointID` and `sampleID` that are repeated (i.e. `6:36132629-17:44965446\tP1` as seen above). \n",
    "\n",
    "Let's filter these out, as they represent the same putative FT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates based on both columns\n",
    "bpsample_pdf_unique = bp_sample_array_pdf.drop_duplicates()\n",
    "\n",
    "# see how many duplicates were removed\n",
    "print(\"Original number of rows:\", len(bp_sample_array_pdf))\n",
    "print(\"Number of rows after removing duplicates:\", len(bpsample_pdf_unique))\n",
    "print(\"Number of duplicates removed:\", len(bp_sample_array_pdf) - len(bpsample_pdf_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we group by each unique `breakpointID` and count how many `sampleID` is associated with this breakpoint (*sharedness degree*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by breakpointID and count unique sampleIDs\n",
    "breakpoint_counts = bpsample_pdf_unique.groupby('breakpointID')['sampleID'].nunique().reset_index()\n",
    "\n",
    "# Rename the column for clarity\n",
    "breakpoint_counts = breakpoint_counts.rename(columns={'sampleID': 'sharednessDegree'})\n",
    "\n",
    "show(breakpoint_counts, maxBytes=0, classes=\"display compact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can count the number of unique `breakpointID`s for each `sharednessDegree` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique breakpointIDs for each sharednessDegree value\n",
    "sharedness_counts = (\n",
    "    breakpoint_counts\n",
    "    .groupby('sharednessDegree')\n",
    "    .agg(\n",
    "        uniqueBPCounts=('breakpointID', 'nunique')\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values('sharednessDegree')\n",
    ")\n",
    "\n",
    "show(sharedness_counts.reset_index(drop=True), maxBytes=0, classes=\"display compact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharedness_counts['uniqueBPCounts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the `sharedness_counts` dataFrame in a bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 8), dpi=300)\n",
    "sns.barplot(x=sharedness_counts['sharednessDegree'],y=sharedness_counts['uniqueBPCounts'], color='crimson')\n",
    "\n",
    "# Add value labels on top of the bars\n",
    "for i, v in enumerate(sharedness_counts['uniqueBPCounts']):\n",
    "    plt.text(i, v, str(v), color='black', ha='center', fontweight='bold', fontsize=8)\n",
    "    \n",
    "# Set labels and title\n",
    "plt.xlabel('Sharedness Degree (Number of Patients A Unique FT Is Observed)')\n",
    "plt.ylabel('Count of Unique FTs')\n",
    "plt.title('Frequency of Unique Tumor-Specific FTs by Sharedness Degree')\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: This means that **94% (41080/43927)** of the unique breakpoint FTs are patient-specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Implement Bipartite Network Analysis**\n",
    "\n",
    "We can use bipartite network analysis from graph theory to explore the underlying relationships between unique `breakpointID` and `sampleID`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design an Analysis Class\n",
    "Create a complex class called `NetworkAnalyzer` to do graph network analysis between `breakpointID` and `sampleID`. \n",
    "\n",
    "(Expand the code below to see the full class methods.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.-|m { input: true, output: true, input_fold: hide}\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from networkx.algorithms.bipartite import density as bipartite_density\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class NetworkAnalyzer:\n",
    "\tdef __init__(self, df=None, patient_col=None, breakpoint_col=None, \n",
    "\t\t\t\t\tprecomputed_matrix=None, patients=None, breakpoints=None):\n",
    "\t\t\"\"\"\n",
    "\t\tInitialize NetworkAnalyzer with either DataFrame or precomputed matrix.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\tdf (pl.DataFrame.Polars, optional): Input DataFrame HAS TO BE IN POLARS\n",
    "\t\t\tpatient_col (str, optional): Column name for patients\n",
    "\t\t\tbreakpoint_col (str, optional): Column name for breakpoints\n",
    "\t\t\tprecomputed_matrix (csr_matrix, optional): Pre-computed sparse adjacency matrix\n",
    "\t\t\tpatients (list, optional): List of patient IDs (required if using precomputed_matrix)\n",
    "\t\t\tbreakpoints (list, optional): List of breakpoint IDs (required if using precomputed_matrix)\n",
    "\t\t\"\"\"\n",
    "\t\tif precomputed_matrix is not None:\n",
    "\t\t\tif patients is None or breakpoints is None:\n",
    "\t\t\t\traise ValueError(\"Must provide patients and breakpoints lists with precomputed matrix\")\n",
    "\t\t\t# Keep matrix in sparse format\n",
    "\t\t\tself.adj_matrix_sparse = precomputed_matrix\n",
    "\t\t\tself.patients = patients\n",
    "\t\t\tself.breakpoints = breakpoints\n",
    "\t\telif df is not None and patient_col and breakpoint_col:\n",
    "\t\t\tself.df = df\n",
    "\t\t\tself.patient_col = patient_col\n",
    "\t\t\tself.breakpoint_col = breakpoint_col\n",
    "\t\t\t\n",
    "\t\t\t# Get unique sets\n",
    "\t\t\tself.patients = sorted(df[patient_col].unique().to_list())\n",
    "\t\t\tself.breakpoints = sorted(df[breakpoint_col].unique().to_list())\n",
    "\t\t\t\n",
    "\t\t\t# Create sparse adjacency matrix\n",
    "\t\t\tself.adj_matrix_sparse, self.patient_idx_dict, self.breakpoint_idx_dict = self._create_adjacency_matrix()\n",
    "\t\telse:\n",
    "\t\t\traise ValueError(\"Must provide either DataFrame with column names or precomputed matrix with labels\")\n",
    "\n",
    "\t\t# Don't calculate metrics immediately - do it lazily\n",
    "\t\tself._metrics_calculated = False\n",
    "\t\t\n",
    "\tdef _ensure_metrics_calculated(self):\n",
    "\t\t\"\"\"Calculate metrics if they haven't been calculated yet.\"\"\"\n",
    "\t\tif not self._metrics_calculated:\n",
    "\t\t\tself._calculate_metrics()\n",
    "\t\t\tself._metrics_calculated = True\n",
    "\n",
    "\tdef _create_adjacency_matrix(self) -> csr_matrix:\n",
    "\t\t\"\"\"Create the sparse adjacency matrix from the input DataFrame.\"\"\"\n",
    "\t\tmatrix = np.zeros((len(self.patients), len(self.breakpoints)))\n",
    "\t\tconnections = self.df.group_by(self.patient_col).agg(\n",
    "\t\t\tpl.col(self.breakpoint_col).alias('breakpoints')\n",
    "\t\t).to_dict(as_series=False)\n",
    "\n",
    "\t\tpatient_idx = {p: i for i, p in enumerate(self.patients)}\n",
    "\t\tbreakpoint_idx = {b: i for i, b in enumerate(self.breakpoints)}\n",
    "\n",
    "\t\tfor i, patient in enumerate(connections[self.patient_col]):\n",
    "\t\t\tfor bp in connections['breakpoints'][i]:\n",
    "\t\t\t\tmatrix[patient_idx[patient]][breakpoint_idx[bp]] = 1\n",
    "\n",
    "\t\treturn csr_matrix(matrix), patient_idx, breakpoint_idx\n",
    "\n",
    "\tdef _calculate_metrics(self):\n",
    "\t\t\"\"\"Calculate various network metrics.\"\"\"\n",
    "\t\t# Convert to dense only when needed for specific calculations\n",
    "\t\tdense_matrix = self.adj_matrix_sparse.toarray()\n",
    "\t\t\n",
    "\t\tself.patient_degrees = np.asarray(self.adj_matrix_sparse.sum(axis=1)).flatten()\n",
    "\t\tself.breakpoint_degrees = np.asarray(self.adj_matrix_sparse.sum(axis=0)).flatten()\n",
    "\t\t\n",
    "\t\t# Only calculate similarity matrices if needed for visualization\n",
    "\t\tself.patient_similarity = squareform(pdist(dense_matrix, metric='jaccard'))\n",
    "\t\tself.breakpoint_similarity = squareform(pdist(dense_matrix.T, metric='jaccard'))\n",
    "\n",
    "\t\t# Create bipartite graph more efficiently\n",
    "\t\tG = nx.Graph()\n",
    "\t\tG.add_nodes_from(range(len(self.patients)), bipartite=0)\n",
    "\t\tG.add_nodes_from(range(len(self.patients), len(self.patients) + len(self.breakpoints)), bipartite=1)\n",
    "\t\t\n",
    "\t\t# Add edges using sparse matrix coordinates\n",
    "\t\trows, cols = self.adj_matrix_sparse.nonzero()\n",
    "\t\tedges = zip(rows, cols + len(self.patients))\n",
    "\t\tG.add_edges_from(edges)\n",
    "\n",
    "\t\tself.density = bipartite_density(G, range(len(self.patients), len(self.patients) + len(self.breakpoints)))\n",
    "\t\t\n",
    "\t\t# Calculate centrality\n",
    "\t\tcentrality = nx.degree_centrality(G)\n",
    "\t\tself.breakpoint_centrality = [centrality[i + len(self.patients)] for i in range(len(self.breakpoints))]\n",
    "\n",
    "\tdef save_matrix(self, filename):\n",
    "\t\t\"\"\"\n",
    "\t\tSave the adjacency matrix in CSR format along with patient and breakpoint labels.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\tfilename (str): Base filename to save the data (without extension)\n",
    "\t\t\"\"\"\n",
    "\t\t# Save the sparse matrix\n",
    "\t\tsparse_matrix = self.adj_matrix_sparse\n",
    "\t\tnp.savez(f\"{filename}_adjac_matrix.npz\",\n",
    "\t\t\t\t\tdata=sparse_matrix.data,\n",
    "\t\t\t\t\tindices=sparse_matrix.indices,\n",
    "\t\t\t\t\tindptr=sparse_matrix.indptr,\n",
    "\t\t\t\t\tshape=sparse_matrix.shape)\n",
    "\t\t\n",
    "\t\t# Save the labels\n",
    "\t\tnp.save(f\"{filename}_matrix_label_patients.npy\", np.array(self.patients))\n",
    "\t\tnp.save(f\"{filename}_matrix_label_breakpoints.npy\", np.array(self.breakpoints))\n",
    "\n",
    "\t@classmethod\n",
    "\tdef load_from_files(cls, filename):\n",
    "\t\t\"\"\"\n",
    "\t\tLoad a NetworkAnalyzer instance from saved files.\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\tfilename (str): Base filename (without extension) used when saving\n",
    "\t\t\t\n",
    "\t\tReturns:\n",
    "\t\t\tNetworkAnalyzer: New instance with loaded data\n",
    "\t\t\"\"\"\n",
    "\t\t# Load the sparse matrix\n",
    "\t\tloader = np.load(f\"{filename}_adjac_matrix.npz\")\n",
    "\t\tmatrix = csr_matrix((loader['data'], loader['indices'], loader['indptr']), shape=loader['shape'])\n",
    "\t\t\n",
    "\t\t# Load the labels\n",
    "\t\tpatients = np.load(f\"{filename}_matrix_label_patients.npy\").tolist()\n",
    "\t\tbreakpoints = np.load(f\"{filename}_matrix_label_breakpoints.npy\").tolist()\n",
    "\n",
    "\t\treturn cls(precomputed_matrix=matrix, patients=patients, breakpoints=breakpoints)\n",
    "\t\n",
    "\tdef create_adjacency_matrix_plot(self, top_bins: list = None, bottom_bins: list = None) -> go.Figure:\n",
    "\t\t\"\"\"Create a standalone plot of the patient-breakpoint adjacency matrix.\"\"\"\n",
    "\t\t# Ensure metrics are calculated before accessing them\n",
    "\t\tself._ensure_metrics_calculated()\n",
    "\n",
    "\t\t# Convert to dense only when needed for specific calculations\n",
    "\t\tdense_matrix = self.adj_matrix_sparse.toarray()\n",
    "\n",
    "\t\t# If no bins are provided, plot the full adjacency matrix\n",
    "\t\tif not top_bins and not bottom_bins:\n",
    "\t\t\t# Sort breakpoints by their degree (number of connected patients)\n",
    "\t\t\tsorted_indices = np.argsort(self.breakpoint_degrees)[::-1]  # Descending order\n",
    "\t\t\ttop_bins = sorted_indices.tolist()\n",
    "\t\t\ttop_breakpoints = [self.breakpoints[i] for i in top_bins]\n",
    "\t\t\ttop_matrix = dense_matrix[:, top_bins]\n",
    "\t\telif top_bins:\n",
    "\t\t\t# Sort the provided top bins by degree\n",
    "\t\t\tsorted_top_bins = sorted(top_bins, key=lambda x: self.breakpoint_degrees[x], reverse=True)\n",
    "\t\t\ttop_breakpoints = [self.breakpoints[i] for i in sorted_top_bins]\n",
    "\t\t\ttop_matrix = dense_matrix[:, sorted_top_bins]\n",
    "\n",
    "\t\t# Create a binary colorscale - only two colors\n",
    "\t\tbinary_colorscale = [\n",
    "\t\t\t[0, 'white'],  # White for 0 (no connection)\n",
    "\t\t\t[1, 'crimson']  # Crimson for 1 (connection exists)\n",
    "\t\t]\n",
    "\n",
    "\t\t# Create the top breakpoints plot\n",
    "\t\tfig = go.Figure(\n",
    "\t\t\tdata=go.Heatmap(\n",
    "\t\t\t\tz=top_matrix,\n",
    "\t\t\t\tx=top_breakpoints,\n",
    "\t\t\t\ty=self.patients,\n",
    "\t\t\t\tcolorscale=binary_colorscale,\n",
    "\t\t\t\tshowscale=True,\n",
    "\t\t\t\thoverongaps=False,\n",
    "\t\t\t\thoverinfo='text',\n",
    "\t\t\t\ttext=[[f\"Patient: {p}<br>Breakpoint: {b}<br>Connected: {'Yes' if top_matrix[i][j] else 'No'}\"\n",
    "\t\t\t\t\t\tfor j, b in enumerate(top_breakpoints)]\n",
    "\t\t\t\t\t\tfor i, p in enumerate(self.patients)],\n",
    "\t\t\t\tcolorbar=dict(title=\"Connection\"),\n",
    "\t\t\t\tname=\"Top Breakpoints\"\n",
    "\t\t\t)\n",
    "\t\t)\n",
    "\n",
    "\t\t# If bottom bins are provided, add the bottom breakpoints plot\n",
    "\t\tif bottom_bins:\n",
    "\t\t\t# Sort the bottom bins by degree\n",
    "\t\t\tsorted_bottom_bins = sorted(bottom_bins, key=lambda x: self.breakpoint_degrees[x], reverse=True)\n",
    "\t\t\tbottom_breakpoints = [self.breakpoints[i] for i in sorted_bottom_bins]\n",
    "\t\t\tbottom_matrix = dense_matrix[:, sorted_bottom_bins]\n",
    "\n",
    "\t\t\tfig.add_trace(\n",
    "\t\t\t\tgo.Heatmap(\n",
    "\t\t\t\t\tz=bottom_matrix,\n",
    "\t\t\t\t\tx=bottom_breakpoints,\n",
    "\t\t\t\t\ty=self.patients,\n",
    "\t\t\t\t\tcolorscale=binary_colorscale,\n",
    "\t\t\t\t\tshowscale=True,\n",
    "\t\t\t\t\thoverongaps=False,\n",
    "\t\t\t\t\thoverinfo='text',\n",
    "\t\t\t\t\ttext=[[f\"Patient: {p}<br>Breakpoint: {b}<br>Connected: {'Yes' if bottom_matrix[i][j] else 'No'}\"\n",
    "\t\t\t\t\t\t\tfor j, b in enumerate(bottom_breakpoints)]\n",
    "\t\t\t\t\t\t\tfor i, p in enumerate(self.patients)],\n",
    "\t\t\t\t\tcolorbar=dict(title=\"Connection\"),\n",
    "\t\t\t\t\tname=\"Bottom Breakpoints\"\n",
    "\t\t\t\t)\n",
    "\t\t\t)\n",
    "\n",
    "\t\tfig.update_layout(\n",
    "\t\t\theight=1500,\n",
    "\t\t\twidth=850,\n",
    "\t\t\ttitle=dict(\n",
    "\t\t\t\ttext=\"Adjacency Matrix\",\n",
    "\t\t\t\tx=0.5,\n",
    "\t\t\t\ty=0.95,\n",
    "\t\t\t\tfont=dict(size=18)\n",
    "\t\t\t),\n",
    "\t\t\txaxis_title=\"Breakpoints (Sorted by Number of Connected Patients)\",\n",
    "\t\t\tyaxis_title=\"Patients\",\n",
    "\t\t\ttemplate=\"simple_white\"\n",
    "\t\t)\n",
    "\n",
    "\t\treturn fig\n",
    "\n",
    "\tdef create_degree_distribution_plot(self, top_bins: list = None, bottom_bins: list = None) -> go.Figure:\n",
    "\t\t\"\"\"Create a standalone plot of the breakpoint degree distribution.\"\"\"\n",
    "\t\t# Ensure metrics are calculated before accessing them\n",
    "\t\tself._ensure_metrics_calculated()\n",
    "\n",
    "\t\t# If no bins are provided, plot the full degree distribution\n",
    "\t\tif not top_bins and not bottom_bins:\n",
    "\t\t\ttop_bins = list(range(len(self.breakpoints)))\n",
    "\t\t\ttop_breakpoints = self.breakpoints\n",
    "\t\t\ttop_degrees = [self.breakpoint_degrees[i] for i in top_bins]\n",
    "\n",
    "\t\telif top_bins:\n",
    "\t\t\ttop_breakpoints = [self.breakpoints[i] for i in top_bins]\n",
    "\t\t\ttop_degrees = [self.breakpoint_degrees[i] for i in top_bins]\n",
    "\n",
    "\t\t# Create the top breakpoints plot\n",
    "\t\tfig = go.Figure(\n",
    "\t\t\tdata=go.Bar(\n",
    "\t\t\t\tx=top_breakpoints,\n",
    "\t\t\t\ty=top_degrees,\n",
    "\t\t\t\thovertext=[f\"Breakpoint: {bp}<br>Connected to {deg} patients\"\n",
    "\t\t\t\t\t\t\tfor bp, deg in zip(top_breakpoints, top_degrees)],\n",
    "\t\t\t\thoverinfo='text',\n",
    "\t\t\t\tmarker_color='steelblue',\n",
    "\t\t\t\tmarker_line_color='rgb(8,48,107)',\n",
    "\t\t\t\tmarker_line_width=1.5,\n",
    "\t\t\t\tname=\"Degree Distribution\"\n",
    "\t\t\t)\n",
    "\t\t)\n",
    "\n",
    "\t\t# If bottom bins are provided, add the bottom breakpoints plot\n",
    "\t\tif bottom_bins:\n",
    "\t\t\tbottom_breakpoints = [self.breakpoints[i] for i in bottom_bins]\n",
    "\t\t\tbottom_degrees = [self.breakpoint_degrees[i] for i in bottom_bins]\n",
    "\n",
    "\t\t\tfig.add_trace(\n",
    "\t\t\t\tgo.Bar(\n",
    "\t\t\t\t\tx=bottom_breakpoints,\n",
    "\t\t\t\t\ty=bottom_degrees,\n",
    "\t\t\t\t\thovertext=[f\"Breakpoint: {bp}<br>Connected to {deg} patients\"\n",
    "\t\t\t\t\t\t\t\tfor bp, deg in zip(bottom_breakpoints, bottom_degrees)],\n",
    "\t\t\t\t\thoverinfo='text',\n",
    "\t\t\t\t\tmarker_color='steelblue',\n",
    "\t\t\t\t\tmarker_line_color='rgb(8,48,107)',\n",
    "\t\t\t\t\tmarker_line_width=1.5,\n",
    "\t\t\t\t\tname=\"Bottom Breakpoints\"\n",
    "\t\t\t\t)\n",
    "\t\t\t)\n",
    "\n",
    "\t\tfig.update_layout(\n",
    "\t\t\theight=1000,\n",
    "\t\t\twidth=850,\n",
    "\t\t\ttitle=dict(\n",
    "\t\t\t\ttext=\"Breakpoint Degree Distribution\",\n",
    "\t\t\t\tx=0.5,\n",
    "\t\t\t\ty=0.95,\n",
    "\t\t\t\tfont=dict(size=18)\n",
    "\t\t\t),\n",
    "\t\t\txaxis_title=\"Breakpoints\",\n",
    "\t\t\tyaxis_title=\"Number of Patients\",\n",
    "\t\t\ttemplate=\"simple_white\"\n",
    "\t\t)\n",
    "\n",
    "\t\treturn fig\n",
    "\t\n",
    "\tdef create_patient_similarity_matrix_plot(self) -> go.Figure:\n",
    "\t\t\"\"\"Create a standalone plot of the patient similarity matrix.\"\"\"\n",
    "\t\t# Ensure metrics are calculated before accessing them\n",
    "\t\tself._ensure_metrics_calculated()\n",
    "\n",
    "\t\t# Extract the lower triangular portion of the matrix (excluding the diagonal)\n",
    "\t\tpatient_similarity_lower = np.tril(1 - self.patient_similarity, -1)\n",
    "\n",
    "\t\t# Create a mask for the upper triangular portion (excluding the diagonal)\n",
    "\t\tpatient_similarity_mask = np.tri(len(self.patients), len(self.patients), k=1, dtype=bool)\n",
    "\n",
    "\t\t# Create the heatmap data, setting the upper triangular portion to the maximum value\n",
    "\t\tpatient_similarity_data = np.where(patient_similarity_mask, np.max(patient_similarity_lower), 1 - self.patient_similarity)\n",
    "\n",
    "\t\tfig = go.Figure(\n",
    "\t\t\tdata=go.Heatmap(\n",
    "\t\t\t\tz=patient_similarity_data,\n",
    "\t\t\t\tx=self.patients,\n",
    "\t\t\t\ty=self.patients,\n",
    "\t\t\t\tcolorscale=\"Viridis\",\n",
    "\t\t\t\tshowscale=True,\n",
    "\t\t\t\tcolorbar=dict(title=\"Similarity\"),\n",
    "\t\t\t\tname=\"Patient Similarity\"\n",
    "\t\t\t)\n",
    "\t\t)\n",
    "\n",
    "\t\tfig.update_layout(\n",
    "\t\t\theight=1000,\n",
    "\t\t\twidth=850,\n",
    "\t\t\ttitle=dict(\n",
    "\t\t\t\ttext=\"Patient Similarity Matrix\",\n",
    "\t\t\t\tx=0.5,\n",
    "\t\t\t\ty=0.95,\n",
    "\t\t\t\tfont=dict(size=18)\n",
    "\t\t\t),\n",
    "\t\t\txaxis_title=\"Patients\",\n",
    "\t\t\tyaxis_title=\"Patients\",\n",
    "\t\t\ttemplate=\"simple_white\"\n",
    "\t\t)\n",
    "\n",
    "\t\treturn fig\n",
    "\n",
    "\tdef create_breakpoint_cooccurrence_plot(self) -> go.Figure:\n",
    "\t\t\"\"\"Create a standalone plot of the breakpoint co-occurrence matrix.\"\"\"\n",
    "\t\t# Ensure metrics are calculated before accessing them\n",
    "\t\tself._ensure_metrics_calculated()\n",
    "\n",
    "\t\t# Extract the lower triangular portion of the matrix (excluding the diagonal)\n",
    "\t\tbreakpoint_similarity_lower = np.tril(1 - self.breakpoint_similarity, -1)\n",
    "\n",
    "\t\t# Create a mask for the upper triangular portion (excluding the diagonal)\n",
    "\t\tbreakpoint_similarity_mask = np.tri(len(self.breakpoints), len(self.breakpoints), k=1, dtype=bool)\n",
    "\n",
    "\t\t# Create the heatmap data, setting the upper triangular portion to the maximum value\n",
    "\t\tbreakpoint_similarity_data = np.where(breakpoint_similarity_mask, np.max(breakpoint_similarity_lower), 1 - self.breakpoint_similarity)\n",
    "\n",
    "\t\tfig = go.Figure(\n",
    "\t\t\tdata=go.Heatmap(\n",
    "\t\t\t\tz=breakpoint_similarity_data,\n",
    "\t\t\t\tx=self.breakpoints,\n",
    "\t\t\t\ty=self.breakpoints,\n",
    "\t\t\t\tcolorscale=\"Viridis\",\n",
    "\t\t\t\tshowscale=True,\n",
    "\t\t\t\tcolorbar=dict(title=\"Co-occurrence\"),\n",
    "\t\t\t\tname=\"Breakpoint Co-occurrence\"\n",
    "\t\t\t)\n",
    "\t\t)\n",
    "\n",
    "\t\tfig.update_layout(\n",
    "\t\t\theight=1000,\n",
    "\t\t\twidth=850,\n",
    "\t\t\ttitle=dict(\n",
    "\t\t\t\ttext=\"Breakpoint Co-occurrence Matrix\",\n",
    "\t\t\t\tx=0.5,\n",
    "\t\t\t\ty=0.95,\n",
    "\t\t\t\tfont=dict(size=18)\n",
    "\t\t\t),\n",
    "\t\t\txaxis_title=\"Breakpoints\",\n",
    "\t\t\tyaxis_title=\"Breakpoints\",\n",
    "\t\t\ttemplate=\"simple_white\"\n",
    "\t\t)\n",
    "\n",
    "\t\treturn fig\n",
    "\n",
    "\tdef create_dashboard(self) -> go.Figure:\n",
    "\t\t\"\"\"Create a comprehensive visualization dashboard.\"\"\"\n",
    "\t\t# Ensure metrics are calculated before accessing them\n",
    "\t\tself._ensure_metrics_calculated()\n",
    "\t\t# Convert to dense only when needed for specific calculations\n",
    "\t\tdense_matrix = self.adj_matrix_sparse.toarray()\n",
    "\t\t# Create a binary colorscale - only two colors\n",
    "\t\tbinary_colorscale = [\n",
    "\t\t[0, 'white'],  # White for 0 (no connection)\n",
    "\t\t[1, 'crimson']     # Crimson for 1 (connection exists)\n",
    "\t\t]\n",
    "\t\tfig = make_subplots(\n",
    "\t\t\thorizontal_spacing=0.15,\n",
    "\t\t\trows=2, cols=2,\n",
    "\t\t\tsubplot_titles=(\"Patient-Breakpoint Adjacency Matrix\", \n",
    "\t\t\t\t\t\t\t\"Breakpoint Degree Distribution\",\n",
    "\t\t\t\t\t\t\t\"Patient Similarity Matrix\", \n",
    "\t\t\t\t\t\t\t\"Breakpoint Co-occurrence Matrix\"),\n",
    "\t\t\tspecs=[[{\"type\": \"heatmap\"}, {\"type\": \"bar\"}],\n",
    "\t\t\t\t\t[{\"type\": \"heatmap\"}, {\"type\": \"heatmap\"}]]\n",
    "\t\t)\n",
    "\n",
    "\t\t# 1. Adjacency Matrix with custom hover text\n",
    "\t\thover_text = [[f\"Patient: {p}<br>Breakpoint: {b}<br>Connected: {'Yes' if dense_matrix[i][j] else 'No'}\"\n",
    "\t\t\t\t\t\tfor j, b in enumerate(self.breakpoints)]\n",
    "\t\t\t\t\t\tfor i, p in enumerate(self.patients)]\n",
    "\n",
    "\t\tfig.add_trace(\n",
    "\t\t\tgo.Heatmap(\n",
    "\t\t\t\tz=dense_matrix,\n",
    "\t\t\t\tx=self.breakpoints,\n",
    "\t\t\t\ty=self.patients,\n",
    "\t\t\t\tcolorscale=binary_colorscale,\n",
    "\t\t\t\tshowscale=True,\n",
    "\t\t\t\thoverongaps=False,\n",
    "\t\t\t\thoverinfo='text',\n",
    "\t\t\t\ttext=hover_text,\n",
    "\t\t\t\tcolorbar=dict(\n",
    "                title=\"Connection\",\n",
    "                x=0.43,  # Position to the right of the first subplot\n",
    "                y=1.00,\n",
    "                len=0.3,\n",
    "                yanchor='top'\n",
    "            ),\n",
    "\t\t\t\tname=\"Connections\"\n",
    "\t\t\t),\n",
    "\t\t\trow=1, col=1\n",
    "\t\t)\n",
    "\n",
    "\t\t# 2. Degree Distribution with custom hover\n",
    "\t\thover_text = [f\"Breakpoint: {bp}<br>Connected to {deg} patients\"\n",
    "\t\t\t\t\t\tfor bp, deg in zip(self.breakpoints, self.breakpoint_degrees)]\n",
    "\n",
    "\t\tfig.add_trace(\n",
    "\t\t\tgo.Bar(\n",
    "\t\t\t\tx=self.breakpoints,\n",
    "\t\t\t\ty=self.breakpoint_degrees,\n",
    "\t\t\t\thovertext=hover_text,\n",
    "\t\t\t\thoverinfo='text',\n",
    "\t\t\t\tmarker_color='crimson',\n",
    "\t\t\t\tmarker_line_color='rgb(8,48,107)',\n",
    "\t\t\t\tmarker_line_width=1.5,\n",
    "\t\t\t\tname=\"Breakpoint Degrees\"\n",
    "\t\t\t),\n",
    "\t\t\trow=1, col=2\n",
    "\t\t)\n",
    "\n",
    "\t\t# 3. Patient Similarity Matrix\n",
    "\t\tfig.add_trace(\n",
    "\t\t\tgo.Heatmap(\n",
    "\t\t\t\tz=1 - self.patient_similarity,\n",
    "\t\t\t\tx=self.patients,\n",
    "\t\t\t\ty=self.patients,\n",
    "\t\t\t\tcolorscale=\"Viridis\",\n",
    "\t\t\t\tshowscale=True,\n",
    "\t\t\t\tcolorbar=dict(title=\"Similarity\",\n",
    "                x=0.43,  # Position to the right of the third subplot\n",
    "                y=0.35,\n",
    "                len=0.3,\n",
    "                yanchor='top'),\n",
    "\t\t\t\tname=\"Patient Similarity\"\n",
    "\t\t\t),\n",
    "\t\t\trow=2, col=1\n",
    "\t\t)\n",
    "\n",
    "\t\t# 4. Breakpoint Co-occurrence\n",
    "\t\tfig.add_trace(\n",
    "\t\t\tgo.Heatmap(\n",
    "\t\t\t\tz=1 - self.breakpoint_similarity,\n",
    "\t\t\t\tx=self.breakpoints,\n",
    "\t\t\t\ty=self.breakpoints,\n",
    "\t\t\t\tcolorscale=\"Viridis\",\n",
    "\t\t\t\tshowscale=True,\n",
    "\t\t\t\tcolorbar=dict(\n",
    "            title=\"Co-occurrence\",\n",
    "                x=1.02,  # Position to the right of the fourth subplot\n",
    "                y=0.35,\n",
    "                len=0.3,\n",
    "                yanchor='top'      # Width of the colorbar\n",
    "        ),\n",
    "\t\t\tname=\"Breakpoint Co-occurrence\"\n",
    "\t\t\t),\n",
    "\t\t\trow=2, col=2\n",
    "\t\t)\n",
    "\n",
    "\t\tfig.update_layout(\n",
    "\t\t\theight=900,\n",
    "\t\t\twidth=1400,\n",
    "\t\t\ttitle=dict(\n",
    "\t\t\t\ttext=\"Network Analysis Dashboard\",\n",
    "\t\t\t\tx=0.5,\n",
    "\t\t\t\ty=0.95,\n",
    "\t\t\t\tfont=dict(size=22)\n",
    "\t\t\t),\n",
    "\t\t\tshowlegend=False,\n",
    "\t\t\ttemplate=\"simple_white\"\n",
    "\t\t)\n",
    "\n",
    "\t\tfont_size = 14\n",
    "\t\tfig.update_xaxes(title_text=\"Breakpoints\", title_font=dict(size=font_size), row=1, col=1)\n",
    "\t\tfig.update_yaxes(title_text=\"Patients\", title_font=dict(size=font_size), row=1, col=1)\n",
    "\t\tfig.update_xaxes(title_text=\"Breakpoints\", title_font=dict(size=font_size), row=1, col=2)\n",
    "\t\tfig.update_yaxes(title_text=\"Number of Patients\", title_font=dict(size=font_size), row=1, col=2)\n",
    "\t\tfig.update_xaxes(title_text=\"Patients\", title_font=dict(size=font_size), row=2, col=1)\n",
    "\t\tfig.update_yaxes(title_text=\"Patients\", title_font=dict(size=font_size), row=2, col=1)\n",
    "\t\tfig.update_xaxes(title_text=\"Breakpoints\", title_font=dict(size=font_size), row=2, col=2)\n",
    "\t\tfig.update_yaxes(title_text=\"Breakpoints\", title_font=dict(size=font_size), row=2, col=2)\n",
    "\n",
    "\t\treturn fig\n",
    "\t\n",
    "\tdef get_breakpoint_bins(self, top_percentile: float = 0.001, bottom_percentile: float = 0.001) -> tuple:\n",
    "\t\t\"\"\"\n",
    "\t\tCalculate the indexes of the breakpoints at the specified percentiles.\n",
    "\t\tReturns a tuple of two lists:\n",
    "\t\t- The first list contains the indexes of the top `top_percentile` breakpoints by degree.\n",
    "\t\t- The second list contains the indexes of the bottom `bottom_percentile` breakpoints by degree.\n",
    "\t\t\"\"\"\n",
    "\t\t# Ensure metrics are calculated before accessing them\n",
    "\t\tself._ensure_metrics_calculated()\n",
    "\t\tsorted_degrees = sorted(self.breakpoint_degrees)\n",
    "\t\ttop_cutoff = int(len(sorted_degrees) * top_percentile)\n",
    "\t\tbottom_cutoff = int(len(sorted_degrees) * (1 - bottom_percentile))\n",
    "\n",
    "\t\ttop_bins = [i for i in range(top_cutoff)]\n",
    "\t\tbottom_bins = [i for i in range(bottom_cutoff, len(sorted_degrees))]\n",
    "\n",
    "\t\treturn top_bins, bottom_bins\n",
    "\n",
    "\tdef print_summary_stats(self):\n",
    "\t\t# Ensure metrics are calculated before accessing them\n",
    "\t\tself._ensure_metrics_calculated()\n",
    "\t\tprint(f\"Network Summary Statistics:\")\n",
    "\t\tprint(f\"---------------------------\")\n",
    "\t\tprint(f\"Number of Patients: {len(self.patients)}\")\n",
    "\t\tprint(f\"Number of Breakpoints: {len(self.breakpoints)}\")\n",
    "\t\tprint(f\"Network Density: {self.density:.3f}\")\n",
    "\t\tprint(f\"Average Patient Degree: {np.mean(self.patient_degrees):.2f}\")\n",
    "\t\tprint(f\"Average Breakpoint Degree: {np.mean(self.breakpoint_degrees):.2f}\")\n",
    "\t\tprint(f\"\\nTop Breakpoints by Degree:\")\n",
    "\t\tfor bp, degree in sorted(zip(self.breakpoints, self.breakpoint_degrees), \n",
    "\t\t\t\t\t\t\t\tkey=lambda x: x[1], reverse=True)[:5]:\n",
    "\t\t\tprint(f\"  {bp}: {degree}\")\n",
    "\t\t# print(f\"\\nTop 10 Breakpoints by Degree Centrality:\")\n",
    "    \t# # Create list of (breakpoint, centrality) tuples and sort by centrality\n",
    "\t\t# centrality_pairs = list(zip(self.breakpoints, self.breakpoint_centrality))\n",
    "\t\t# sorted_by_centrality = sorted(centrality_pairs, key=lambda x: x[1], reverse=True)\n",
    "\t\t\n",
    "\t\t# # Print top 10\n",
    "\t\t# for bp, centrality in sorted_by_centrality[:10]:\n",
    "\t\t# \tprint(f\"  {bp}: {centrality:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Class on Toy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create toy data\n",
    "np.random.seed(420)  # for reproducibility\n",
    "\n",
    "patients = [f'P{i}' for i in range(1, 21)]  # 20 patients\n",
    "breakpoints = [f'BP{i}' for i in range(1, 16)]  # 15 breakpoints\n",
    "\n",
    "# Create random connections (each patient has 2-6 breakpoints)\n",
    "data = []\n",
    "for patient in patients:\n",
    "\tnum_breakpoints = np.random.randint(2, 7)\n",
    "\tpatient_breakpoints = np.random.choice(breakpoints, size=num_breakpoints, replace=False)\n",
    "\tfor bp in patient_breakpoints:\n",
    "\t\tdata.append({'patient_id': patient, 'breakpoint_id': bp})\n",
    "\n",
    "# Create Polars DataFrame\n",
    "df = pl.DataFrame(data)\n",
    "\n",
    "# now test the class\n",
    "# Create and display visualization\n",
    "analyzer = NetworkAnalyzer(df, patient_col='patient_id', breakpoint_col='breakpoint_id')\n",
    "fig = analyzer.create_dashboard()\n",
    "fig.show()\n",
    "\n",
    "# Print summary statistics\n",
    "analyzer.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%jmd \n",
    "{{ fig.to_html(include_plotlyjs=False, full_html=False, default_height=400, default_width=600) }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Adjacency Matrix as Precomputed `.npz` File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%jmd\n",
    "We can call the method `save_matrix` directly like so:\n",
    "\n",
    "> `analyzer.save_matrix('output/toy_data')`\n",
    "\n",
    "and then try reloading into an instance using the decorated class method `load_from_file` like so:\n",
    "\n",
    "> `analyzer_reloaded = NetworkAnalyzer.load_from_files('output/toy_data')`\n",
    "\n",
    "Finally print summary statistics as sanity check.\n",
    "\n",
    "> `analyzer.print_summary_stats()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep FT Breakpoints Seen in More than 9 Patients (1% of Cohort)\n",
    "\n",
    "The distribution of the sharedness degree of each unique breakpoint, is as expected, skewed towards having a lot of unique, patient-specific connections, and very few shared breakpoints across patients. \n",
    "\n",
    "We can try to visualize the adjacency matrix, but because of the massive matrix dimension we have (**988 patients x 43927 unique breakpoints**), it is best to first filter out patient-specific breakpoints first. In fact, as the putative FT neoantigen distribution is so skewed towards individualized presence, let's create a filtering threshold of keeping only the breakpoint IDs that are seen in **more than 9 patients (approximately 1% of the MyBrCa cohort)**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.-|m { input: true, output: true, input_fold: show}\n",
    "# This is DF with redundant duplicate breakpointID-sampleID pairing FILTERED OUT\n",
    "\n",
    "bpsample_poldf = pl.from_pandas(bpsample_pdf_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.-|m { input: false, output: true}\n",
    "\n",
    "display(HTML(\"Nonredundant <b>normal-filtered</b> BreakPoint–Patient Connection dimension: \" + f\"<b>{bpsample_poldf.shape}</b>\"))\n",
    "display(HTML(\"Displaying truncated table:\"))\n",
    "show(bpsample_poldf.head(10), maxBytes=0, classes=\"display compact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then count how many patients from our cohort these 53k unique breakpoints cover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: More explicit way\n",
    "unique_samples = set(bpsample_poldf[\"sampleID\"])\n",
    "len(unique_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we go back to the `sharednessDegree` Pandas dataFrame to select rows that has *sharednessDegree* > 9.\n",
    "\n",
    "Alternatively we can directly use the Polars dataframe `normfilt_my_sharedness` by running the command below:\n",
    "\n",
    "> bp_sharedness_gt9 = normfilt_mybrca_sharedness.filter(pl.col('sharednessDegree') > 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.-|m { input: true, output: true}\n",
    "bp_sharedness_gt9 = pl.from_pandas(breakpoint_counts).filter(pl.col('sharednessDegree') > 9)\n",
    "\n",
    "show(bp_sharedness_gt9, maxBytes=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explore how many breakpoints and patients are covered when we filter out just the patient-specific breakpoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_sharedness_gt1 = pl.from_pandas(breakpoint_counts).filter(pl.col('sharednessDegree') > 1)\n",
    "\n",
    "show(bp_sharedness_gt1, maxBytes=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the unique, filtered, thresholded elements in the `breakpointID` column of the filtered dataFrame above as the filtering list to keep only these same breakpoints in the other dataFrame used to instantiate `NetworkAnalyzer`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting ***connection*** dataframe can be used to instantiate a `NetworkAnalyzer` instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the *greater than 9 sharedness degree* connection dataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.-|m { input: true, output: true}\n",
    "# bp_sharedness_gt9 is the dataframe with unique breakpointIDs to be used as filter\n",
    "# bpsample_poldf is the dataframe to be filtered\n",
    "\n",
    "filt_bpsample_gt9_poldf = bpsample_poldf.filter(\n",
    "    pl.col(\"breakpointID\").is_in(bp_sharedness_gt9[\"breakpointID\"])\n",
    ")\n",
    "\n",
    "show(filt_bpsample_gt9_poldf, maxBytes=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the *greater than 1 sharedness degree* connection dataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.-|m { input: true, output: true}\n",
    "# bp_sharedness_gt1 is the dataframe with unique breakpointIDs to be used as filter\n",
    "# bpsample_poldf is the dataframe to be filtered\n",
    "\n",
    "filt_bpsample_gt1_poldf = bpsample_poldf.filter(\n",
    "    pl.col(\"breakpointID\").is_in(bp_sharedness_gt1[\"breakpointID\"])\n",
    ")\n",
    "\n",
    "show(filt_bpsample_gt1_poldf, maxBytes=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate NetworkAnalyzer on Filtered Data\n",
    "\n",
    "Now instantiate the class we built on our normal-filtered, unique-breakpoint-only subset dataFrame from MyBrCa FT data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.-|m { input: true, output: true, input_fold: show}\n",
    "analyzer_my_filt_gt9 = NetworkAnalyzer(filt_bpsample_gt9_poldf, patient_col='sampleID', breakpoint_col='breakpointID')\n",
    "\n",
    "# Print summary statistics\n",
    "analyzer_my_filt_gt9.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Percentage of Patients in the Cohort covered by breakpoints with more than 9 sharedness degree: {776/988*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.-|m { input: true, output: true, input_fold: show}\n",
    "analyzer_my_filt_gt1 = NetworkAnalyzer(filt_bpsample_gt1_poldf, patient_col='sampleID', breakpoint_col='breakpointID')\n",
    "\n",
    "# Print summary statistics\n",
    "analyzer_my_filt_gt1.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Percentage of Patients in the Cohort covered by breakpoints with more than 1 sharedness degree: {933/988*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Adjacency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = analyzer_my_filt_gt9.create_adjacency_matrix_plot()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%jmd \n",
    "{{ plot.to_html(include_plotlyjs=False, full_html=False, default_height=400, default_width=600) }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Breakpoint Degree Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = analyzer_my_filt_gt9.create_degree_distribution_plot()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%jmd \n",
    "{{ plot.to_html(include_plotlyjs=False, full_html=False, default_height=400, default_width=600) }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Patient Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = analyzer_my_filt_gt9.create_patient_similarity_matrix_plot()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%jmd \n",
    "{{ plot.to_html(include_plotlyjs=False, full_html=False, default_height=400, default_width=600) }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Breakpoint Co-Occurrence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = analyzer_my_filt_gt9.create_breakpoint_cooccurrence_plot()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%jmd \n",
    "{{ plot.to_html(include_plotlyjs=False, full_html=False, default_height=400, default_width=600) }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Implement Set Cover Problem Solution (Greedy Algorithm)**\n",
    "\n",
    "### Background\n",
    "\n",
    "If we have a set of unique FT breakpoints found in different subsets of patients in our cohort, it is possible to find the most minimal number of unique FT breakpoints that collectively appear in 100% of our filtered cohort.\n",
    "\n",
    "This is called **set cover problem** in set theory, and it is considered one of the classical problems in combinatorics, with real-world applications such as shift scheduling and operations optimization (https://en.wikipedia.org/wiki/Set_cover_problem).\n",
    "\n",
    "The traditional formalization of the problem is as follows:\n",
    "\n",
    "> Given a universe set $U$ of $n$ elements, $U := \\{ e1, . . . , en \\}$ and a collection of subsets\n",
    "of $U$, $S := \\{ S1, . . . , Sk \\}$ with a cost function $c : S → Q+$, the goal is to pick the minimum-cost subcollection\n",
    "of $S$ that covers all the elements of $U$.\n",
    "\n",
    "We can reformulate this problem in the context of patient cohort and unique breakpoint neoantigens found in the cohort as follows:\n",
    "\n",
    ">Given a patient cohort $U$ of $n$ patients, such that;\n",
    "\n",
    "$$U := \\{ P1, P2, . . . , Pn \\}$$ \n",
    "\n",
    ">and a collection of patient subsets, $S$, such that each subset represents the subset of patients within which a unique neoantigen ($NeoX$) can be found; \n",
    "\n",
    "$$S:= \\{ NeoA\\{ P1, P2, P3 \\}, NeoB\\{ P2, P3 \\}, . . . , NeoZ\\{ P1, P4 \\} \\}$$\n",
    "\n",
    ">with a *neoantigen quality* function; \n",
    "\n",
    "$$q : S → Q+$$ \n",
    "\n",
    ">the goal is to pick the ***most minimal*** subcollection of $S$ that covers all the patients in the cohort $U$ (while either minimizing cost function or maximizing neoantigen quality function)\n",
    "\n",
    "The resulting minimal subcollection of patient subsets represent the most minimal unique set of neoantigens that would cover the whole patient cohort.\n",
    "\n",
    "![Venn Diagram](../docs/assets/neo-patient-venn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the **connection** dataframe filtered for just breakpoints above sharedness degree of 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(filt_bpsample_gt9_poldf, maxBytes=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the **connection** dataframe filtered for just the breakpoints that are shared between at least 2 patients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(filt_bpsample_gt1_poldf, maxBytes=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using the ***connection*** dataframe above (that maps the bipartite relationships between `breakpointID`s and `sampleID`s), let us create a dataframe that is group by `breakpointID`, then aggregated on the `sampleID` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.-|m { input: false, output: true}\n",
    "# create a dataframe that group by breakpointID\n",
    "\n",
    "temp_df = (\n",
    "    filt_bpsample_gt9_poldf\n",
    "    .group_by(pl.col(\"breakpointID\"))\n",
    "    .agg([pl.col(\"sampleID\")])\n",
    "    .sort(\"breakpointID\")\n",
    ")\n",
    "\n",
    "# print(\"Type of temp_df:\", type(temp_df))\n",
    "# print(\"\\nSchema of temp_df:\")\n",
    "# print(temp_df.schema)\n",
    "# print(\"\\nFirst few rows of temp_df:\")\n",
    "# print(temp_df.head())\n",
    "\n",
    "# # Let's also look at a single row\n",
    "# first_row = temp_df.row(0)\n",
    "# print(\"\\nType of first row:\", type(first_row))\n",
    "# print(\"First row contents:\", first_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Next, let us create a dictionary that has each `breakpointID` value as the key, and a set object containing the `sampleID` for each breakpoint as the dictionary value. This allows us to map each breakpoint to a subset of patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.-|m { input: false, output: true}\n",
    "# Then convert the tuples of each row into a key-val dictionary entry. Turn the list values into sets\n",
    "\n",
    "bp_coverage_dict = {}\n",
    "for row in temp_df.iter_rows():\n",
    "    breakpoint_id = row[0]\n",
    "    sample_set = set(row[1])    # convert list to set\n",
    "    bp_coverage_dict[breakpoint_id] = sample_set\n",
    "\n",
    "# print(bp_coverage_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_gt9_matrix = analyzer_my_filt_gt9.adj_matrix_sparse.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_gt1_matrix = analyzer_my_filt_gt1.adj_matrix_sparse.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can implement a greedy algorithm in a function to search for the minimal subset of breakpoints that covers 100% of the patient space. As we have filtered out patients who do not has any breakpoint at all, this means that given enough computation, there would be a (or many) solution of our set cover problem that covers 100% of the input patient space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_minimal_covering_subset(adjacency_matrix, coverage_threshold, label_to_index_dict=None):\n",
    "    \"\"\"\n",
    "    Find minimal subset of rows that covers at least coverage_threshold fraction of columns,\n",
    "    with handling for dictionary-based label mapping.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    adjacency_matrix : np.ndarray\n",
    "        Binary matrix where rows are members of set A and columns are members of set B\n",
    "        1 indicates overlap, 0 indicates no overlap\n",
    "    coverage_threshold : float\n",
    "        Fraction of set B that needs to be covered (between 0 and 1)\n",
    "    label_to_index_dict : dict, optional\n",
    "        Dictionary mapping labels (strings) to indices (int)\n",
    "        Example: {'label1': 0, 'label2': 1, ...}\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (selected_indices, selected_labels, actual_coverage)\n",
    "        - selected_indices: List of numerical indices of selected rows\n",
    "        - selected_labels: List of original labels corresponding to the indices\n",
    "        - actual_coverage: Achieved coverage fraction\n",
    "    \"\"\"\n",
    "    # Create reverse mapping from index to label\n",
    "    if label_to_index_dict is not None:\n",
    "        index_to_label = {v: k for k, v in label_to_index_dict.items()}\n",
    "    \n",
    "    # Work with transpose of the matrix\n",
    "    working_matrix = adjacency_matrix.T\n",
    "    num_rows, num_cols = working_matrix.shape\n",
    "    \n",
    "    # Calculate target coverage\n",
    "    target_coverage = int(np.ceil(num_cols * coverage_threshold))\n",
    "    print(f\"Target coverage: {target_coverage} columns out of {num_cols}\")\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    selected_rows = []\n",
    "    covered_cols = np.zeros(num_cols, dtype=bool)\n",
    "    \n",
    "    while np.sum(covered_cols) < target_coverage:\n",
    "        # Calculate coverage gains for remaining rows\n",
    "        available_rows = [i for i in range(num_rows) if i not in selected_rows]\n",
    "        \n",
    "        if not available_rows:\n",
    "            break\n",
    "            \n",
    "        coverage_gains = np.array([\n",
    "            np.sum(~covered_cols & (working_matrix[i] == 1))\n",
    "            for i in available_rows\n",
    "        ])\n",
    "        \n",
    "        if np.max(coverage_gains) == 0:\n",
    "            print(\"No more improvements possible\")\n",
    "            break\n",
    "            \n",
    "        # Select the row that covers the most new columns\n",
    "        best_row_idx = available_rows[np.argmax(coverage_gains)]\n",
    "        selected_rows.append(best_row_idx)\n",
    "        \n",
    "        # Print progress with label if available\n",
    "        if label_to_index_dict is not None:\n",
    "            label = index_to_label[best_row_idx]\n",
    "            new_coverage = np.sum(~covered_cols & (working_matrix[best_row_idx] == 1))\n",
    "            print(f\"Selected {label} (index {best_row_idx}) covering {new_coverage} new columns\")\n",
    "        \n",
    "        # Update covered columns\n",
    "        covered_cols = covered_cols | (working_matrix[best_row_idx] == 1)\n",
    "    \n",
    "    # Calculate actual coverage achieved\n",
    "    actual_coverage = np.sum(covered_cols) / num_cols\n",
    "    print(f\"Achieved {actual_coverage:.2%} coverage\")\n",
    "    \n",
    "    # Convert indices to labels if dictionary provided\n",
    "    if label_to_index_dict is not None:\n",
    "        selected_labels = [index_to_label[idx] for idx in selected_rows]\n",
    "    else:\n",
    "        selected_labels = None\n",
    "    \n",
    "    return selected_rows, selected_labels, actual_coverage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the function on the matrix of breakpoints shared between more than 9 patients first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakpoint_dict_gt9 = analyzer_my_filt_gt9.breakpoint_idx_dict\n",
    "filt_bp_indices_gt9, filt_bp_labels_gt9, coverage_gt9 = find_minimal_covering_subset(filt_gt9_matrix, coverage_threshold=0.4, label_to_index_dict=breakpoint_dict_gt9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Minimal coverage percent (%): {coverage_gt9 * 100}\")\n",
    "print(f\"Minimal set of breakpoints: {filt_bp_indices_gt9}\")\n",
    "print(f\"Length of set: {len(filt_bp_indices_gt9)}\")\n",
    "# print(f\"Labels of the minimal cover set of breakpoints: {filt_bp_labels_gt9}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run the same function on the matrix of breakpoints shared between 2 or more patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakpoint_dict_gt1 = analyzer_my_filt_gt1.breakpoint_idx_dict\n",
    "filt_bp_indices_gt1, filt_bp_labels_gt1, coverage_gt1 = find_minimal_covering_subset(filt_gt1_matrix, coverage_threshold=1.0, label_to_index_dict=breakpoint_dict_gt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Minimal coverage percent (%): {coverage_gt1 * 100}\")\n",
    "print(f\"Minimal set of breakpoints: {filt_bp_indices_gt1}\")\n",
    "print(f\"Length of set: {len(filt_bp_indices_gt1)}\")\n",
    "print(f\"Labels of the minimal cover set of breakpoints: {filt_bp_labels_gt1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_dict_gt9 = analyzer_my_filt_gt9.patient_idx_dict\n",
    "# print(patient_dict_gt9)\n",
    "patient_dict_gt1 = analyzer_my_filt_gt1.patient_idx_dict\n",
    "# print(patient_dict_gt1)\n",
    "# subset original matrix\n",
    "minimal_set_cover_subset_gt9_matrix = filt_gt9_matrix[:, filt_bp_indices_gt9]\n",
    "print(minimal_set_cover_subset_gt9_matrix.shape)\n",
    "minimal_set_cover_subset_gt1_matrix = filt_gt1_matrix[:, filt_bp_indices_gt1]\n",
    "print(minimal_set_cover_subset_gt1_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the subset dataframes containing the minimal breakpoint sets for the 'greater than 9 sharedness degree' and 'greater than 1 sharedness degree' matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset the original df used to generate analyzer_my_filt instance\n",
    "\n",
    "filt_bpsample_minsetcover_gt9_poldf = bpsample_poldf.filter(\n",
    "    pl.col(\"breakpointID\").is_in(filt_bp_labels_gt9)\n",
    ")\n",
    "\n",
    "show(filt_bpsample_minsetcover_gt9_poldf, maxBytes=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset the original df used to generate analyzer_my_filt instance\n",
    "\n",
    "filt_bpsample_minsetcover_gt1_poldf = bpsample_poldf.filter(\n",
    "    pl.col(\"breakpointID\").is_in(filt_bp_labels_gt1)\n",
    ")\n",
    "\n",
    "show(filt_bpsample_minsetcover_gt1_poldf, maxBytes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new NetworkAnalyzer instance\n",
    "analyzer_subset_gt9_filt = NetworkAnalyzer(filt_bpsample_minsetcover_gt9_poldf, patient_col=\"sampleID\", breakpoint_col=\"breakpointID\")\n",
    "analyzer_subset_gt1_filt = NetworkAnalyzer(filt_bpsample_minsetcover_gt1_poldf, patient_col=\"sampleID\", breakpoint_col=\"breakpointID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary statistics\n",
    "analyzer_subset_gt9_filt.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary statistics\n",
    "analyzer_subset_gt1_filt.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = analyzer_subset_gt9_filt.create_adjacency_matrix_plot()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = analyzer_subset_gt1_filt.create_adjacency_matrix_plot()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Filtering Out non-TNBCs**\n",
    "\n",
    "We can filter out rows corresponding to `sampleID` more than 172, because these are not TNBC samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(bp_sharedness_gt1, maxBytes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_bpsample_gt1_poldf = bpsample_poldf.filter(\n",
    "    pl.col(\"breakpointID\").is_in(bp_sharedness_gt1[\"breakpointID\"])\n",
    ")\n",
    "\n",
    "show(filt_bpsample_gt1_poldf, maxBytes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_bpsample_gt1_tnbc_poldf = filt_bpsample_gt1_poldf.with_columns(\n",
    "    pl.col('sampleID').str.replace('P', '').cast(pl.Int64).alias('sampleID')\n",
    ")\n",
    "filt_bpsample_gt1_tnbc_poldf =  filt_bpsample_gt1_tnbc_poldf.filter(pl.col('sampleID') < 173)\n",
    "\n",
    "show(filt_bpsample_gt1_tnbc_poldf, maxBytes=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **SANITY CHECK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the original Pandas df that was normal-filtered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original number of rows:\", len(bp_sample_array_pdf))\n",
    "print(\"Number of rows after removing duplicates:\", len(bpsample_pdf_unique))\n",
    "print(\"Number of duplicates removed:\", len(bp_sample_array_pdf) - len(bpsample_pdf_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'P' prefix and convert to integer\n",
    "bpsample_pdf_unique_converted = bpsample_pdf_unique.copy()\n",
    "bpsample_pdf_unique_converted['sampleID'] = bpsample_pdf_unique_converted['sampleID'].str.replace('P', '').astype(int)\n",
    "show(bpsample_pdf_unique_converted, maxBytes=0)\n",
    "\n",
    "# Group by breakpointID and count unique sampleIDs\n",
    "breakpoint_counts_tnbc = bpsample_pdf_unique_converted.groupby('breakpointID')['sampleID'].nunique().reset_index()\n",
    "\n",
    "# Rename the column for clarity\n",
    "breakpoint_counts_tnbc = breakpoint_counts_tnbc.rename(columns={'sampleID': 'sharednessDegree'})\n",
    "\n",
    "tnbc_breakpoints_gt1_pandf = breakpoint_counts_tnbc[breakpoint_counts_tnbc['sharednessDegree'] > 1]\n",
    "show(tnbc_breakpoints_gt1_pandf, maxBytes=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now filter for TNBC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnbc_only_breakpoints_poldf = pl.from_pandas(bpsample_pdf_unique_converted).filter(pl.col('sampleID') < 173)\n",
    "\n",
    "show(tnbc_only_breakpoints_poldf, maxBytes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnbc_only_gt1_breakpoints_uniq_poldf = tnbc_only_breakpoints_poldf.filter(\n",
    "    pl.col(\"breakpointID\").is_in(tnbc_breakpoints_gt1_pandf[\"breakpointID\"]))\n",
    "\n",
    "show(tnbc_only_gt1_breakpoints_uniq_poldf, maxBytes=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate NetworkAnalyzer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_tnbc_filt_gt1 = NetworkAnalyzer(tnbc_only_gt1_breakpoints_uniq_poldf, patient_col='sampleID', breakpoint_col='breakpointID')\n",
    "\n",
    "# Print summary statistics\n",
    "analyzer_tnbc_filt_gt1.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_tnbc_gt1_matrix = analyzer_tnbc_filt_gt1.adj_matrix_sparse.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakpoint_dict_gt1_tnbc = analyzer_tnbc_filt_gt1.breakpoint_idx_dict\n",
    "filt_bp_indices_gt1_tnbc, filt_bp_labels_gt1_tnbc, coverage_gt1_tnbc = find_minimal_covering_subset(filt_tnbc_gt1_matrix, coverage_threshold=1.0, label_to_index_dict=breakpoint_dict_gt1_tnbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Minimal coverage percent (%): {coverage_gt1_tnbc * 100}\")\n",
    "print(f\"Minimal set of breakpoints: {filt_bp_indices_gt1_tnbc}\")\n",
    "print(f\"Length of set: {len(filt_bp_indices_gt1_tnbc)}\")\n",
    "# print(f\"Labels of the minimal cover set of breakpoints: {filt_bp_labels_gt1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(filt_bp_labels_gt1_tnbc))\n",
    "print(len(set(filt_bp_labels_gt1_tnbc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read 85 Breakpoint List from Joyce for Filtering\n",
    "\n",
    "We can read the breakpoint list from Joyce to restrict the `tnbc_only_gt1_breakpoints_uniq_poldf` dataframe to just TNBC data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read breakpointIDs from text file\n",
    "with open('../data/tnbc_breakpoints.txt', 'r') as f:\n",
    "    breakpoint_list = [line.strip() for line in f]\n",
    "\n",
    "print(breakpoint_list)\n",
    "print(len(set(breakpoint_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read breakpointIDs from text file\n",
    "with open('../data/tnbc_breakpoints_validated.txt', 'r') as f:\n",
    "    breakpoint_val_list = [line.strip() for line in f]\n",
    "\n",
    "print(breakpoint_val_list)\n",
    "print(len(set(breakpoint_val_list)))\n",
    "\n",
    "# # Filter DataFrame to keep only rows where breakpointID is in the list\n",
    "# tnbc_connection_validated_poldf = tnbc_only_gt1_breakpoints_uniq_poldf.filter(pl.col('breakpointID').is_in(breakpoint_list))\n",
    "\n",
    "# show(tnbc_connection_validated_poldf, maxBytes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in breakpoint_val_list:\n",
    "\tif i in set(filt_bp_labels_gt1):\n",
    "\t\tprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(breakpoint_list) & set(filt_bp_labels_gt1))\n",
    "print(len(set(breakpoint_list) & set(filt_bp_labels_gt1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Work Out The Coverage of a Specific Subset of Breakpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coverage_from_breakpoints(adjacency_matrix, selected_breakpoints, label_to_index_dict=None):\n",
    "    \"\"\"\n",
    "    Calculate the coverage of samples achieved by a given set of breakpoints.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    adjacency_matrix : np.ndarray\n",
    "        Binary matrix where rows are breakpoints and columns are samples\n",
    "        1 indicates overlap, 0 indicates no overlap\n",
    "    selected_breakpoints : list\n",
    "        List of breakpoint indices or labels to evaluate\n",
    "    label_to_index_dict : dict, optional\n",
    "        Dictionary mapping breakpoint labels to matrix indices\n",
    "        Example: {'breakpoint1': 0, 'breakpoint2': 1, ...}\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (coverage_fraction, covered_samples, uncovered_samples)\n",
    "        - coverage_fraction: Fraction of samples covered by the selected breakpoints\n",
    "        - covered_samples: Indices of covered samples\n",
    "        - uncovered_samples: Indices of samples not covered by any selected breakpoint\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Convert labels to indices if dictionary is provided\n",
    "    if label_to_index_dict is not None:\n",
    "        try:\n",
    "            selected_indices = [label_to_index_dict[label] for label in selected_breakpoints]\n",
    "        except KeyError as e:\n",
    "            raise KeyError(f\"Breakpoint label {e} not found in label dictionary\")\n",
    "    else:\n",
    "        selected_indices = selected_breakpoints\n",
    "    \n",
    "    # Validate indices\n",
    "    if max(selected_indices) >= adjacency_matrix.shape[0]:\n",
    "        raise ValueError(\"Selected breakpoint index exceeds matrix dimensions\")\n",
    "    \n",
    "    # Extract rows for selected breakpoints\n",
    "    selected_matrix = adjacency_matrix[selected_indices]\n",
    "    \n",
    "    # Calculate which samples are covered (have at least one 1 in any selected breakpoint)\n",
    "    covered_samples = np.where(np.any(selected_matrix == 1, axis=0))[0]\n",
    "    \n",
    "    # Calculate which samples are not covered\n",
    "    uncovered_samples = np.where(~np.any(selected_matrix == 1, axis=0))[0]\n",
    "    \n",
    "    # Calculate coverage fraction\n",
    "    total_samples = adjacency_matrix.shape[1]\n",
    "    coverage_fraction = len(covered_samples) / total_samples\n",
    "    \n",
    "    return coverage_fraction, covered_samples, uncovered_samples\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    import numpy as np\n",
    "    \n",
    "    # Example adjacency matrix (5 breakpoints × 8 samples)\n",
    "    matrix = np.array([\n",
    "        [1, 1, 0, 0, 1, 0, 0, 1],  # breakpoint 0\n",
    "        [0, 1, 1, 0, 0, 1, 0, 0],  # breakpoint 1\n",
    "        [1, 0, 0, 1, 0, 0, 1, 0],  # breakpoint 2\n",
    "        [0, 0, 1, 1, 0, 0, 0, 1],  # breakpoint 3\n",
    "        [1, 0, 0, 0, 1, 1, 0, 0],  # breakpoint 4\n",
    "    ])\n",
    "    \n",
    "    # Example with indices\n",
    "    selected_breakpoints = [0, 2]  # Using breakpoints 0 and 2\n",
    "    coverage, covered, uncovered = calculate_coverage_from_breakpoints(matrix, selected_breakpoints)\n",
    "    \n",
    "    print(f\"Coverage with breakpoints {selected_breakpoints}: {coverage:.2%}\")\n",
    "    print(f\"Covered samples: {covered}\")\n",
    "    print(f\"Uncovered samples: {uncovered}\")\n",
    "    \n",
    "    # Example with labels\n",
    "    labels = {\n",
    "        'break_A': 0,\n",
    "        'break_B': 1,\n",
    "        'break_C': 2,\n",
    "        'break_D': 3,\n",
    "        'break_E': 4\n",
    "    }\n",
    "    \n",
    "    selected_labels = ['break_A', 'break_C']\n",
    "    coverage, covered, uncovered = calculate_coverage_from_breakpoints(\n",
    "        matrix, \n",
    "        selected_labels,\n",
    "        labels\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nCoverage with breakpoints {selected_labels}: {coverage:.2%}\")\n",
    "    print(f\"Covered samples: {covered}\")\n",
    "    print(f\"Uncovered samples: {uncovered}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakpoint_val_list_edit = ['10:49885203-10:42791627', '18:32092886-18:32068298', '19:804438-19:726138', '1:26239838-1:26197686', '11:102448812-11:102610030', '8:60743097-8:61376605', '3:132713126-3:138131282', '9:92108740-9:92210889']\n",
    "\n",
    "coverage, covered, uncovered = calculate_coverage_from_breakpoints(filt_tnbc_gt1_matrix, breakpoint_val_list_edit, label_to_index_dict=breakpoint_dict_gt1_tnbc)\n",
    "\n",
    "print(f\"Coverage with breakpoints {breakpoint_val_list_edit}: {coverage:.2%}\")\n",
    "print(f\"Covered samples: {covered}\")\n",
    "print(f\"Uncovered samples: {uncovered}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
