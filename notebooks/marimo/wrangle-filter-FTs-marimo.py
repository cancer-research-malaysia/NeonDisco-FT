import marimo

__generated_with = "0.13.15"
app = marimo.App()


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""## **Collating and Filtering Raw Fusion Calling Output TSV**""")
    return


@app.cell(hide_code=True)
def _():
    import marimo as mo
    return (mo,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""This notebook details the collate-FTs process for the TSV file generated by individual fusion caller and how we combine these information and filter them.""")
    return


@app.cell
def _():
    # load raw tsv
    import polars as pl

    # load_tsv
    arr_pldf = pl.scan_csv('/home/ec2-user/repos/FT-NeonDisco/data/minimal-test/124T_arr.tsv', separator='\t').collect()
    fc_pldf = pl.scan_csv('/home/ec2-user/repos/FT-NeonDisco/data/minimal-test/124T_fc.tsv', separator='\t').fill_null(".").collect()
    sf_pldf = pl.scan_csv('/home/ec2-user/repos/FT-NeonDisco/data/minimal-test/124T_sf.tsv', separator='\t').collect()
    return arr_pldf, fc_pldf, pl, sf_pldf


@app.cell
def _(arr_pldf):
    arr_pldf
    return


@app.cell
def _(sf_pldf):
    sf_pldf
    return


@app.cell
def _(fc_pldf):
    fc_pldf
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    Now we need to define the column structure for the collated file. 

    **Base columns:**

    `fusionTranscriptID    fusionGenePair    breakpointID    5pStrand    3pStrand originalTool    sampleID    sampleNum    sampleNum_Padded`

    **Tool specific columns:**

    Arriba: `5pSite_ARR    3pSite_ARR    mutationType_ARR confidenceLabel_ARR`

    FusionCatcher: `fusionPairAnnotation_FC predictedEffect_FC`

    STARFusion: `largeAnchorSupport_SF junctionReadCount_SF    spanningFragCount_SF    fusionPairAnnotation_SF`

    """
    )
    return


@app.cell(hide_code=True)
def _():
    # #!/usr/bin/env python3

    # import os
    # import re
    # import sys
    # import argparse
    # import polars as pl
    # from pathlib import Path
    # from typing import List, Tuple, Dict, Optional

    # def extract_sample_num(filename: str, tool_suffix: str) -> Optional[str]:
    #     pattern = rf'^(\d+)[TN]_{re.escape(tool_suffix)}\.tsv$'
    #     match = re.search(pattern, os.path.basename(filename))
    #     return match.group(1) if match else None

    # def wrangle_df(file_path: str, sample_id: str, sample_num: str, tool_name: str) -> pl.LazyFrame:
    #     """Process input file based on the tool name and return a standardized lazy DataFrame."""
    #     # Read with null replacement
    #     lazy_df = pl.scan_csv(file_path, separator="\t").fill_null(".")

    #     match tool_name:
    #         case 'Arriba':
    #             # Base columns
    #             base_columns = [
    #                 (pl.col('#gene1') + "::" + pl.col('gene2') + '__' + pl.col('breakpoint1').str.replace("chr", "") + "-" + pl.col('breakpoint2').str.replace("chr", "")).alias("fusionTranscriptID"),
    #                 (pl.col('#gene1') + "::" + pl.col('gene2')).alias("fusionGenePair"),
    #                 (pl.col('breakpoint1').str.replace("chr", "") + "-" + pl.col('breakpoint2').str.replace("chr", "")).alias("breakpointID"),
    #                 (pl.col('strand1(gene/fusion)').str.split("/").list.get(1)).alias("5pStrand"),
    #                 (pl.col('strand2(gene/fusion)').str.split("/").list.get(1)).alias("3pStrand"),
    #                 pl.lit(tool_name).alias("originalTool"),
    #                 pl.lit(sample_id).alias("sampleID"),
    #                 pl.lit(sample_num).cast(pl.Int64).alias("sampleNum"),
    #                 pl.lit(sample_num).cast(pl.Utf8).str.zfill(4).alias("sampleNum_Padded")
    #             ]

    #             # Arriba-specific columns
    #             arriba_columns = [
    #                 (pl.col('site1')).alias("5pSite_ARR"),
    #                 (pl.col('site2')).alias("3pSite_ARR"), 
    #                 (pl.col('type')).alias("mutationType_ARR"),
    #                 (pl.col('confidence')).alias("confidenceLabel_ARR")
    #             ]

    #             # Placeholder columns for other tools
    #             fc_columns = [
    #                 pl.lit('.').alias("fusionPairAnnotation_FC"),
    #                 pl.lit('.').alias("predictedEffect_FC")
    #             ]

    #             sf_columns = [
    #                 pl.lit('.').alias("largeAnchorSupport_SF"),
    #                 pl.lit(None).cast(pl.Int64).alias("junctionReadCount_SF"),
    #                 pl.lit(None).cast(pl.Int64).alias("spanningFragCount_SF"),
    #                 pl.lit('.').alias("fusionPairAnnotation_SF")
    #             ]

    #             return lazy_df.select(base_columns + arriba_columns + fc_columns + sf_columns)

    #         case 'FusionCatcher':
    #             # Handle NaN values in gene symbol columns by replacing with gene IDs
    #             gene1_expr = (
    #                 pl.when(pl.col('Gene_1_symbol(5end_fusion_partner)').is_null() | (pl.col('Gene_1_symbol(5end_fusion_partner)') == "."))
    #                 .then(pl.col('Gene_1_id(5end_fusion_partner)'))
    #                 .otherwise(pl.col('Gene_1_symbol(5end_fusion_partner)'))
    #             )

    #             gene2_expr = (
    #                 pl.when(pl.col('Gene_2_symbol(3end_fusion_partner)').is_null() | (pl.col('Gene_2_symbol(3end_fusion_partner)') == "."))
    #                 .then(pl.col('Gene_2_id(3end_fusion_partner)'))
    #                 .otherwise(pl.col('Gene_2_symbol(3end_fusion_partner)'))
    #             )

    #             # Base columns
    #             base_columns = [
    #                 (gene1_expr + "::" + gene2_expr + '__' + pl.col('Fusion_point_for_gene_1(5end_fusion_partner)').str.split(':').list.slice(0, 2).list.join(':') + "-" + pl.col('Fusion_point_for_gene_2(3end_fusion_partner)').str.split(':').list.slice(0, 2).list.join(':')).alias("fusionTranscriptID"),
    #                 (gene1_expr + "::" + gene2_expr).alias("fusionGenePair"),
    #                 (pl.col('Fusion_point_for_gene_1(5end_fusion_partner)').str.split(':').list.slice(0, 2).list.join(':') + "-" + pl.col('Fusion_point_for_gene_2(3end_fusion_partner)').str.split(':').list.slice(0, 2).list.join(':')).alias("breakpointID"),
    #                 (pl.col('Fusion_point_for_gene_1(5end_fusion_partner)').str.split(":").list.get(2)).alias("5pStrand"),
    #                 (pl.col('Fusion_point_for_gene_2(3end_fusion_partner)').str.split(":").list.get(2)).alias("3pStrand"),
    #                 pl.lit(tool_name).alias("originalTool"),
    #                 pl.lit(sample_id).alias("sampleID"),
    #                 pl.lit(sample_num).cast(pl.Int64).alias("sampleNum"),
    #                 pl.lit(sample_num).cast(pl.Utf8).str.zfill(4).alias("sampleNum_Padded")
    #             ]

    #             # FusionCatcher-specific columns
    #             fc_columns = [
    #                 pl.col('Fusion_description').alias("fusionPairAnnotation_FC")
    #             ]

    #             # Check if 'Predicted_effect' column exists and add it
    #             if 'Predicted_effect' in lazy_df.collect_schema().names():
    #                 fc_columns.append(pl.col('Predicted_effect').alias('predictedEffect_FC'))
    #             else:
    #                 fc_columns.append(pl.lit('.').alias('predictedEffect_FC'))

    #             # Placeholder columns for other tools
    #             arr_columns = [
    #                 pl.lit('.').alias("5pSite_ARR"),
    #                 pl.lit('.').alias("3pSite_ARR"),
    #                 pl.lit('.').alias("mutationType_ARR"),
    #                 pl.lit('.').alias("confidenceLabel_ARR")
    #             ]

    #             sf_columns = [
    #                 pl.lit('.').alias("largeAnchorSupport_SF"),
    #                 pl.lit(None).cast(pl.Int64).alias("junctionReadCount_SF"),
    #                 pl.lit(None).cast(pl.Int64).alias("spanningFragCount_SF"),
    #                 pl.lit('.').alias("fusionPairAnnotation_SF")
    #             ]

    #             return lazy_df.select(base_columns + arr_columns + fc_columns + sf_columns)

    #         case 'STARFusion':
    #             # Extract gene names with fallback to gene IDs
    #             gene1_expr = (
    #                 pl.when(pl.col('LeftGene').str.split("^").list.get(0) != "")
    #                 .then(pl.col('LeftGene').str.split("^").list.get(0))
    #                 .otherwise(
    #                     pl.col('LeftGene').str.split("^").list.get(1).str.split(".").list.get(0)
    #                 )
    #             )
    #             gene2_expr = (
    #                 pl.when(pl.col('RightGene').str.split("^").list.get(0) != "")
    #                 .then(pl.col('RightGene').str.split("^").list.get(0))
    #                 .otherwise(
    #                     pl.col('RightGene').str.split("^").list.get(1).str.split(".").list.get(0)
    #                 )
    #             )

    #             # Handle breakpoints: Format from chr17:38243106:+ to 17:38243106
    #             left_breakpoint = (
    #                 pl.col('LeftBreakpoint')
    #                 .str.replace(r'^chr', '')  # Remove 'chr' prefix
    #                 .str.split(':').list.slice(0, 2).list.join(':')  # Convert format
    #             )

    #             right_breakpoint = (
    #                 pl.col('RightBreakpoint')
    #                 .str.replace(r'^chr', '')  # Remove 'chr' prefix
    #                 .str.split(':').list.slice(0, 2).list.join(':')  # Convert format
    #             )

    #             # Extract strands from breakpoint columns
    #             left_strand = pl.col('LeftBreakpoint').str.split(':').list.get(2)
    #             right_strand = pl.col('RightBreakpoint').str.split(':').list.get(2)

    #             # Base columns
    #             base_columns = [
    #                 (gene1_expr + "::" + gene2_expr + '__' + left_breakpoint + "-" + right_breakpoint).alias("fusionTranscriptID"),
    #                 (gene1_expr + "::" + gene2_expr).alias("fusionGenePair"),
    #                 (left_breakpoint + "-" + right_breakpoint).alias("breakpointID"),
    #                 left_strand.alias("5pStrand"),
    #                 right_strand.alias("3pStrand"),
    #                 pl.lit(tool_name).alias("originalTool"),
    #                 pl.lit(sample_id).alias("sampleID"),
    #                 pl.lit(sample_num).cast(pl.Int64).alias("sampleNum"),
    #                 pl.lit(sample_num).cast(pl.Utf8).str.zfill(4).alias("sampleNum_Padded")
    #             ]

    #             # STARFusion-specific columns
    #             sf_columns = [
    #                 pl.col('LargeAnchorSupport').alias("largeAnchorSupport_SF"),
    #                 pl.col('JunctionReadCount').alias("junctionReadCount_SF"),
    #                 pl.col('SpanningFragCount').alias("spanningFragCount_SF"),
    #                 pl.col('annots').alias("fusionPairAnnotation_SF")
    #             ]

    #             # Placeholder columns for other tools
    #             arr_columns = [
    #                 pl.lit('.').alias("5pSite_ARR"),
    #                 pl.lit('.').alias("3pSite_ARR"),
    #                 pl.lit('.').alias("mutationType_ARR"),
    #                 pl.lit('.').alias("confidenceLabel_ARR")
    #             ]

    #             fc_columns = [
    #                 pl.lit('.').alias("fusionPairAnnotation_FC"),
    #                 pl.lit('.').alias("predictedEffect_FC")
    #             ]

    #             return lazy_df.select(base_columns + arr_columns + fc_columns + sf_columns)

    #         case _:
    #             raise ValueError(f"Unsupported tool name: {tool_name}")

    # def collate_fusion_data(
    #     sample_id: str, 
    #     output_filename: str, 
    #     input_files: List[Tuple[str, str]]
    # ) -> None:
    #     """
    #     Collate fusion transcript data from multiple fusion detection tools.

    #     Args:
    #         sample_id: The sample identifier
    #         output_filename: Output file prefix (no file extension)
    #         input_files: List of tuples containing (file_path, tool_suffix)
    #     """
    #     # Map tool suffixes to full tool names
    #     tool_name_map = {
    #         'arr': 'Arriba', 
    #         'fc': 'FusionCatcher',
    #         'sf': 'STARFusion'
    #     }

    #     print(f"Sample ID: {sample_id}")
    #     print(f"Input files: {input_files}")
    #     print(f"Output filename: {output_filename}")

    #     # Initialize an empty list to store the lazy DataFrames
    #     lazy_dfs = []

    #     for input_path, suffix in input_files:
    #         if not Path(input_path).exists():
    #             print(f"Error: File {input_path} does not exist. Skipping...")
    #             continue

    #         print('Setting tool name...')
    #         tool_name = tool_name_map.get(suffix)

    #         if not tool_name:
    #             print(f"Warning: Unknown tool suffix '{suffix}'. Skipping file {input_path}")
    #             continue

    #         # Extract sample number
    #         sample_num = extract_sample_num(input_path, suffix)
    #         if not sample_num:
    #             print(f"Error: Could not extract sample number from {input_path}.")
    #             sys.exit(1)

    #         print(f'Reading {tool_name} of {sample_id} TSV file...(sample ID: {sample_id})')

    #         # Create a lazy dataframe for each file
    #         try:
    #             lazy_df = wrangle_df(input_path, sample_id, sample_num, tool_name)
    #             # Append the lazy DataFrame to the list
    #             lazy_dfs.append(lazy_df)
    #         except Exception as e:
    #             print(f"Error processing {input_path}: {str(e)}")
    #             continue

    #     if not lazy_dfs:
    #         raise ValueError("No valid input files were processed. Aborting...")

    #     print(f"Concatenating lazy DataFrames from {len(lazy_dfs)} tools...")

    #     # Concatenate all lazy DataFrames
    #     combined_lazy_df = pl.concat(lazy_dfs, rechunk=True)

    #     print("Concatenation completed. Collecting...")

    #     # Define categorical columns for the new structure
    #     base_categoricals = [
    #         "fusionTranscriptID",
    #         "fusionGenePair",
    #         "breakpointID",
    #         "5pStrand",
    #         "3pStrand",
    #         "originalTool",
    #         "sampleID"
    #     ]

    #     # Tool-specific categorical columns
    #     tool_categoricals = [
    #         # Arriba columns
    #         "5pSite_ARR",
    #         "3pSite_ARR",
    #         "mutationType_ARR",
    #         "confidenceLabel_ARR",
    #         # FusionCatcher columns
    #         "predictedEffect_FC",
    #         # STARFusion columns
    #         "largeAnchorSupport_SF",
    #     ]

    #     all_categoricals = base_categoricals + tool_categoricals

    #     # Integer columns
    #     ints = [
    #         "sampleNum",
    #         "junctionReadCount_SF",
    #         "spanningFragCount_SF"
    #     ]

    #     # Cast columns to appropriate types
    #     results = combined_lazy_df.with_columns(
    #         [pl.col(col).cast(pl.Categorical) for col in all_categoricals]
    #     ).with_columns(
    #         [pl.col(col).cast(pl.Int64) for col in ints]
    #     ).collect()

    #     print("DataFrame collected.")
    #     print(f"Final DataFrame shape: {results.shape}")
    #     print(f"Columns: {results.columns}")
    #     print(f"Saving as parquet and tsv files...")

    #     # Save as parquet and tsv
    #     results.write_parquet(f"{output_filename}.parquet")
    #     results.write_csv(f"{output_filename}.tsv", separator="\t")

    #     print("Done.")

    # def parse_arguments():
    #     """Parse command line arguments using argparse."""
    #     parser = argparse.ArgumentParser(
    #         description="Collate fusion transcript data from multiple fusion detection tools."
    #     )

    #     parser.add_argument("--sample_id", "-s", required=True, help="Sample identifier")
    #     parser.add_argument("--output", "-o", required=True, help="Output filename path (without extension)")

    #     # Add input file arguments
    #     input_group = parser.add_argument_group('input files')
    #     input_group.add_argument("--arriba", help="Path to Arriba output file")
    #     input_group.add_argument("--fusioncatcher", help="Path to FusionCatcher output file")
    #     input_group.add_argument("--starfusion", help="Path to STARFusion output file")

    #     # Add a way to provide additional inputs in the original positional format
    #     parser.add_argument("--inputs", nargs="+", help="Additional input files in format: file1 suffix1 file2 suffix2...")

    #     return parser.parse_args()

    # def main():
    #     """Main function to parse arguments and call the collate_fusion_data function."""
    #     # Support both new argparse style and legacy positional arguments
    #     if len(sys.argv) > 1 and sys.argv[1].startswith('-'):
    #         # Use argparse for modern argument parsing
    #         args = parse_arguments()

    #         # Prepare input files list
    #         input_files = []

    #         # Add specified tool files
    #         if args.arriba:
    #             print(f"Adding Arriba file: {args.arriba}")
    #             input_files.append((os.path.abspath(args.arriba), 'arr'))
    #         if args.fusioncatcher:
    #             print(f"Adding FusionCatcher file: {args.fusioncatcher}")
    #             input_files.append((os.path.abspath(args.fusioncatcher), 'fc'))
    #         if args.starfusion:
    #             print(f"Adding STARFusion file: {args.starfusion}")
    #             input_files.append((os.path.abspath(args.starfusion), 'sf'))

    #         # Add additional inputs if provided
    #         if args.inputs:
    #             if len(args.inputs) % 2 != 0:
    #                 print("Error: Additional inputs must be provided as pairs of file path and tool suffix.")
    #                 sys.exit(1)

    #             for i in range(0, len(args.inputs), 2):
    #                 input_files.append((os.path.abspath(args.inputs[i]), args.inputs[i+1]))

    #         print(f"Input files: {input_files}")
    #         # Call the collate function
    #         try:
    #             collate_fusion_data(
    #                 sample_id=args.sample_id,
    #                 output_filename=args.output,
    #                 input_files=input_files
    #             )
    #         except Exception as e:
    #             print(f"Error: {str(e)}")
    #             sys.exit(1)

    # if __name__ == "__main__":
    #     main()
    return


@app.cell
def _(mo):
    mo.md(r"""Run the script above on the terminal as a py script with the right arguments.""")
    return


@app.cell
def _(mo):
    mo.md(r"""### Loading Collated TSV for Filtering Steps""")
    return


@app.cell
def _(pl):
    # load collated TSV

    collated_df = pl.scan_csv('/home/ec2-user/repos/FT-NeonDisco/output/minimal-test/collate-RAW-OUT-test.tsv', separator='\t').collect()

    collated_df
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""First we filter for unique rows based purely on `fusionTranscriptID` and `originalTool`, on the collated TSV. This keeps similar breakpoints that are annotated with paralog gene names. This also retains potentially similar genePair that have different breakpoints.""")
    return


@app.cell
def _(collated_pldf):
    # get unique rows based on fusionTranscriptID column and originalTool column

    unique_collated_df = collated_pldf.unique(subset=["fusionTranscriptID", "originalTool"])

    unique_collated_df # type: ignore

    return (unique_collated_df,)


@app.cell
def _(pl, unique_collated_df):
    # create a new df with unique fusionTranscriptIDs and a list of tools that detected these unique IDs
    tool_group_df = (
        unique_collated_df
        .group_by('fusionTranscriptID')
        .agg(
            pl.col('originalTool').unique().alias('detectedBy'),
            pl.col('originalTool').unique().count().alias('toolOverlapCount')

        )
    )
    return (tool_group_df,)


@app.cell
def _(tool_group_df):
    tool_group_df
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""Now we are ready to use the information in the `tool_group_df` to add the new 'detectedBy' column to the original unique column, by dropping `originalTool` column, running `unique` on `fusionTranscriptID` again, then joining the now truly unique original df with the `tool_group_df` on the `fusionTranscriptID` column.""")
    return


@app.cell
def _(pl, unique_collated_df):
    # unique_fusions_df = (
    #     unique_collated_df
    #     .drop('originalTool')
    #     .unique('fusionTranscriptID')
    #     .join(tool_group_df, on='fusionTranscriptID')
    # )

    unique_fusions_df = (
        unique_collated_df
        .group_by('fusionTranscriptID')
        .agg([
            # Keep all unique values from tool-specific columns
            pl.col('originalTool').unique().alias('detectedBy'),
            pl.col('originalTool').unique().count().alias('toolOverlapCount'),

            # For other columns that might vary by tool, you can:
            # - Take the first value if they should be the same
            pl.col('fusionGenePair').first(),
            pl.col('breakpointID').first(),
            pl.col('5pStrand').first(),
            pl.col('3pStrand').first(),
            pl.col('5pSite').first(),
            pl.col('3pSite').first(),
            pl.col('mutationType').first(),

            # - Or collect all unique values if they might differ
            pl.col('confidenceLabel').unique().alias('confidenceLabels'),
            pl.col('largeAnchorSupport').unique().alias('largeAnchorSupports'),
            pl.col('junctionReadCount').unique().alias('junctionReadCounts'),
            pl.col('spanningFragCount').unique().alias('spanningFragCounts'),
            pl.col('fusionPairAnnotation').unique().alias('fusionPairAnnotations'),

            # Keep sample info
            pl.col('sampleID').first(),
            pl.col('sampleNum').first(),
            pl.col('sampleNum_Padded').first()
        ])
    )

    unique_fusions_df
    return (unique_fusions_df,)


@app.cell
def _(mo):
    mo.md(
        r"""
    ### Load Up CCLE & Internal Cell Line FTs

    We can now load up the parquet file containing the CCLE & Internal Cell Line FT data.
    """
    )
    return


@app.cell
def _(pl):
    # load up CCLE+internal

    ccle_df = pl.scan_parquet('/home/ec2-user/repos/FT-NeonDisco/output/CCLE+internal/01-CCLE+internal-ALL-FT-UNFILTERED.parquet').collect()
    ccle_df
    return (ccle_df,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## Add 'foundInCCLE&InternalCLs' column to unique fusions
    We can now add the `foundInCCLE&InternalCLs` column to the unique fusions dataframe. This will add a new column to the unique fusions dataframe, indicating whether the fusion was found in the CCLE & Internal Cell Lines.

    To achieve this, we will first use the extracted `breakpointID` column from the `ccle_bp_uniq_df` dataframe, and then convert it into a set. This will allow us to check if the `breakpointID` in the unique fusions dataframe is present in the set of `breakpointID`s from the CCLE & Internal Cell Lines dataframe.

    We then will need to create a lambda function that will check if the `breakpointID` in the unique fusions dataframe is present in the set of `breakpointID`s from the CCLE & Internal Cell Lines dataframe. If it is, we will return `True`, otherwise we will return `False`.
    """
    )
    return


@app.cell
def _(ccle_df, pl, unique_fusions_df):

    # create a set of breakpointIDs from the ccle_df dataframe
    ccle_set = set(ccle_df['breakpointID'].to_list())
    # add foundInCCLE&InternalCLs column to unique fusions df by directly checking if the breakpointID is in the set of breakpointIDs from the ccle_df dataframe
    ccle_added_df = unique_fusions_df.with_columns(
        pl.when(pl.col('breakpointID').is_in(ccle_set)).then(True).otherwise(False).alias('foundInCCLE&InternalCLs')
    )

    ccle_added_df
    return (ccle_added_df,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""## Filter Out FTs Found in Panel of Normals (TCGA Normals) Dataset""")
    return


@app.cell
def _(pl):
    # load panel of normals file

    pon_df = pl.scan_csv('/home/ec2-user/repos/FT-NeonDisco/output/TCGANormals/Arr-and-FC_TCGANormals-FTs-with-UNIQUE-breakpointIDs.tsv', separator='\t').collect().drop('detectedBy')

    # save pon_df to parquet
    pon_df.write_parquet('/home/ec2-user/repos/FT-NeonDisco/output/TCGANormals/Arr-and-FC_TCGANormals-FTs-with-UNIQUE-breakpointIDs-v2.parquet')
    # also save to tsv
    pon_df.write_csv('/home/ec2-user/repos/FT-NeonDisco/output/TCGANormals/Arr-and-FC_TCGANormals-FTs-with-UNIQUE-breakpointIDs-v2.tsv', separator='\t')
    # load pon_df from parquet
    pon_pqdf = pl.scan_parquet('/home/ec2-user/repos/FT-NeonDisco/output/TCGANormals/Arr-and-FC_TCGANormals-FTs-with-UNIQUE-breakpointIDs-v2.parquet').collect()
    pon_pqdf
    return (pon_pqdf,)


@app.cell
def _(pon_pqdf):
    set(pon_pqdf['breakpointID'].to_list())
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""Now we can filter out any breakpoints that appear in the PoN dataframe.""")
    return


@app.cell
def _(ccle_added_df, pl, pon_pqdf):
    # create a set of breakpointIDs from the pon_df dataframe
    pon_set = set(pon_pqdf['breakpointID'].to_list())
    # retain only the rows in the ccle_added_df dataframe where the breakpointID is not in the set of breakpointIDs from the pon_df dataframe
    results_df = ccle_added_df.filter(~pl.col('breakpointID').is_in(pon_set))
    results_df
    return (results_df,)


@app.cell
def _(mo):
    mo.md(r"""Finally, we can add a column for FusionInspector, where it contains the same value as 'fusionGenePair' but with the separator :: changed into `--`.""")
    return


@app.cell
def _(pl, results_df):
    # add a column for FusionInspector, where it contains the same value as 'fusionGenePair' but with the separator :: changed into --
    results_df_fusIns = results_df.with_columns(
        pl.col('fusionGenePair').cast(pl.Utf8).str.replace('::', '--').alias('fusionGenePair_FusIns')
    )
    results_df_fusIns


    return (results_df_fusIns,)


@app.cell
def _(pl, results_df_fusIns):
    # we need to represent the nested structure of the detectedBy column as a list of strings
    # Format detectedBy column to use " | " as separator between tools because polars represents the list as nested data
    export_df = results_df_fusIns.with_columns([
        pl.col('detectedBy').list.eval(pl.element().cast(pl.Utf8)).list.join(" | ").alias('detectedBy')
    ])
    export_df
    return (export_df,)


@app.cell
def _(export_df, pl):
    export_filt_df = export_df.filter(pl.col("toolOverlapCount") > 1)
    export_filt_df
    return


@app.cell
def _(export_df, pl):
    # let's grab just the fusionGenePair_FusIns column
    fusins_df = export_df.select(
        pl.col('fusionGenePair_FusIns').unique()
    )
    fusins_df
    fusins_df.write_csv("/home/ec2-user/repos/FT-NeonDisco/output/minimal-test/SAMPLEID_fusins.txt", include_header=False)
    return


if __name__ == "__main__":
    app.run()
